[{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee2"
  },
  "title": "Lecture Notes: Optimization for Machine Learning",
  "abstract": "Lecture notes on optimization for machine learning, derived from a course at Princeton University and tutorials given in MLSS, Buenos Aires, as well as Simons Foundation, Berkeley.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Elad Hazan",
  "date": "2019-09-08T21:49:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.579Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.579Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee3"
  },
  "title": "An Optimal Control View of Adversarial Machine Learning",
  "abstract": "I describe an optimal control view of adversarial machine learning, where the dynamical system is the machine learner, the input are adversarial actions, and the control costs are defined by the adversary's goals to do harm and be hard to detect. This view encompasses many types of adversarial machine learning, including test-item attacks, training-data poisoning, and adversarial reward shaping. The view encourages adversarial machine learning researcher to utilize advances in control theory and reinforcement learning.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Xiaojin Zhu",
  "date": "2018-11-11T14:28:34",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.587Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.587Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee4"
  },
  "title": "Minimax deviation strategies for machine learning and recognition with short learning samples",
  "abstract": "The article is devoted to the problem of small learning samples in machine learning. The flaws of maximum likelihood learning and minimax learning are looked into and the concept of minimax deviation learning is introduced that is free of those flaws.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Michail Schlesinger",
  "date": "2017-07-16T09:15:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.588Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.588Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee5"
  },
  "title": "Machine Learning for Clinical Predictive Analytics",
  "abstract": "In this chapter, we provide a brief overview of applying machine learning techniques for clinical prediction tasks. We begin with a quick introduction to the concepts of machine learning and outline some of the most common machine learning algorithms. Next, we demonstrate how to apply the algorithms with appropriate toolkits to conduct machine learning experiments for clinical prediction tasks. The objectives of this chapter are to (1) understand the basics of machine learning techniques and the reasons behind why they are useful for solving clinical prediction problems, (2) understand the intuition behind some machine learning models, including regression, decision trees, and support vector machines, and (3) understand how to apply these models to clinical prediction problems using publicly available datasets via case studies.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Wei-Hung Weng",
  "date": "2019-09-19T22:02:00",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.589Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.589Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee6"
  },
  "title": "Towards Modular Machine Learning Solution Development: Benefits and Trade-offs",
  "abstract": "Machine learning technologies have demonstrated immense capabilities in various domains. They play a key role in the success of modern businesses. However, adoption of machine learning technologies has a lot of untouched potential. Cost of developing custom machine learning solutions that solve unique business problems is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic nature prevalent in today's machine learning applications stands in the way of efficient and cost effective customized machine learning solution development. In this work we explore the benefits of modular machine learning solutions and discuss how modular machine learning solutions can overcome some of the major solution engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and monolithic machine learning solutions through three deep learning problems; one text based and the two image based. Our experimental results show that modular machine learning solutions have a promising potential to reap the solution engineering advantages of modularity while gaining performance and data advantages in a way the monolithic machine learning solutions do not permit.",
  "tags": [
    "Machine Learning",
    "Software Engineering"
  ],
  "author": "Samiyuru Menik",
  "date": "2023-01-23T22:54:34",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.591Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.591Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee7"
  },
  "title": "Introduction to Machine Learning: Class Notes 67577",
  "abstract": "Introduction to Machine learning covering Statistical Inference (Bayes, EM, ML/MaxEnt duality), algebraic and spectral methods (PCA, LDA, CCA, Clustering), and PAC learning (the Formal model, VC dimension, Double Sampling theorem).",
  "tags": [
    "Machine Learning"
  ],
  "author": "Amnon Shashua",
  "date": "2009-04-23T11:40:57",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.592Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.592Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee8"
  },
  "title": "The Tribes of Machine Learning and the Realm of Computer Architecture",
  "abstract": "Machine learning techniques have influenced the field of computer architecture like many other fields. This paper studies how the fundamental machine learning techniques can be applied towards computer architecture problems. We also provide a detailed survey of computer architecture research that employs different machine learning methods. Finally, we present some future opportunities and the outstanding challenges that need to be overcome to exploit full potential of machine learning for computer architecture.",
  "tags": [
    "Machine Learning",
    "Architecture"
  ],
  "author": "Ayaz Akram",
  "date": "2020-12-07T23:10:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.593Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.593Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ee9"
  },
  "title": "A Machine Learning Tutorial for Operational Meteorology, Part I: Traditional Machine Learning",
  "abstract": "Recently, the use of machine learning in meteorology has increased greatly. While many machine learning methods are not new, university classes on machine learning are largely unavailable to meteorology students and are not required to become a meteorologist. The lack of formal instruction has contributed to perception that machine learning methods are 'black boxes' and thus end-users are hesitant to apply the machine learning methods in their every day workflow. To reduce the opaqueness of machine learning methods and lower hesitancy towards machine learning in meteorology, this paper provides a survey of some of the most common machine learning methods. A familiar meteorological example is used to contextualize the machine learning methods while also discussing machine learning topics using plain language. The following machine learning methods are demonstrated: linear regression; logistic regression; decision trees; random forest; gradient boosted decision trees; naive Bayes; and support vector machines. Beyond discussing the different methods, the paper also contains discussions on the general machine learning process as well as best practices to enable readers to apply machine learning to their own datasets. Furthermore, all code (in the form of Jupyter notebooks and Google Colaboratory notebooks) used to make the examples in the paper is provided in an effort to catalyse the use of machine learning in meteorology.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Randy J. Chase",
  "date": "2022-04-15T14:48:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.594Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.594Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006eea"
  },
  "title": "Position Paper: Towards Transparent Machine Learning",
  "abstract": "Transparent machine learning is introduced as an alternative form of machine learning, where both the model and the learning system are represented in source code form. The goal of this project is to enable direct human understanding of machine learning models, giving us the ability to learn, verify, and refine them as programs. If solved, this technology could represent a best-case scenario for the safety and security of AI systems going forward.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Dustin Juliano",
  "date": "2019-11-12T10:49:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.596Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.596Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006eeb"
  },
  "title": "Understanding Bias in Machine Learning",
  "abstract": "Bias is known to be an impediment to fair decisions in many domains such as human resources, the public sector, health care etc. Recently, hope has been expressed that the use of machine learning methods for taking such decisions would diminish or even resolve the problem. At the same time, machine learning experts warn that machine learning models can be biased as well. In this article, our goal is to explain the issue of bias in machine learning from a technical perspective and to illustrate the impact that biased data can have on a machine learning model. To reach such a goal, we develop interactive plots to visualizing the bias learned from synthetic data.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Jindong Gu",
  "date": "2019-09-02T20:36:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.597Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.597Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006eec"
  },
  "title": "A Unified Analytical Framework for Trustable Machine Learning and Automation Running with Blockchain",
  "abstract": "Traditional machine learning algorithms use data from databases that are mutable, and therefore the data cannot be fully trusted. Also, the machine learning process is difficult to automate. This paper proposes building a trustable machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts are used to automate the machine learning process. This paper makes three contributions. First, it establishes a link between machine learning technology and blockchain technology. Previously, machine learning and blockchain have been considered two independent technologies without an obvious link. Second, it proposes a unified analytical framework for trustable machine learning by using blockchain technology. This unified framework solves both the trustability and automation issues in machine learning. Third, it enables a computer to translate core machine learning implementation from a single thread on a single machine to multiple threads on multiple machines running with blockchain by using a unified approach. The paper uses association rule mining as an example to demonstrate how trustable machine learning can be implemented with blockchain, and it shows how this approach can be used to analyze opioid prescriptions to help combat the opioid crisis.",
  "tags": [
    "Machine Learning",
    "Cryptography and Security"
  ],
  "author": "Tao Wang",
  "date": "2019-03-21T02:17:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.599Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.599Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006eed"
  },
  "title": "MLBench: How Good Are Machine Learning Clouds for Binary Classification Tasks on Structured Data?",
  "abstract": "We conduct an empirical study of machine learning functionalities provided by major cloud service providers, which we call machine learning clouds. Machine learning clouds hold the promise of hiding all the sophistication of running large-scale machine learning: Instead of specifying how to run a machine learning task, users only specify what machine learning task to run and the cloud figures out the rest. Raising the level of abstraction, however, rarely comes free - a performance penalty is possible. How good, then, are current machine learning clouds on real-world machine learning workloads?   We study this question with a focus on binary classication problems. We present mlbench, a novel benchmark constructed by harvesting datasets from Kaggle competitions. We then compare the performance of the top winning code available from Kaggle with that of running machine learning clouds from both Azure and Amazon on mlbench. Our comparative study reveals the strength and weakness of existing machine learning clouds and points out potential future directions for improvement.",
  "tags": [
    "Distributed Computing",
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Yu Liu",
  "date": "2017-07-29T21:59:18",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.600Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.600Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006eee"
  },
  "title": "Data Pricing in Machine Learning Pipelines",
  "abstract": "Machine learning is disruptive. At the same time, machine learning can only succeed by collaboration among many parties in multiple steps naturally as pipelines in an eco-system, such as collecting data for possible machine learning applications, collaboratively training models by multiple parties and delivering machine learning services to end users. Data is critical and penetrating in the whole machine learning pipelines. As machine learning pipelines involve many parties and, in order to be successful, have to form a constructive and dynamic eco-system, marketplaces and data pricing are fundamental in connecting and facilitating those many parties. In this article, we survey the principles and the latest research development of data pricing in machine learning pipelines. We start with a brief review of data marketplaces and pricing desiderata. Then, we focus on pricing in three important steps in machine learning pipelines. To understand pricing in the step of training data collection, we review pricing raw data sets and data labels. We also investigate pricing in the step of collaborative training of machine learning models, and overview pricing machine learning models for end users in the step of machine learning deployment. We also discuss a series of possible future directions.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Zicun Cong",
  "date": "2021-08-18T00:57:06",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.601Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.601Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006eef"
  },
  "title": "Techniques for Automated Machine Learning",
  "abstract": "Automated machine learning (AutoML) aims to find optimal machine learning solutions automatically given a machine learning problem. It could release the burden of data scientists from the multifarious manual tuning process and enable the access of domain experts to the off-the-shelf machine learning solutions without extensive experience. In this paper, we review the current developments of AutoML in terms of three categories, automated feature engineering (AutoFE), automated model and hyperparameter learning (AutoMHL), and automated deep learning (AutoDL). State-of-the-art techniques adopted in the three categories are presented, including Bayesian optimization, reinforcement learning, evolutionary algorithm, and gradient-based approaches. We summarize popular AutoML frameworks and conclude with current open challenges of AutoML.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Statistical Machine Learning"
  ],
  "author": "Yi-Wei Chen",
  "date": "2019-07-21T04:03:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.602Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.602Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef0"
  },
  "title": "The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning",
  "abstract": "With the advance of the powerful heterogeneous, parallel and distributed computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Distributed Computing"
  ],
  "author": "Omer Subasi",
  "date": "2023-12-05T20:40:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.605Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.605Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef1"
  },
  "title": "Parallelization of Machine Learning Algorithms Respectively on Single Machine and Spark",
  "abstract": "With the rapid development of big data technologies, how to dig out useful information from massive data becomes an essential problem. However, using machine learning algorithms to analyze large data may be time-consuming and inefficient on the traditional single machine. To solve these problems, this paper has made some research on the parallelization of several classic machine learning algorithms respectively on the single machine and the big data platform Spark. We compare the runtime and efficiency of traditional machine learning algorithms with parallelized machine learning algorithms respectively on the single machine and Spark platform. The research results have shown significant improvement in runtime and efficiency of parallelized machine learning algorithms.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Jiajun Shen",
  "date": "2022-05-08T03:47:30",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.606Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.606Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef2"
  },
  "title": "AutoCompete: A Framework for Machine Learning Competition",
  "abstract": "In this paper, we propose AutoCompete, a highly automated machine learning framework for tackling machine learning competitions. This framework has been learned by us, validated and improved over a period of more than two years by participating in online machine learning competitions. It aims at minimizing human interference required to build a first useful predictive model and to assess the practical difficulty of a given machine learning challenge. The proposed system helps in identifying data types, choosing a machine learn- ing model, tuning hyper-parameters, avoiding over-fitting and optimization for a provided evaluation metric. We also observe that the proposed system produces better (or comparable) results with less runtime as compared to other approaches.",
  "tags": [
    "Statistical Machine Learning",
    "Machine Learning"
  ],
  "author": "Abhishek Thakur",
  "date": "2015-07-08T15:07:39",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.607Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.607Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef3"
  },
  "title": "Joint Training of Deep Boltzmann Machines",
  "abstract": "We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.",
  "tags": [
    "Statistical Machine Learning",
    "Machine Learning"
  ],
  "author": "Ian Goodfellow",
  "date": "2012-12-12T01:59:27",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.608Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.608Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef4"
  },
  "title": "Proceedings of the 2016 ICML Workshop on #Data4Good: Machine Learning in Social Good Applications",
  "abstract": "This is the Proceedings of the ICML Workshop on #Data4Good: Machine Learning in Social Good Applications, which was held on June 24, 2016 in New York.",
  "tags": [
    "Statistical Machine Learning",
    "Computers and Society",
    "Machine Learning"
  ],
  "author": "Kush R. Varshney",
  "date": "2016-07-08T16:55:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.609Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.609Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef5"
  },
  "title": "Mathematical Perspective of Machine Learning",
  "abstract": "We take a closer look at some theoretical challenges of Machine Learning as a function approximation, gradient descent as the default optimization algorithm, limitations of fixed length and width networks and a different approach to RNNs from a mathematical perspective.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Yarema Boryshchak",
  "date": "2020-07-03T05:26:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.610Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.610Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef6"
  },
  "title": "Private Machine Learning via Randomised Response",
  "abstract": "We introduce a general learning framework for private machine learning based on randomised response. Our assumption is that all actors are potentially adversarial and as such we trust only to release a single noisy version of an individual's datapoint. We discuss a general approach that forms a consistent way to estimate the true underlying machine learning model and demonstrate this in the case of logistic regression.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "David Barber",
  "date": "2020-01-14T17:56:16",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.611Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.611Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef7"
  },
  "title": "A Survey of Optimization Methods from a Machine Learning Perspective",
  "abstract": "Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning fields. Finally, we explore and give some challenges and open problems for the optimization in machine learning.",
  "tags": [
    "Machine Learning",
    "Optimization and Control",
    "Statistical Machine Learning"
  ],
  "author": "Shiliang Sun",
  "date": "2019-06-17T02:54:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.612Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.612Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef8"
  },
  "title": "Ten-year Survival Prediction for Breast Cancer Patients",
  "abstract": "This report assesses different machine learning approaches to 10-year survival prediction of breast cancer patients.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Changmao Li",
  "date": "2019-11-02T19:53:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.614Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.614Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ef9"
  },
  "title": "When Machine Learning Meets Privacy: A Survey and Outlook",
  "abstract": "The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Cryptography and Security"
  ],
  "author": "Bo Liu",
  "date": "2020-11-24T00:52:49",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.615Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.615Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006efa"
  },
  "title": "Augmented Q Imitation Learning (AQIL)",
  "abstract": "The study of unsupervised learning can be generally divided into two categories: imitation learning and reinforcement learning. In imitation learning the machine learns by mimicking the behavior of an expert system whereas in reinforcement learning the machine learns via direct environment feedback. Traditional deep reinforcement learning takes a significant time before the machine starts to converge to an optimal policy. This paper proposes Augmented Q-Imitation-Learning, a method by which deep reinforcement learning convergence can be accelerated by applying Q-imitation-learning as the initial training process in traditional Deep Q-learning.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence"
  ],
  "author": "Xiao Lei Zhang",
  "date": "2020-03-31T18:08:23",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.616Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.616Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006efb"
  },
  "title": "Natural Language Processing using Hadoop and KOSHIK",
  "abstract": "Natural language processing, as a data analytics related technology, is used widely in many research areas such as artificial intelligence, human language processing, and translation. At present, due to explosive growth of data, there are many challenges for natural language processing. Hadoop is one of the platforms that can process the large amount of data required for natural language processing. KOSHIK is one of the natural language processing architectures, and utilizes Hadoop and contains language processing components such as Stanford CoreNLP and OpenNLP. This study describes how to build a KOSHIK platform with the relevant tools, and provides the steps to analyze wiki data. Finally, it evaluates and discusses the advantages and disadvantages of the KOSHIK architecture, and gives recommendations on improving the processing performance.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Emre Erturk",
  "date": "2016-08-15T23:09:21",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.617Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.617Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006efc"
  },
  "title": "Integrating AI Planning with Natural Language Processing: A Combination of Explicit and Tacit Knowledge",
  "abstract": "Natural language processing (NLP) aims at investigating the interactions between agents and humans, processing and analyzing large amounts of natural language data. Large-scale language models play an important role in current natural language processing. However, the challenges of explainability and complexity come along with the developments of language models. One way is to introduce logical relations and rules into natural language processing models, such as making use of Automated Planning. Automated planning (AI planning) focuses on building symbolic domain models and synthesizing plans to transit initial states to goals based on domain models. Recently, there have been plenty of works related to these two fields, which have the abilities to generate explicit knowledge, e.g., preconditions and effects of action models, and learn from tacit knowledge, e.g., neural models, respectively. Integrating AI planning and natural language processing effectively improves the communication between human and intelligent agents. This paper outlines the commons and relations between AI planning and natural language processing, argues that each of them can effectively impact on the other one by five areas: (1) planning-based text understanding, (2) planning-based natural language processing, (3) planning-based explainability, (4) text-based human-robot interaction, and (5) applications. We also explore some potential future issues between AI planning and natural language processing. To the best of our knowledge, this survey is the first work that addresses the deep connections between AI planning and Natural language processing.",
  "tags": [
    "Artificial Intelligence",
    "Computation and Language"
  ],
  "author": "Kebing Jin",
  "date": "2022-02-15T02:19:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.618Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.618Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006efd"
  },
  "title": "Simple Natural Language Processing Tools for Danish",
  "abstract": "This technical note describes a set of baseline tools for automatic processing of Danish text. The tools are machine-learning based, using natural language processing models trained over previously annotated documents. They are maintained at ITU Copenhagen and will always be freely available.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Leon Derczynski",
  "date": "2019-06-27T13:15:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.619Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.619Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006efe"
  },
  "title": "Natural Language Generation",
  "abstract": "This article provides a brief overview of the field of Natural Language Generation. The term Natural Language Generation (NLG), in its broadest definition, refers to the study of systems that verbalize some form of information through natural language. That information could be stored in a large database or knowledge graph (in data-to-text applications), but NLG researchers may also study summarisation (text-to-text) or image captioning (image-to-text), for example. As a subfield of Natural Language Processing, NLG is closely related to other sub-disciplines such as Machine Translation (MT) and Dialog Systems. Some NLG researchers exclude MT from their definition of the field, since there is no content selection involved where the system has to determine what to say. Conversely, dialog systems do not typically fall under the header of Natural Language Generation since NLG is just one component of dialog systems (the others being Natural Language Understanding and Dialog Management). However, with the rise of Large Language Models (LLMs), different subfields of Natural Language Processing have converged on similar methodologies for the production of natural language and the evaluation of automatically generated text.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Emiel van Miltenburg",
  "date": "2025-03-20T22:12:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.620Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.620Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006eff"
  },
  "title": "Towards the Study of Morphological Processing of the Tangkhul Language",
  "abstract": "There is no or little work on natural language processing of Tangkhul language. The current work is a humble beginning of morphological processing of this language using an unsupervised approach. We use a small corpus collected from different sources of text books, short stories and articles of other topics. Based on the experiments carried out, the morpheme identification task using morphessor gives reasonable and interesting output despite using a small corpus.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Mirinso Shadang",
  "date": "2020-06-29T17:24:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.621Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.621Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f00"
  },
  "title": "A Primer on Neural Network Models for Natural Language Processing",
  "abstract": "Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Yoav Goldberg",
  "date": "2015-10-02T20:17:33",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.623Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.623Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f01"
  },
  "title": "Natural Language Understanding with Distributed Representation",
  "abstract": "This is a lecture note for the course DS-GA 3001 <Natural Language Understanding with Distributed Representation> at the Center for Data Science , New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.",
  "tags": [
    "Computation and Language",
    "Statistical Machine Learning"
  ],
  "author": "Kyunghyun Cho",
  "date": "2015-11-24T23:23:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.624Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.624Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f02"
  },
  "title": "Deploying Technology to Save Endangered Languages",
  "abstract": "Computer scientists working on natural language processing, native speakers of endangered languages, and field linguists to discuss ways to harness Automatic Speech Recognition, especially neural networks, to automate annotation, speech tagging, and text parsing on endangered languages.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Hilaria Cruz",
  "date": "2019-08-23T18:31:35",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.625Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.625Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f03"
  },
  "title": "Multilingual Text Classification for Dravidian Languages",
  "abstract": "As the fourth largest language family in the world, the Dravidian languages have become a research hotspot in natural language processing (NLP). Although the Dravidian languages contain a large number of languages, there are relatively few public available resources. Besides, text classification task, as a basic task of natural language processing, how to combine it to multiple languages in the Dravidian languages, is still a major difficulty in Dravidian Natural Language Processing. Hence, to address these problems, we proposed a multilingual text classification framework for the Dravidian languages. On the one hand, the framework used the LaBSE pre-trained model as the base model. Aiming at the problem of text information bias in multi-task learning, we propose to use the MLM strategy to select language-specific words, and used adversarial training to perturb them. On the other hand, in view of the problem that the model cannot well recognize and utilize the correlation among languages, we further proposed a language-specific representation module to enrich semantic information for the model. The experimental results demonstrated that the framework we proposed has a significant performance in multilingual text classification tasks with each strategy achieving certain improvements.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Xiaotian Lin",
  "date": "2021-12-03T04:26:49",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.626Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.626Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f04"
  },
  "title": "Supporting Undotted Arabic with Pre-trained Language Models",
  "abstract": "We observe a recent behaviour on social media, in which users intentionally remove consonantal dots from Arabic letters, in order to bypass content-classification algorithms. Content classification is typically done by fine-tuning pre-trained language models, which have been recently employed by many natural-language-processing applications. In this work we study the effect of applying pre-trained Arabic language models on \"undotted\" Arabic texts. We suggest several ways of supporting undotted texts with pre-trained models, without additional training, and measure their performance on two Arabic natural-language-processing downstream tasks. The results are encouraging; in one of the tasks our method shows nearly perfect performance.",
  "tags": [
    "Computation and Language",
    "Machine Learning"
  ],
  "author": "Aviad Rom",
  "date": "2021-11-18T16:47:56",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.627Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.627Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f05"
  },
  "title": "A Precis of Language Models are not Models of Language",
  "abstract": "Natural Language Processing is one of the leading application areas in the current resurgence of Artificial Intelligence, spearheaded by Artificial Neural Networks. We show that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill-suited as comprehensive models of natural language. The wider implication is that, in spite of the often overbearing optimism about AI, modern neural models do not represent a revolution in our understanding of cognition.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Csaba Veres",
  "date": "2022-05-16T12:50:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.628Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.628Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f06"
  },
  "title": "Fence - An Efficient Parser with Ambiguity Support for Model-Driven Language Specification",
  "abstract": "Model-based language specification has applications in the implementation of language processors, the design of domain-specific languages, model-driven software development, data integration, text mining, natural language processing, and corpus-based induction of models. Model-based language specification decouples language design from language processing and, unlike traditional grammar-driven approaches, which constrain language designers to specific kinds of grammars, it needs general parser generators able to deal with ambiguities. In this paper, we propose Fence, an efficient bottom-up parsing algorithm with lexical and syntactic ambiguity support that enables the use of model-based language specification in practice.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Luis Quesada",
  "date": "2011-07-23T12:56:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.629Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.629Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f07"
  },
  "title": "Including Signed Languages in Natural Language Processing",
  "abstract": "Signed languages are the primary means of communication for many deaf and hard of hearing individuals. Since signed languages exhibit all the fundamental linguistic properties of natural language, we believe that tools and theories of Natural Language Processing (NLP) are crucial towards its modeling. However, existing research in Sign Language Processing (SLP) seldom attempt to explore and leverage the linguistic organization of signed languages. This position paper calls on the NLP community to include signed languages as a research area with high social and scientific impact. We first discuss the linguistic properties of signed languages to consider during their modeling. Then, we review the limitations of current SLP models and identify the open challenges to extend NLP to signed languages. Finally, we urge (1) the adoption of an efficient tokenization method; (2) the development of linguistically-informed models; (3) the collection of real-world signed language data; (4) the inclusion of local signed language communities as an active and leading voice in the direction of research.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Kayo Yin",
  "date": "2021-05-11T17:37:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.630Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.630Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f08"
  },
  "title": "Self-move and Other-move: Quantum Categorical Foundations of Japanese",
  "abstract": "The purpose of this work is to contribute toward the larger goal of creating a Quantum Natural Language Processing (QNLP) translator program. This work contributes original diagrammatic representations of the Japanese language based on prior work that accomplished on the English language based on category theory. The germane differences between the English and Japanese languages are emphasized to help address English language bias in the current body of research. Additionally, topological principles of these diagrams and many potential avenues for further research are proposed. Why is this endeavor important? Hundreds of languages have developed over the course of millennia coinciding with the evolution of human interaction across time and geographic location. These languages are foundational to human survival, experience, flourishing, and living the good life. They are also, however, the strongest barrier between people groups. Over the last several decades, advancements in Natural Language Processing (NLP) have made it easier to bridge the gap between individuals who do not share a common language or culture. Tools like Google Translate and DeepL make it easier than ever before to share our experiences with people globally. Nevertheless, these tools are still inadequate as they fail to convey our ideas across the language barrier fluently, leaving people feeling anxious and embarrassed. This is particularly true of languages born out of substantially different cultures, such as English and Japanese. Quantum computers offer the best chance to achieve translation fluency in that they are better suited to simulating the natural world and natural phenomenon such as natural speech.   Keywords: category theory, DisCoCat, DisCoCirc, Japanese grammar, English grammar, translation, topology, Quantum Natural Language Processing, Natural Language Processing",
  "tags": [
    "Computation and Language"
  ],
  "author": "Ryder Dale Walton",
  "date": "2022-10-10T06:26:59",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.631Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.631Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f09"
  },
  "title": "PersianLLaMA: Towards Building First Persian Large Language Model",
  "abstract": "Despite the widespread use of the Persian language by millions globally, limited efforts have been made in natural language processing for this language. The use of large language models as effective tools in various natural language processing tasks typically requires extensive textual data and robust hardware resources. Consequently, the scarcity of Persian textual data and the unavailability of powerful hardware resources have hindered the development of large language models for Persian. This paper introduces the first large Persian language model, named PersianLLaMA, trained on a collection of Persian texts and datasets. This foundational model comes in two versions, with 7 and 13 billion parameters, trained on formal and colloquial Persian texts using two different approaches. PersianLLaMA has been evaluated for natural language generation tasks based on the latest evaluation methods, namely using larger language models, and for natural language understanding tasks based on automated machine metrics. The results indicate that PersianLLaMA significantly outperforms its competitors in both understanding and generating Persian text. PersianLLaMA marks an important step in the development of Persian natural language processing and can be a valuable resource for the Persian-speaking community. This large language model can be used for various natural language processing tasks, especially text generation like chatbots, question-answering, machine translation, and text summarization",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Mohammad Amin Abbasi",
  "date": "2023-12-25T12:48:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.632Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.632Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f0a"
  },
  "title": "Problems and Countermeasures in Natural Language Processing Evaluation",
  "abstract": "Evaluation in natural language processing guides and promotes research on models and methods. In recent years, new evalua-tion data sets and evaluation tasks have been continuously proposed. At the same time, a series of problems exposed by ex-isting evaluation have also restricted the progress of natural language processing technology. Starting from the concept, com-position, development and meaning of natural language evaluation, this article classifies and summarizes the tasks and char-acteristics of mainstream natural language evaluation, and then summarizes the problems and causes of natural language pro-cessing evaluation. Finally, this article refers to the human language ability evaluation standard, puts forward the concept of human-like machine language ability evaluation, and proposes a series of basic principles and implementation ideas for hu-man-like machine language ability evaluation from the three aspects of reliability, difficulty and validity.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Qingxiu Dong",
  "date": "2021-04-20T01:35:16",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.633Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.633Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f0b"
  },
  "title": "Continuous multilinguality with language vectors",
  "abstract": "Most existing models for multilingual natural language processing (NLP) treat language as a discrete category, and make predictions for either one language or the other. In contrast, we propose using continuous vector representations of language. We show that these can be learned efficiently with a character-based neural language model, and used to improve inference about language varieties not seen during training. In experiments with 1303 Bible translations into 990 different languages, we empirically explore the capacity of multilingual language models, and also show that the language vectors capture genetic relationships between languages.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Robert Östling",
  "date": "2016-12-22T08:29:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.634Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.634Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f0c"
  },
  "title": "Specifying Logic Programs in Controlled Natural Language",
  "abstract": "Writing specifications for computer programs is not easy since one has to take into account the disparate conceptual worlds of the application domain and of software development. To bridge this conceptual gap we propose controlled natural language as a declarative and application-specific specification language. Controlled natural language is a subset of natural language that can be accurately and efficiently processed by a computer, but is expressive enough to allow natural usage by non-specialists. Specifications in controlled natural language are automatically translated into Prolog clauses, hence become formal and executable. The translation uses a definite clause grammar (DCG) enhanced by feature structures. Inter-text references of the specification, e.g. anaphora, are resolved with the help of discourse representation theory (DRT). The generated Prolog clauses are added to a knowledge base. We have implemented a prototypical specification system that successfully processes the specification of a simple automated teller machine.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Norbert E. Fuchs",
  "date": "1995-07-21T17:44:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.635Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.635Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f0d"
  },
  "title": "A natural language interface to a graph-based bibliographic information retrieval system",
  "abstract": "With the ever-increasing scientific literature, there is a need on a natural language interface to bibliographic information retrieval systems to retrieve related information effectively. In this paper, we propose a natural language interface, NLI-GIBIR, to a graph-based bibliographic information retrieval system. In designing NLI-GIBIR, we developed a novel framework that can be applicable to graph-based bibliographic information retrieval systems. Our framework integrates algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. NLI-GIBIR allows users to search for a variety of bibliographic data through natural language. A series of text- and linguistic-based techniques are used to analyze and answer natural language queries, including tokenization, named entity recognition, and syntactic analysis. We find that our framework can effectively represents and addresses complex bibliographic information needs. Thus, the contributions of this paper are as follows: First, to our knowledge, it is the first attempt to propose a natural language interface to graph-based bibliographic information retrieval. Second, we propose a novel customized natural language processing framework that integrates a few original algorithms/heuristics for interpreting and analyzing natural language bibliographic queries. Third, we show that the proposed framework and natural language interface provide a practical solution in building real-world natural language interface-based bibliographic information retrieval systems. Our experimental results show that the presented system can correctly answer 39 out of 40 example natural language queries with varying lengths and complexities.",
  "tags": [
    "Information Retrieval",
    "Computation and Language"
  ],
  "author": "Yongjun Zhu",
  "date": "2016-12-10T00:32:28",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.636Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.636Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f0e"
  },
  "title": "A Survey of Resources and Methods for Natural Language Processing of Serbian Language",
  "abstract": "The Serbian language is a Slavic language spoken by over 12 million speakers and well understood by over 15 million people. In the area of natural language processing, it can be considered a low-resourced language. Also, Serbian is considered a high-inflectional language. The combination of many word inflections and low availability of language resources makes natural language processing of Serbian challenging. Nevertheless, over the past three decades, there have been a number of initiatives to develop resources and methods for natural language processing of Serbian, ranging from developing a corpus of free text from books and the internet, annotated corpora for classification and named entity recognition tasks to various methods and models performing these tasks. In this paper, we review the initiatives, resources, methods, and their availability.",
  "tags": [
    "Computation and Language",
    "Human-Computer Interaction"
  ],
  "author": "Ulfeta A. Marovac",
  "date": "2023-04-11T19:33:41",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.637Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.637Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f0f"
  },
  "title": "ANGLEr: A Next-Generation Natural Language Exploratory Framework",
  "abstract": "Natural language processing is used for solving a wide variety of problems. Some scholars and interest groups working with language resources are not well versed in programming, so there is a need for a good graphical framework that allows users to quickly design and test natural language processing pipelines without the need for programming. The existing frameworks do not satisfy all the requirements for such a tool. We, therefore, propose a new framework that provides a simple way for its users to build language processing pipelines. It also allows a simple programming language agnostic way for adding new modules, which will help the adoption by natural language processing developers and researchers. The main parts of the proposed framework consist of (a) a pluggable Docker-based architecture, (b) a general data model, and (c) APIs description along with the graphical user interface. The proposed design is being used for implementation of a new natural language processing framework, called ANGLEr.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Timotej Knez",
  "date": "2022-05-10T13:32:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.638Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.638Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f10"
  },
  "title": "Natural Language Generation Using Link Grammar for General Conversational Intelligence",
  "abstract": "Many current artificial general intelligence (AGI) and natural language processing (NLP) architectures do not possess general conversational intelligence--that is, they either do not deal with language or are unable to convey knowledge in a form similar to the human language without manual, labor-intensive methods such as template-based customization. In this paper, we propose a new technique to automatically generate grammatically valid sentences using the Link Grammar database. This natural language generation method far outperforms current state-of-the-art baselines and may serve as the final component in a proto-AGI question answering pipeline that understandably handles natural language material.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Vignav Ramesh",
  "date": "2021-04-19T06:16:07",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.639Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.639Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f11"
  },
  "title": "Information Flow in Pregroup Models of Natural Language",
  "abstract": "This paper is about pregroup models of natural languages, and how they relate to the explicitly categorical use of pregroups in Compositional Distributional Semantics and Natural Language Processing. These categorical interpretations make certain assumptions about the nature of natural languages that, when stated formally, may be seen to impose strong restrictions on pregroup grammars for natural languages.   We formalize this as a hypothesis about the form that pregroup models of natural languages must take, and demonstrate by an artificial language example that these restrictions are not imposed by the pregroup axioms themselves. We compare and contrast the artificial language examples with natural languages (using Welsh, a language where the 'noun' type cannot be taken as primitive, as an illustrative example).   The hypothesis is simply that there must exist a causal connection, or information flow, between the words of a sentence in a language whose purpose is to communicate information. This is not necessarily the case with formal languages that are simply generated by a series of 'meaning-free' rules. This imposes restrictions on the types of pregroup grammars that we expect to find in natural languages; we formalize this in algebraic, categorical, and graphical terms.   We take some preliminary steps in providing conditions that ensure pregroup models satisfy these conjectured properties, and discuss the more general forms this hypothesis may take.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Peter M. Hines",
  "date": "2018-11-08T05:10:34",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.640Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.640Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f12"
  },
  "title": "Natural Language Understanding Based on Semantic Relations between Sentences",
  "abstract": "In this paper, we define event expression over sentences of natural language and semantic relations between events. Based on this definition, we formally consider text understanding process having events as basic unit.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Hyeok Kong",
  "date": "2012-12-19T14:40:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.641Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.641Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f13"
  },
  "title": "Implications of Computer Vision Driven Assistive Technologies Towards Individuals with Visual Impairment",
  "abstract": "Computer vision based technology is becoming ubiquitous in society. One application area that has seen an increase in computer vision is assistive technologies, specifically for those with visual impairment. Research has shown the ability of computer vision models to achieve tasks such provide scene captions, detect objects and recognize faces. Although assisting individuals with visual impairment with these tasks increases their independence and autonomy, concerns over bias, privacy and potential usefulness arise. This paper addresses the positive and negative implications computer vision based assistive technologies have on individuals with visual impairment, as well as considerations for computer vision researchers and developers in order to mitigate the amount of negative implications.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Computers and Society"
  ],
  "author": "Linda Wang",
  "date": "2019-05-20T02:00:56",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.642Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.642Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f14"
  },
  "title": "Second Croatian Computer Vision Workshop (CCVW 2013)",
  "abstract": "Proceedings of the Second Croatian Computer Vision Workshop (CCVW 2013, http://www.fer.unizg.hr/crv/ccvw2013) held September 19, 2013, in Zagreb, Croatia. Workshop was organized by the Center of Excellence for Computer Vision of the University of Zagreb.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Sven Lončarić",
  "date": "2013-10-01T14:26:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.644Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.644Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f15"
  },
  "title": "Multiband NFC for High-Throughput Wireless Computer Vision Sensor Network",
  "abstract": "Vision sensors lie in the heart of computer vision. In many computer vision applications, such as AR/VR, non-contacting near-field communication (NFC) with high throughput is required to transfer information to algorithms. In this work, we proposed a novel NFC system which utilizes multiple frequency bands to achieve high throughput.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "F. Li",
  "date": "2017-05-28T06:43:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.645Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.645Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f16"
  },
  "title": "Deep Learning vs. Traditional Computer Vision",
  "abstract": "Deep Learning has pushed the limits of what was possible in the domain of Digital Image Processing. However, that is not to say that the traditional computer vision techniques which had been undergoing progressive development in years prior to the rise of DL have become obsolete. This paper will analyse the benefits and drawbacks of each approach. The aim of this paper is to promote a discussion on whether knowledge of classical computer vision techniques should be maintained. The paper will also explore how the two sides of computer vision can be combined. Several recent hybrid methodologies are reviewed which have demonstrated the ability to improve computer vision performance and to tackle problems not suited to Deep Learning. For example, combining traditional computer vision techniques with Deep Learning has been popular in emerging domains such as Panoramic Vision and 3D vision for which Deep Learning models have not yet been fully optimised",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Machine Learning"
  ],
  "author": "Niall O' Mahony",
  "date": "2019-10-30T12:25:10",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.646Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.646Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f17"
  },
  "title": "Enhancing camera surveillance using computer vision: a research note",
  "abstract": "$\\mathbf{Purpose}$ - The growth of police operated surveillance cameras has out-paced the ability of humans to monitor them effectively. Computer vision is a possible solution. An ongoing research project on the application of computer vision within a municipal police department is described. The paper aims to discuss these issues.   $\\mathbf{Design/methodology/approach}$ - Following the demystification of computer vision technology, its potential for police agencies is developed within a focus on computer vision as a solution for two common surveillance camera tasks (live monitoring of multiple surveillance cameras and summarizing archived video files). Three unaddressed research questions (can specialized computer vision applications for law enforcement be developed at this time, how will computer vision be utilized within existing public safety camera monitoring rooms, and what are the system-wide impacts of a computer vision capability on local criminal justice systems) are considered.   $\\mathbf{Findings}$ - Despite computer vision becoming accessible to law enforcement agencies the impact of computer vision has not been discussed or adequately researched. There is little knowledge of computer vision or its potential in the field.   $\\mathbf{Originality/value}$ - This paper introduces and discusses computer vision from a law enforcement perspective and will be valuable to police personnel tasked with monitoring large camera networks and considering computer vision as a system upgrade.",
  "tags": [
    "Computers and Society"
  ],
  "author": "Haroon Idrees",
  "date": "2018-08-12T20:01:37",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.647Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.647Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f18"
  },
  "title": "Are object detection assessment criteria ready for maritime computer vision?",
  "abstract": "Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Dilip K. Prasad",
  "date": "2018-09-12T20:18:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.648Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.648Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f19"
  },
  "title": "BMVC 2019: Workshop on Interpretable and Explainable Machine Vision",
  "abstract": "Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable Machine Vision, Cardiff, UK, September 12, 2019.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Alun Preece",
  "date": "2019-09-16T14:44:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.649Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.649Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f1a"
  },
  "title": "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models",
  "abstract": "Large-scale Vision-Language Models (LVLMs) have significantly advanced with text-aligned vision inputs. They have made remarkable progress in computer vision tasks by aligning text modality with vision inputs. There are also endeavors to incorporate multi-vision sensors beyond RGB, including thermal, depth, and medical X-ray images. However, we observe that current LVLMs view images taken from multi-vision sensors as if they were in the same RGB domain without considering the physical characteristics of multi-vision sensors. They fail to convey the fundamental multi-vision sensor information from the dataset and the corresponding contextual knowledge properly. Consequently, alignment between the information from the actual physical environment and the text is not achieved correctly, making it difficult to answer complex sensor-related questions that consider the physical environment. In this paper, we aim to establish a multi-vision Sensor Perception And Reasoning benchmarK called SPARK that can reduce the fundamental multi-vision sensor information gap between images and multi-vision sensors. We generated 6,248 vision-language test samples to investigate multi-vision sensory perception and multi-vision sensory reasoning on physical sensor knowledge proficiency across different formats, covering different types of sensor-related questions. We utilized these samples to assess ten leading LVLMs. The results showed that most models displayed deficiencies in multi-vision sensory reasoning to varying extents. Codes and data are available at https://github.com/top-yun/SPARK",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Youngjoon Yu",
  "date": "2024-08-22T03:59:48",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.650Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.650Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f1b"
  },
  "title": "Vision Transformers in Medical Computer Vision -- A Contemplative Retrospection",
  "abstract": "Recent escalation in the field of computer vision underpins a huddle of algorithms with the magnificent potential to unravel the information contained within images. These computer vision algorithms are being practised in medical image analysis and are transfiguring the perception and interpretation of Imaging data. Among these algorithms, Vision Transformers are evolved as one of the most contemporary and dominant architectures that are being used in the field of computer vision. These are immensely utilized by a plenty of researchers to perform new as well as former experiments. Here, in this article we investigate the intersection of Vision Transformers and Medical images and proffered an overview of various ViTs based frameworks that are being used by different researchers in order to decipher the obstacles in Medical Computer Vision. We surveyed the application of Vision transformers in different areas of medical computer vision such as image-based disease classification, anatomical structure segmentation, registration, region-based lesion Detection, captioning, report generation, reconstruction using multiple medical imaging modalities that greatly assist in medical diagnosis and hence treatment process. Along with this, we also demystify several imaging modalities used in Medical Computer Vision. Moreover, to get more insight and deeper understanding, self-attention mechanism of transformers is also explained briefly. Conclusively, we also put some light on available data sets, adopted methodology, their performance measures, challenges and their solutions in form of discussion. We hope that this review article will open future directions for researchers in medical computer vision.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Machine Learning"
  ],
  "author": "Arshi Parvaiz",
  "date": "2022-03-29T06:32:43",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.651Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.651Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f1c"
  },
  "title": "Adapting Computer Vision Algorithms for Omnidirectional Video",
  "abstract": "Omnidirectional (360{\\deg}) video has got quite popular because it provides a highly immersive viewing experience. For computer vision algorithms, it poses several challenges, like the special (equirectangular) projection commonly employed and the huge image size. In this work, we give a high-level overview of these challenges and outline strategies how to adapt computer vision algorithm for the specifics of omnidirectional video.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Hannes Fassold",
  "date": "2019-07-22T11:12:35",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.652Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.652Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f1d"
  },
  "title": "Real-time Tracking Based on Neuromrophic Vision",
  "abstract": "Real-time tracking is an important problem in computer vision in which most methods are based on the conventional cameras. Neuromorphic vision is a concept defined by incorporating neuromorphic vision sensors such as silicon retinas in vision processing system. With the development of the silicon technology, asynchronous event-based silicon retinas that mimic neuro-biological architectures has been developed in recent years. In this work, we combine the vision tracking algorithm of computer vision with the information encoding mechanism of event-based sensors which is inspired from the neural rate coding mechanism. The real-time tracking of single object with the advantage of high speed of 100 time bins per second is successfully realized. Our method demonstrates that the computer vision methods could be used for the neuromorphic vision processing and we can realize fast real-time tracking using neuromorphic vision sensors compare to the conventional camera.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Hongmin Li",
  "date": "2015-10-18T16:27:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.653Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.653Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f1e"
  },
  "title": "Reconfiguring the Imaging Pipeline for Computer Vision",
  "abstract": "Advancements in deep learning have ignited an explosion of research on efficient hardware for embedded computer vision. Hardware vision acceleration, however, does not address the cost of capturing and processing the image data that feeds these algorithms. We examine the role of the image signal processing (ISP) pipeline in computer vision to identify opportunities to reduce computation and save energy. The key insight is that imaging pipelines should be designed to be configurable: to switch between a traditional photography mode and a low-power vision mode that produces lower-quality image data suitable only for computer vision. We use eight computer vision algorithms and a reversible pipeline simulation tool to study the imaging system's impact on vision performance. For both CNN-based and classical vision algorithms, we observe that only two ISP stages, demosaicing and gamma compression, are critical for task performance. We propose a new image sensor design that can compensate for skipping these stages. The sensor design features an adjustable resolution and tunable analog-to-digital converters (ADCs). Our proposed imaging system's vision mode disables the ISP entirely and configures the sensor to produce subsampled, lower-precision image data. This vision mode can save ~75% of the average energy of a baseline photography mode while having only a small impact on vision task accuracy.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Mark Buckler",
  "date": "2017-05-11T18:57:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.654Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.654Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f1f"
  },
  "title": "Integration and Performance Analysis of Artificial Intelligence and Computer Vision Based on Deep Learning Algorithms",
  "abstract": "This paper focuses on the analysis of the application effectiveness of the integration of deep learning and computer vision technologies. Deep learning achieves a historic breakthrough by constructing hierarchical neural networks, enabling end-to-end feature learning and semantic understanding of images. The successful experiences in the field of computer vision provide strong support for training deep learning algorithms. The tight integration of these two fields has given rise to a new generation of advanced computer vision systems, significantly surpassing traditional methods in tasks such as machine vision image classification and object detection. In this paper, typical image classification cases are combined to analyze the superior performance of deep neural network models while also pointing out their limitations in generalization and interpretability, proposing directions for future improvements. Overall, the efficient integration and development trend of deep learning with massive visual data will continue to drive technological breakthroughs and application expansion in the field of computer vision, making it possible to build truly intelligent machine vision systems. This deepening fusion paradigm will powerfully promote unprecedented tasks and functions in computer vision, providing stronger development momentum for related disciplines and industries.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Artificial Intelligence"
  ],
  "author": "Bo Liu",
  "date": "2023-12-20T09:37:06",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.655Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.655Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f20"
  },
  "title": "Scaling Up Computer Vision Neural Networks Using Fast Fourier Transform",
  "abstract": "Deep Learning-based Computer Vision field has recently been trying to explore larger kernels for convolution to effectively scale up Convolutional Neural Networks. Simultaneously, new paradigm of models such as Vision Transformers find it difficult to scale up to larger higher resolution images due to their quadratic complexity in terms of input sequence. In this report, Fast Fourier Transform is utilised in various ways to provide some solutions to these issues.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Machine Learning"
  ],
  "author": "Siddharth Agrawal",
  "date": "2023-02-02T19:19:10",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.656Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.656Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f21"
  },
  "title": "Are Vision-Language Models Truly Understanding Multi-vision Sensor?",
  "abstract": "Large-scale Vision-Language Models (VLMs) have advanced by aligning vision inputs with text, significantly improving performance in computer vision tasks. Moreover, for VLMs to be effectively utilized in real-world applications, an understanding of diverse multi-vision sensor data, such as thermal, depth, and X-ray information, is essential. However, we find that current VLMs process multi-vision sensor images without deep understanding of sensor information, disregarding each sensor's unique physical properties. This limitation restricts their capacity to interpret and respond to complex questions requiring multi-vision sensor reasoning. To address this, we propose a novel Multi-vision Sensor Perception and Reasoning (MS-PR) benchmark, assessing VLMs on their capacity for sensor-specific reasoning. Moreover, we introduce Diverse Negative Attributes (DNA) optimization to enable VLMs to perform deep reasoning on multi-vision sensor tasks, helping to bridge the core information gap between images and sensor data. Extensive experimental results validate that the proposed DNA method can significantly improve the multi-vision sensor reasoning for VLMs.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Sangyun Chung",
  "date": "2024-12-30T06:44:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.657Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.657Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f22"
  },
  "title": "Ethics and Creativity in Computer Vision",
  "abstract": "This paper offers a retrospective of what we learnt from organizing the workshop *Ethical Considerations in Creative applications of Computer Vision* at CVPR 2021 conference and, prior to that, a series of workshops on *Computer Vision for Fashion, Art and Design* at ECCV 2018, ICCV 2019, and CVPR 2020. We hope this reflection will bring artists and machine learning researchers into conversation around the ethical and social dimensions of creative applications of computer vision.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Computers and Society",
    "Machine Learning"
  ],
  "author": "Negar Rostamzadeh",
  "date": "2021-12-06T15:23:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.658Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.658Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f23"
  },
  "title": "I'm sorry to say, but your understanding of image processing fundamentals is absolutely wrong",
  "abstract": "The ongoing discussion whether modern vision systems have to be viewed as visually-enabled cognitive systems or cognitively-enabled vision systems is groundless, because perceptual and cognitive faculties of vision are separate components of human (and consequently, artificial) information processing system modeling.",
  "tags": [
    "Artificial Intelligence",
    "Computer Vision and Pattern Recognition",
    "Information Retrieval",
    "Robotics",
    "Neurons and Cognition"
  ],
  "author": "Emanuel Diamant",
  "date": "2008-08-01T04:45:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.659Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.659Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f24"
  },
  "title": "Nomic Embed Vision: Expanding the Latent Space",
  "abstract": "This technical report describes the training of nomic-embed-vision, a highly performant, open-code, open-weights image embedding model that shares the same latent space as nomic-embed-text. Together, nomic-embed-vision and nomic-embed-text form the first unified latent space to achieve high performance across vision, language, and multimodal tasks.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Artificial Intelligence"
  ],
  "author": "Zach Nussbaum",
  "date": "2024-06-06T21:02:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.660Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.660Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f25"
  },
  "title": "Computer Vision and Abnormal Patient Gait Assessment a Comparison of Machine Learning Models",
  "abstract": "Abnormal gait, its associated falls and complications have high patient morbidity, mortality. Computer vision detects, predicts patient gait abnormalities, assesses fall risk and serves as clinical decision support tool for physicians. This paper performs a systematic review of how computer vision, machine learning models perform an abnormal patient's gait assessment. Computer vision is beneficial in gait analysis, it helps capture the patient posture. Several literature suggests the use of different machine learning algorithms such as SVM, ANN, K-Star, Random Forest, KNN, among others to perform the classification on the features extracted to study patient gait abnormalities.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Jasmin Hundall",
  "date": "2020-03-22T02:00:15",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.661Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.661Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f26"
  },
  "title": "Tuning computer vision models with task rewards",
  "abstract": "Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "André Susano Pinto",
  "date": "2023-02-16T11:49:48",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.663Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.663Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f27"
  },
  "title": "The possibility of making \\$138,000 from shredded banknote pieces using computer vision",
  "abstract": "Every country must dispose of old banknotes. At the Hong Kong Monetary Authority visitor center, visitors can buy a paperweight souvenir full of shredded banknotes. Even though the shredded banknotes are small, by using computer vision, it is possible to reconstruct the whole banknote like a jigsaw puzzle. Each paperweight souvenir costs \\$100 HKD, and it is claimed to contain shredded banknotes equivalent to 138 complete \\$1000 HKD banknotes. In theory, \\$138,000 HKD can be recovered by using computer vision. This paper discusses the technique of collecting shredded banknote pieces and applying a computer vision program.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Chung To Kong",
  "date": "2023-11-17T02:25:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.664Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.664Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f28"
  },
  "title": "Computer Stereo Vision for Autonomous Driving",
  "abstract": "As an important component of autonomous systems, autonomous car perception has had a big leap with recent advances in parallel computing architectures. With the use of tiny but full-feature embedded supercomputers, computer stereo vision has been prevalently applied in autonomous cars for depth perception. The two key aspects of computer stereo vision are speed and accuracy. They are both desirable but conflicting properties, as the algorithms with better disparity accuracy usually have higher computational complexity. Therefore, the main aim of developing a computer stereo vision algorithm for resource-limited hardware is to improve the trade-off between speed and accuracy. In this chapter, we introduce both the hardware and software aspects of computer stereo vision for autonomous car systems. Then, we discuss four autonomous car perception tasks, including 1) visual feature detection, description and matching, 2) 3D information acquisition, 3) object detection/recognition and 4) semantic image segmentation. The principles of computer stereo vision and parallel computing on multi-threading CPU and GPU architectures are then detailed.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Rui Fan",
  "date": "2020-12-06T06:54:03",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.665Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.665Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f29"
  },
  "title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks",
  "abstract": "The success of large language models (LLMs) has fostered a new research trend of multi-modality large language models (MLLMs), which changes the paradigm of various fields in computer vision. Though MLLMs have shown promising results in numerous high-level vision and vision-language tasks such as VQA and text-to-image, no works have demonstrated how low-level vision tasks can benefit from MLLMs. We find that most current MLLMs are blind to low-level features due to their design of vision modules, thus are inherently incapable for solving low-level vision tasks. In this work, we purpose $\\textbf{LM4LV}$, a framework that enables a FROZEN LLM to solve a range of low-level vision tasks without any multi-modal data or prior. This showcases the LLM's strong potential in low-level vision and bridges the gap between MLLMs and low-level vision tasks. We hope this work can inspire new perspectives on LLMs and deeper understanding of their mechanisms. Code is available at https://github.com/bytetriper/LM4LV.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Boyang Zheng",
  "date": "2024-05-24T17:25:00",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.666Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.666Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f2a"
  },
  "title": "A survey of the Vision Transformers and their CNN-Transformer based Variants",
  "abstract": "Vision transformers have become popular as a possible substitute to convolutional neural networks (CNNs) for a variety of computer vision applications. These transformers, with their ability to focus on global relationships in images, offer large learning capacity. However, they may suffer from limited generalization as they do not tend to model local correlation in images. Recently, in vision transformers hybridization of both the convolution operation and self-attention mechanism has emerged, to exploit both the local and global image representations. These hybrid vision transformers, also referred to as CNN-Transformer architectures, have demonstrated remarkable results in vision applications. Given the rapidly growing number of hybrid vision transformers, it has become necessary to provide a taxonomy and explanation of these hybrid architectures. This survey presents a taxonomy of the recent vision transformer architectures and more specifically that of the hybrid vision transformers. Additionally, the key features of these architectures such as the attention mechanisms, positional embeddings, multi-scale processing, and convolution are also discussed. In contrast to the previous survey papers that are primarily focused on individual vision transformer architectures or CNNs, this survey uniquely emphasizes the emerging trend of hybrid vision transformers. By showcasing the potential of hybrid vision transformers to deliver exceptional performance across a range of computer vision tasks, this survey sheds light on the future directions of this rapidly evolving architecture.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Asifullah Khan",
  "date": "2023-05-17T01:27:27",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.669Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.669Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f2b"
  },
  "title": "Computers Should Be Uniters Not Dividers: A Vision of Computer-Enhanced Happy Future",
  "abstract": "This manifesto provides a vision of how computers can be used to bring people together, to enhance people's use of their natural creativity, and thus, make them happier.",
  "tags": [
    "Computers and Society"
  ],
  "author": "Alexander Titovets",
  "date": "2014-08-30T19:55:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.670Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.670Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f2c"
  },
  "title": "Cryptography: Classical versus Post-Quantum",
  "abstract": "The advantages of post-quantum cryptography over classical cryptography are covered in this survey. We address several post-quantum cryptography techniques. We conclude that the deployment of quantum-safe cryptographic systems is anticipated to be the future of secure communication, and that the development of post-quantum cryptography is essential to guarantee the security of sensitive information in the post quantum era.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Abhinav Awasthi",
  "date": "2024-02-16T10:56:45",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.671Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.671Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f2d"
  },
  "title": "Realization Scheme for Visual Cryptography with Computer-generated Holograms",
  "abstract": "We propose to realize visual cryptography in an indirect way with the help of computer-generated hologram. At present, the recovery method of visual cryptography is mainly superimposed on transparent film or superimposed by computer equipment, which greatly limits the application range of visual cryptography. In this paper, the shares of the visual cryptography were encoded with computer-generated hologram, and the shares is reproduced by optical means, and then superimposed and decrypted. This method can expand the application range of visual cryptography and further increase the security of visual cryptography.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Tao Yu",
  "date": "2022-12-10T04:59:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.672Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.672Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f2e"
  },
  "title": "Indexing Properties of Primitive Pythagorean Triples for Cryptography Applications",
  "abstract": "This paper presents new properties of Primitive Pythagorean Triples (PPT) that have relevance in applications where events of different probability need to be generated and in cryptography.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Yashwanth Kothapalli",
  "date": "2011-01-21T21:53:18",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.673Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.673Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f2f"
  },
  "title": "Probability theory and public-key cryptography",
  "abstract": "In this short note, we address a common misconception at the interface of probability theory and public-key cryptography.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Mariya Bessonov",
  "date": "2020-06-02T13:46:53",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.674Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.674Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f30"
  },
  "title": "Quantum-Resistant Cryptography",
  "abstract": "Quantum-resistant cryptography is cryptography that aims to deliver cryptographic functions and protocols that remain secure even if large-scale fault-tolerant quantum computers are built. NIST will soon announce the first selected public-key cryptography algorithms in its Post-Quantum Cryptography (PQC) standardization which is the most important current effort in the field of quantum-resistant cryptography. This report provides an overview to security experts who do not yet have a deep understanding of quantum-resistant cryptography. It surveys the computational model of quantum computers; the quantum algorithms that affect cryptography the most; the risk of Cryptographically Relevant Quantum Computers (CRQCs) being built; the security of symmetric and public-key cryptography in the presence of CRQCs; the NIST PQC standardization effort; the migration to quantum-resistant public-key cryptography; the relevance of Quantum Key Distribution as a complement to conventional cryptography; and the relevance of Quantum Random Number Generators as a complement to current hardware Random Number Generators.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "John Preuß Mattsson",
  "date": "2021-12-01T10:36:16",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.675Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.675Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f31"
  },
  "title": "Post-Quantum Cryptography",
  "abstract": "In this survey we propose to cover the prose of post-quantum cryptography over classical cryptography. We talk about the various cryptographic methods that are being practiced to safeguard our information. The future of secure communication is expected to be the implementation of quantum-safe cryptographic systems, and that in the post-quantum era, the development of post-quantum cryptography is essential for ensuring the security of sensitive data.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Pranjal",
  "date": "2024-02-16T11:04:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.676Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.676Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f32"
  },
  "title": "Modern Symmetric Cryptography methodologies and its applications",
  "abstract": "Nowadays, using cryptographic systems play an effective role in security and safety technologies. One of the most applied kind of cryptography is Symmetric Cryptography and its applications. New aspects of symmetric Cryptography methodologies and applications has been presented by this paper. Security-based networks and some complex technologies such as RFID and parallel security settings has been intro-duced by using Symmetric Cryptography is the main base of discussion in this paper. Designing an unique protocol for Symmetric Cryptography in security networks elements is our focus. Reviewing benefits of using these methodologies has been pre-sented and discussed in this paper.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Amin Daneshmand Malayeri",
  "date": "2009-12-06T11:07:40",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.677Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.677Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f33"
  },
  "title": "Post Quantum Cryptography and its Comparison with Classical Cryptography",
  "abstract": "Cryptography plays a pivotal role in safeguarding sensitive information and facilitating secure communication. Classical cryptography relies on mathematical computations, whereas quantum cryptography operates on the principles of quantum mechanics, offering a new frontier in secure communication. Quantum cryptographic systems introduce novel dimensions to security, capable of detecting and thwarting eavesdropping attempts. By contrasting quantum cryptography with its classical counterpart, it becomes evident how quantum mechanics revolutionizes the landscape of secure communication.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Tanmay Tripathi",
  "date": "2024-03-28T10:38:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.678Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.678Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f34"
  },
  "title": "ChatGPT's Potential in Cryptography Misuse Detection: A Comparative Analysis with Static Analysis Tools",
  "abstract": "The correct adoption of cryptography APIs is challenging for mainstream developers, often resulting in widespread API misuse. Meanwhile, cryptography misuse detectors have demonstrated inconsistent performance and remain largely inaccessible to most developers. We investigated the extent to which ChatGPT can detect cryptography misuses and compared its performance with that of the state-of-the-art static analysis tools. Our investigation, mainly based on the CryptoAPI-Bench benchmark, demonstrated that ChatGPT is effective in identifying cryptography API misuses, and with the use of prompt engineering, it can even outperform leading static cryptography misuse detectors.",
  "tags": [
    "Cryptography and Security",
    "Artificial Intelligence"
  ],
  "author": "Ehsan Firouzi",
  "date": "2024-09-10T14:50:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.679Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.679Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f35"
  },
  "title": "A Pseudo DNA Cryptography Method",
  "abstract": "The DNA cryptography is a new and very promising direction in cryptography research. DNA can be used in cryptography for storing and transmitting the information, as well as for computation. Although in its primitive stage, DNA cryptography is shown to be very effective. Currently, several DNA computing algorithms are proposed for quite some cryptography, cryptanalysis and steganography problems, and they are very powerful in these areas. However, the use of the DNA as a means of cryptography has high tech lab requirements and computational limitations, as well as the labor intensive extrapolation means so far. These make the efficient use of DNA cryptography difficult in the security world now. Therefore, more theoretical analysis should be performed before its real applications.   In this project, We do not intended to utilize real DNA to perform the cryptography process; rather, We will introduce a new cryptography method based on central dogma of molecular biology. Since this method simulates some critical processes in central dogma, it is a pseudo DNA cryptography method. The theoretical analysis and experiments show this method to be efficient in computation, storage and transmission; and it is very powerful against certain attacks. Thus, this method can be of many uses in cryptography, such as an enhancement insecurity and speed to the other cryptography methods. There are also extensions and variations to this method, which have enhanced security, effectiveness and applicability.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Kang Ning",
  "date": "2009-03-16T04:22:45",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.680Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.680Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f36"
  },
  "title": "Quantum Cryptography for Enhanced Network Security: A Comprehensive Survey of Research, Developments, and Future Directions",
  "abstract": "With the ever-growing concern for internet security, the field of quantum cryptography emerges as a promising solution for enhancing the security of networking systems. In this paper, 20 notable papers from leading conferences and journals are reviewed and categorized based on their focus on various aspects of quantum cryptography, including key distribution, quantum bit commitment, post quantum cryptography, and counterfactual quantum key distribution. The paper explores the motivations and challenges of employing quantum cryptography, addressing security and privacy concerns along with existing solutions. Secure key distribution, a critical component in ensuring the confidentiality and integrity of transmitted information over a network, is emphasized in the discussion. The survey examines the potential of quantum cryptography to enable secure key exchange between parties, even when faced with eavesdropping, and other applications of quantum cryptography. Additionally, the paper analyzes the methodologies, findings, and limitations of each reviewed study, pinpointing trends such as the increasing focus on practical implementation of quantum cryptography protocols and the growing interest in postquantum cryptography research. Furthermore, the survey identifies challenges and open research questions, including the need for more efficient quantum repeater networks, improved security proofs for continuous variable quantum key distribution, and the development of quantum resistant cryptographic algorithms.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Mst Shapna Akter",
  "date": "2023-06-02T23:07:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.681Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.681Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f37"
  },
  "title": "The dangerous path towards your own cryptography method",
  "abstract": "Would you like to have your own cryptography method? Experts say you should not do it. If you think you can develop a better cryptography method anyway. We present a brief discussion about some well known cryptography methods and how our model fails against the traditional attacks. We do not want to discourage anybody, we just want to show that, despite of the importance of developing better cryptography models, it is a very hard task.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Warley M. S. Alves",
  "date": "2018-09-26T13:52:28",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.682Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.682Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f38"
  },
  "title": "From Golden to Unimodular Cryptography",
  "abstract": "We introduce a natural generalization of the golden cryptography, which uses general unimodular matrices in place of the traditional Q-matrices, and prove that it preserves the original error correction properties of the encryption. Moreover, the additional parameters involved in generating the coding matrices make this unimodular cryptography resilient to the chosen plaintext attacks that worked against the golden cryptography. Finally, we show that even the golden cryptography is generally unable to correct double errors in the same row of the ciphertext matrix, and offer an additional check number which, if transmitted, allows for the correction.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Sergiy Koshkin",
  "date": "2019-02-05T00:07:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.683Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.683Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f39"
  },
  "title": "Cryptography Vulnerabilities on HackerOne",
  "abstract": "Previous studies have shown that cryptography is hard for developers to use and misusing cryptography leads to severe security vulnerabilities. We studied relevant vulnerability reports on the HackerOne bug bounty platform to understand what types of cryptography vulnerabilities exist in the wild. We extracted eight themes of vulnerabilities from the vulnerability reports and discussed their real-world implications and mitigation strategies. We hope that our findings alert developers, familiarize them with the dire consequences of cryptography misuses, and support them in avoiding such mistakes.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Mohammadreza Hazhirpasand",
  "date": "2021-11-06T11:43:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.684Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.684Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f3a"
  },
  "title": "Software-Defined Cryptography: A Design Feature of Cryptographic Agility",
  "abstract": "Given the widespread use of cryptography in Enterprise IT, migration to post-quantum cryptography (PQC) is not drop-in replacement at all. Cryptographic agility, or crypto-agility, is a design feature that enables seamless updates to new cryptographic algorithms and standards without the need to modify or replace the surrounding infrastructure. This paper introduces a notion of software-defined cryptography as the desired design feature for crypto-agility, emphasizing the role of software in providing centralized governance for cryptography and automated enforcement of cryptographic policies, such as migration to PQC.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Jihoon Cho",
  "date": "2024-04-02T10:11:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.685Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.685Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f3b"
  },
  "title": "Trusted Certificates in Quantum Cryptography",
  "abstract": "This paper analyzes the performance of Kak's three stage quantum cryptographic protocol based on public key cryptography against a man-in-the-middle attack. A method for protecting against such an attack is presented using certificates distributed by a trusted third party.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "William Perkins",
  "date": "2006-03-10T23:05:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.686Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.686Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f3c"
  },
  "title": "Implementing the Three-Stage Quantum Cryptography Protocol",
  "abstract": "We present simple implementations of Kak's three-stage quantum cryptography protocol. The case where the transformation is applied to more than one qubit at the same time is also considered.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Priya Sivakumar",
  "date": "2006-03-16T22:20:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.687Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.687Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f3d"
  },
  "title": "Full Restoration of Visual Encrypted Color Images",
  "abstract": "While strictly black and white images have been the basis for visual cryptography, there has been a lack of an easily implemented format for colour images. This paper establishes a simple, yet secure way of implementing visual cryptography with colour, assuming a binary data representation.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Simeon Persson",
  "date": "2011-11-18T18:33:50",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.688Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.688Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f3e"
  },
  "title": "Intensity and State Estimation in Quantum Cryptography",
  "abstract": "This paper describes how the communicating parties can employ intensity and state estimation to detect if the eavesdropper has siphoned off and injected photons in the received communication. This is of relevance in quantum cryptography based on random rotations of photon polarizations.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Sindhu Chitikela",
  "date": "2013-02-07T18:22:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.689Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.689Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f3f"
  },
  "title": "Some application of difference equations in Cryptography and Coding Theory",
  "abstract": "In this paper, we present some applications of a difference equation of degree k in Cryptography and Coding Theory.",
  "tags": [
    "Information Theory",
    "Cryptography and Security"
  ],
  "author": "Cristina Flaut",
  "date": "2018-02-08T08:46:47",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.690Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.690Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f40"
  },
  "title": "Problems in group theory motivated by cryptography",
  "abstract": "This is a survey of algorithmic problems in group theory, old and new, motivated by applications to cryptography.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Vladimir Shpilrain",
  "date": "2018-02-20T19:33:20",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.691Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.691Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f41"
  },
  "title": "A Note on the Bellare-Rivest Protocol for Translucent Cryptography",
  "abstract": "We remark that the Bellare-Rivest protocol for translucent cryptography [J. Cryptology (1999) 12: 117-139] can not truly enable the government to decrypt partial encrypted communications.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Zhengjun Cao",
  "date": "2014-08-15T00:49:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.692Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.692Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f42"
  },
  "title": "Breaking the Hidden Irreducible Polynomials Scheme",
  "abstract": "In 2019 G\\'omez described a new public key cryptography scheme based on ideas from multivariate public key cryptography using hidden irreducible polynomials. We show that the scheme's design has a flaw which lets an attacker recover the private key directly from the public key.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Christian Eder",
  "date": "2019-11-05T14:48:56",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.693Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.693Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f43"
  },
  "title": "A Comparison of Cryptography Courses",
  "abstract": "The author taught two courses on cryptography, one at Duke University aimed at non-mathematics majors and one at Rose-Hulman Institute of Technology aimed at mathematics and computer science majors. Both tried to incorporate technical and societal aspects of cryptography, with varying emphases. This paper will discuss the strengths and weaknesses of both courses and compare the differences in the author's approach.",
  "tags": [
    "Cryptography and Security",
    "Computers and Society"
  ],
  "author": "Joshua Holden",
  "date": "2004-01-03T20:11:07",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.694Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.694Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f44"
  },
  "title": "Considerations for Cloud Security Operations",
  "abstract": "Information Security in Cloud Computing environments is explored. Cloud Computing is presented, security needs are discussed, and mitigation approaches are listed. Topics covered include Information Security, Cloud Computing, Private Cloud, Public Cloud, SaaS, PaaS, IaaS, ISO 27001, OWASP, Secure SDLC.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "James Cusick",
  "date": "2016-01-23T17:08:22",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.695Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.695Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f45"
  },
  "title": "Security of Cloud FPGAs: A Survey",
  "abstract": "Integrating Field Programmable Gate Arrays (FPGAs) with cloud computing instances is a rapidly emerging trend on commercial cloud computing platforms such as Amazon Web Services (AWS), Huawei cloud, and Alibaba cloud. Cloud FPGAs allow cloud users to build hardware accelerators to speed up the computation in the cloud. However, since the cloud FPGA technology is still in its infancy, the security implications of this integration of FPGAs in the cloud are not clear. In this paper, we survey the emerging field of cloud FPGA security, providing a comprehensive overview of the security issues related to cloud FPGAs, and highlighting future challenges in this research area.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Chenglu Jin",
  "date": "2020-05-11T05:31:15",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.696Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.696Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f46"
  },
  "title": "Interoperability and Standardization of Intercloud Cloud Computing",
  "abstract": "Cloud computing is getting mature, and the interoperability and standardization of the clouds is still waiting to be solved. This paper discussed the interoperability among clouds about message transmission, data transmission and virtual machine transfer. Starting from IEEE Pioneering Cloud Computing Initiative, this paper discussed about standardization of the cloud computing, especially intercloud cloud computing. This paper also discussed the standardization from the market-oriented view.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Jingxin K. Wang",
  "date": "2012-12-24T19:24:35",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.697Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.697Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f47"
  },
  "title": "Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management",
  "abstract": "In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low resource utilization and unbalanced load in the cloud environment, this study proposes a comprehensive solution, including optimization methods such as deep learning and genetic algorithm, to improve system performance and efficiency, and thus bring new breakthroughs and progress in the field of cloud computing resource management.Rational allocation of resources plays a crucial role in cloud computing. In the resource allocation of cloud computing, the cloud computing center has limited cloud resources, and users arrive in sequence. Each user requests the cloud computing center to use a certain number of cloud resources at a specific time.",
  "tags": [
    "Distributed Computing",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Yifan Zhang",
  "date": "2024-02-27T05:14:27",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.698Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.698Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f48"
  },
  "title": "Evolution of Cloud Storage as Cloud Computing Infrastructure Service",
  "abstract": "Enterprises are driving towards less cost, more availability, agility, managed risk - all of which is accelerated towards Cloud Computing. Cloud is not a particular product, but a way of delivering IT services that are consumable on demand, elastic to scale up and down as needed, and follow a pay-for-usage model. Out of the three common types of cloud computing service models, Infrastructure as a Service (IaaS) is a service model that provides servers, computing power, network bandwidth and Storage capacity, as a service to their subscribers. Cloud can relate to many things but without the fundamental storage pieces, which is provided as a service namely Cloud Storage, none of the other applications is possible. This paper introduces Cloud Storage, which covers the key technologies in cloud computing and Cloud Storage, management insights about cloud computing, different types of cloud services, driving forces of cloud computing and cloud storage, advantages and challenges of cloud storage and concludes by pinpointing few challenges to be addressed by the cloud storage providers.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Arokia Paul Rajan",
  "date": "2013-08-05T06:11:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.699Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.699Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f49"
  },
  "title": "A Survey on Cloud Security Issues and Techniques",
  "abstract": "Today, cloud computing is an emerging way of computing in computer science. Cloud computing is a set of resources and services that are offered by the network or internet. Cloud computing extends various computing techniques like grid computing, distributed computing. Today cloud computing is used in both industrial field and academic field. Cloud facilitates its users by providing virtual resources via internet. As the field of cloud computing is spreading the new techniques are developing. This increase in cloud computing environment also increases security challenges for cloud developers. Users of cloud save their data in the cloud hence the lack of security in cloud can lose the users trust. In this paper we will discuss some of the cloud security issues in various aspects like multi-tenancy, elasticity, availability etc. The paper also discuss existing security techniques and approaches for a secure cloud. This paper will enable researchers and professionals to know about different security threats and models and tools proposed.",
  "tags": [
    "Distributed Computing",
    "Cryptography and Security"
  ],
  "author": "Shubhanjali Sharma",
  "date": "2014-03-22T08:49:30",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.700Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.700Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f4a"
  },
  "title": "Discussion of various models related to cloud performance",
  "abstract": "This paper discusses the various models related to cloud computing. Knowing the metrics related to infrastructure is very critical to enhance the performance of cloud services. Various metrics related to clouds such as pageview response time, admission control and enforcing elasticity to cloud infrastructure are very crucial in analyzing the characteristics of the cloud to enhance the cloud performance.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Chaitanya Krishna Kande",
  "date": "2015-05-01T18:10:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.701Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.701Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f4b"
  },
  "title": "A Comparative Study of Load Balancing Algorithms in Cloud Computing Environment",
  "abstract": "Cloud Computing is a new trend emerging in IT environment with huge requirements of infrastructure and resources. Load Balancing is an important aspect of cloud computing environment. Efficient load balancing scheme ensures efficient resource utilization by provisioning of resources to cloud users on demand basis in pay as you say manner. Load Balancing may even support prioritizing users by applying appropriate scheduling criteria. This paper presents various load balancing schemes in different cloud environment based on requirements specified in Service Level Agreement (SLA).",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Mayanka Katyal",
  "date": "2014-03-27T05:07:28",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.702Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.702Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f4c"
  },
  "title": "Resource Management in Cloud Computing: Classification and Taxonomy",
  "abstract": "Cloud Computing is a new era of remote computing / Internet based computing where one can access their personal resources easily from any computer through Internet. Cloud delivers computing as a utility as it is available to the cloud consumers on demand. It is a simple pay-per-use consumer-provider service model. It contains large number of shared resources. So Resource Management is always a major issue in cloud computing like any other computing paradigm. Due to the availability of finite resources it is very challenging for cloud providers to provide all the requested resources. From the cloud providers perspective cloud resources must be allocated in a fair and efficient manner. Research Survey is not available from the perspective of resource management as a process in cloud computing. So this research paper provides a detailed sequential view / steps on resource management in cloud computing. Firstly this research paper classifies various resources in cloud computing. It also gives taxonomy on resource management in cloud computing through which one can do further research. Lastly comparisons on various resource management algorithms has been presented.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Swapnil M Parikh",
  "date": "2017-02-24T11:39:59",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.703Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.703Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f4d"
  },
  "title": "Securing the Data in Clouds with Hyperelliptic Curve Cryptography",
  "abstract": "In todays world, Cloud computing has attracted research communities as it provides services in reduced cost due to virtualizing all the necessary resources. Even modern business architecture depends upon Cloud computing .As it is a internet based utility, which provides various services over a network, it is prone to network based attacks. Hence security in clouds is the most important in case of cloud computing. Cloud Security concerns the customer to fully rely on storing data on clouds. That is why Cloud security has attracted attention of the research community. This paper will discuss securing the data in clouds by implementing key agreement, encryption and signature verification/generation with hyperelliptic curve cryptography.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Debajyoti Mukhopadhyay",
  "date": "2014-11-25T08:56:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.704Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.704Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f4e"
  },
  "title": "A Survey on Cloud Computing Security",
  "abstract": "Computation encounter the new approach of cloud computing which maybe keeps the world and possibly can prepare all the human's necessities. In other words, cloud computing is the subsequent regular step in the evolution of on-demand information technology services and products. The Cloud is a metaphor for the Internet and is a concept for the covered complicated infrastructure; it also depends on sketching in computer network diagrams. In this paper we will focus on concept of cloud computing, cloud deployment models, cloud security challenges encryption and data protection, privacy and security and data management and movement from grid to cloud.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Hero Modares",
  "date": "2012-06-24T07:20:46",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.705Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.705Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f4f"
  },
  "title": "A Preliminary Study On Emerging Cloud Computing Security Challenges",
  "abstract": "Cloud computing is the internet based provisioning of the computing resources, software, and information on demand. Cloud Computing is referred to as one of most recent emerging paradigms of computing utilities. Since Cloud computing is the dominant infrastructure of the shared services over the internet, it is important to be aware of the security risk and the challenges associated with this emerging computing paradigm. This survey provides a brief introduction to the cloud computing, its major characteristics, and service models. It also explores cloud security threats, lists a few security solutions , and proposes a promsing research direction to deal with the evolving security challenges in Cloud computing.",
  "tags": [
    "Cryptography and Security",
    "Distributed Computing"
  ],
  "author": "Babin Bhandari",
  "date": "2018-08-13T10:45:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.706Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.706Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f50"
  },
  "title": "Framework for cloud computing adoption: A road map for Smes to cloud migration",
  "abstract": "Small and Medium size Enterprises (SME) are considered as a backbone of many developing and developed economies of the world; they are the driving force to any major economy across the globe. Through Cloud Computing firms outsource their entire information technology (IT) process while concentrating more on their core business. It allows businesses to cut down heavy cost incurred over IT infrastructure without losing focus on customer needs. However, Cloud industry to an extent has struggled to grow among SMEs due to the reluctance and concerns expressed by them. Throughout the course of this study several interviews were conducted and the literature was reviewed to understand how cloud providers offer services and what challenges SMEs are facing. The study identified issues like cloud knowledge, interoperability, security and contractual concerns to be hindering SMEs adoption of cloud services. From the interviews common practices followed by cloud vendors and what concerns SMEs have were identified as a basis for a cloud framework which will bridge gaps between cloud vendors and SMEs. A stepwise framework for cloud adoption is formulated which identifies and provides recommendation to four most predominant challenges which are hurting cloud industry and taking SMEs away from cloud computing, as well as guide SMEs aiding in successful cloud adoption. Moreover, this framework streamlines the cloud adoption process for SMEs by removing ambiguity in regards to fundamentals associated with their organisation and cloud adoption process.",
  "tags": [
    "Computers and Society"
  ],
  "author": "Nabeel Khan",
  "date": "2016-01-07T17:21:41",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.707Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.707Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f51"
  },
  "title": "Usage of Cloud Computing Simulators and Future Systems For Computational Research",
  "abstract": "Cloud Computing is an Internet based computing, whereby shared resources, software and information, are provided to computers and devices on demand, like the electricity grid. Currently, IaaS (Infrastructure as a Service), PaaS (Platform as a Service) and SaaS (Software as a Service) are used as a business model for Cloud Computing. Nowadays, the adoption and deployment of Cloud Computing is increasing in various domains, forcing researchers to conduct research in the area of Cloud Computing globally. Setting up the research environment is critical for the researchers in the developing countries to evaluate the research outputs. Currently, modeling, simulation technology and access of resources from various university data centers has become a useful and powerful tool in cloud computing research. Several cloud simulators have been specifically developed by various universities to carry out Cloud Computing research, including CloudSim, SPECI, Green Cloud and Future Systems (the Indiana University machines India, Bravo, Delta, Echo and Foxtrot) supports leading edge data science research and a broad range of computing-enabled education as well as integration of ideas from cloud and HPC systems. In this paper, the features, suitability, adaptability and the learning curve of the existing Cloud Computing simulators and Future Systems are reviewed and analyzed.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Ramkumar Lakshminarayanan",
  "date": "2016-04-30T09:30:14",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.708Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.708Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f52"
  },
  "title": "Is Cloud Computing Steganography-proof?",
  "abstract": "The paper focuses on characterisation of information hiding possibilities in Cloud Computing. After general introduction to cloud computing and its security we move to brief description of steganography. In particular we introduce classification of steganographic communication scenarios in cloud computing which is based on location of the steganograms receiver. These scenarios as well as the threats that steganographic methods can cause must be taken into account when designing secure cloud computing services.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Wojciech Mazurczyk",
  "date": "2011-07-20T19:18:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.709Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.709Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f53"
  },
  "title": "SecLaaS: Secure Logging-as-a-Service for Cloud Forensics",
  "abstract": "Cloud computing has emerged as a popular computing paradigm in recent years. However, today's cloud computing architectures often lack support for computer forensic investigations. Analyzing various logs (e.g., process logs, network logs) plays a vital role in computer forensics. Unfortunately, collecting logs from a cloud is very hard given the black-box nature of clouds and the multi-tenant cloud models, where many users share the same processing and network resources. Researchers have proposed using log API or cloud management console to mitigate the challenges of collecting logs from cloud infrastructure. However, there has been no concrete work, which shows how to provide cloud logs to investigator while preserving users' privacy and integrity of the logs. In this paper, we introduce Secure-Logging-as-a-Service (SecLaaS), which stores virtual machines' logs and provides access to forensic investigators ensuring the confidentiality of the cloud users. Additionally, SeclaaS preserves proofs of past log and thus protects the integrity of the logs from dishonest investigators or cloud providers. Finally, we evaluate the feasibility of the scheme by implementing SecLaaS for network access logs in OpenStack - a popular open source cloud platform.",
  "tags": [
    "Cryptography and Security",
    "Distributed Computing"
  ],
  "author": "Shams Zawoad",
  "date": "2013-02-25T22:36:06",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.710Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.710Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f54"
  },
  "title": "soCloud: A service-oriented component-based PaaS for managing portability, provisioning, elasticity, and high availability across multiple clouds",
  "abstract": "Multi-cloud computing is a promising paradigm to support very large scale world wide distributed applications. Multi-cloud computing is the usage of multiple, independent cloud environments, which assumed no priori agreement between cloud providers or third party. However, multi-cloud computing has to face several key challenges such as portability, provisioning, elasticity, and high availability. Developers will not only have to deploy applications to a specific cloud, but will also have to consider application portability from one cloud to another, and to deploy distributed applications spanning multiple clouds. This article presents soCloud a service-oriented component-based Platform as a Service (PaaS) for managing portability, elasticity, provisioning, and high availability across multiple clouds. soCloud is based on the OASIS Service Component Architecture (SCA) standard in order to address portability. soCloud provides services for managing provisioning, elasticity, and high availability across multiple clouds. soCloud has been deployed and evaluated on top of ten existing cloud providers: Windows Azure, DELL KACE, Amazon EC2, CloudBees, OpenShift, dotCloud, Jelastic, Heroku, Appfog, and an Eucalyptus private cloud.",
  "tags": [
    "Software Engineering",
    "Distributed Computing"
  ],
  "author": "Fawaz Paraiso",
  "date": "2014-07-08T06:20:48",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.711Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.711Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f55"
  },
  "title": "Cloud Computing and Grid Computing 360-Degree Compared",
  "abstract": "Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Ian Foster",
  "date": "2008-12-31T19:13:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.712Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.712Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f56"
  },
  "title": "Cloud Adoption A Modern Approach",
  "abstract": "Todays Information Technology world is cloud-centric. Companies are intrigued to migrate their workload private cloud from on-premise Datacenter to Public cloud to take advantage of the latest innovations. It drives the business growth and competitiveness of the organization. At the same time, it is important for Enterprise Architects to understand the drawbacks and challenges to migrate the workload to Cloud. This paper aims to identify the key factors to migrate the workload to the cloud. It also helps an organization to identify the candidate for cloud migration. An impulsive decision to move to the Cloud may be detrimental to an organization. Also, I will discuss one case study to see the benefits and disadvantages of cloud migration. This will help the organization to maximize its ROI.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Subhadip Kumar",
  "date": "2023-05-16T20:26:22",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.713Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.713Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f57"
  },
  "title": "Cloud Security and Security Challenges Revisited",
  "abstract": "In recent years, Cloud Computing has transformed local businesses and created new business models on the Internet- and Cloud services are still flourishing. But after the emphatic hype in the early years, a more realistic perception of Cloud services has emerged. One reason for this surely is that today, Cloud Computing is considered as an established and well-accepted technology and no longer as a technical novelty. But the second reason for this assessment might also be numerous security issues that Cloud Computing in general or specific Cloud services have experienced since then. In this paper, we revisit attacks on Cloud services and Cloud-related attack vectors that have been published in recent years. We then consider successful or proposed solutions to cope with these challenges. Based on these findings, we apply a security metric in order to rank all these Cloud-related security challenges concerning their severity. This should assist security professionals to prioritize their efforts toward addressing these issues.",
  "tags": [
    "Cryptography and Security",
    "Distributed Computing"
  ],
  "author": "Fabian Süß",
  "date": "2024-05-18T17:42:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.715Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.715Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f58"
  },
  "title": "Application of Ontologies in Cloud Computing: The State-Of-The-Art",
  "abstract": "This paper presents a systematic survey on existing literature and seminal works relevant to the application of ontologies in different aspects of Cloud computing. Our hypothesis is that ontologies along with their reasoning capabilities can have significant impact on improving various aspects of the Cloud computing phenomena. Ontologies can promote intelligent decision support mechanisms for various Cloud based services. They can also provide effective interoperability among the Cloud based systems and resources. This survey can promote a comprehensive understanding on the roles and significance of ontologies within the overall domain of Cloud Computing. Also, this project can potentially form the basis of new research area and possibilities for both ontology and Cloud computing communities.",
  "tags": [
    "Distributed Computing",
    "Artificial Intelligence"
  ],
  "author": "Fahim T. Imam",
  "date": "2016-10-06T05:39:37",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.716Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.716Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f59"
  },
  "title": "A Slow Read attack Using Cloud",
  "abstract": "Cloud computing relies on sharing computing resources rather than having local servers or personal devices to handle applications. Nowadays, cloud computing has become one of the fastest growing fields in information technology. However, several new security issues of cloud computing have emerged due to its service delivery models. In this paper, we discuss the case of distributed denial-of-service (DDoS) attack using Cloud resources. First, we show how such attack using a cloud platform could not be detected by previous techniques. Then we present a tricky solution based on the cloud as well.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Darine Ameyed",
  "date": "2017-12-05T21:41:00",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.717Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.717Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f5a"
  },
  "title": "KCES: A Workflow Containerization Scheduling Scheme Under Cloud-Edge Collaboration Framework",
  "abstract": "As more IoT applications gradually move towards the cloud-edge collaborative mode, the containerized scheduling of workflows extends from the cloud to the edge. However, given the high delay of the communication network, loose coupling of structure, and resource heterogeneity between cloud and edge, workflow containerization scheduling in the cloud-edge scenarios faces the difficulty of resource coordination and application collaboration management. To address these two issues, we propose a KubeEdge-Cloud-Edge-Scheduling scheme named KCES, a workflow containerization scheduling scheme for the KubeEdge cloud-edge framework. The KCES includes a cloud-edge workflow scheduling engine for KubeEdge and workflow scheduling strategies for task horizontal roaming and vertical offloading. Considering the scheduling optimization of cloud-edge workflows, this paper proposes a cloud-edge workflow scheduling model and cloud-edge node model and designs a cloud-edge workflow scheduling engine to maximize cloud-edge resource utilization under the constraint of workflow task delay. A cloud-edge resource hybrid management technology is used to design the cloud-edge resource evaluation and resource allocation algorithms to achieve cloud-edge resource collaboration. Based on the ideas of distributed functional roles and the hierarchical division of computing power, the horizontal roaming among the edges and vertical offloading strategies between the cloud and edges for workflow tasks are designed to realize the cloud-edge application collaboration. Through a customized IoT application workflow instance, experimental results show that KCES is superior to the baseline in total workflow duration, average workflow duration, and resource usage and has the capabilities of horizontal roaming and vertical offloading for workflow tasks.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Chenggang Shan",
  "date": "2024-01-02T14:11:24",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.718Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.718Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f5b"
  },
  "title": "Deep Learning in Bioinformatics",
  "abstract": "In the era of big data, transformation of biomedical big data into valuable knowledge has been one of the most important challenges in bioinformatics. Deep learning has advanced rapidly since the early 2000s and now demonstrates state-of-the-art performance in various fields. Accordingly, application of deep learning in bioinformatics to gain insight from data has been emphasized in both academia and industry. Here, we review deep learning in bioinformatics, presenting examples of current research. To provide a useful and comprehensive perspective, we categorize research both by the bioinformatics domain (i.e., omics, biomedical imaging, biomedical signal processing) and deep learning architecture (i.e., deep neural networks, convolutional neural networks, recurrent neural networks, emergent architectures) and present brief descriptions of each study. Additionally, we discuss theoretical and practical issues of deep learning in bioinformatics and suggest future research directions. We believe that this review will provide valuable insights and serve as a starting point for researchers to apply deep learning approaches in their bioinformatics studies.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Seonwoo Min",
  "date": "2016-03-21T13:55:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.719Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.719Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f5c"
  },
  "title": "Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review",
  "abstract": "The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future developments.",
  "tags": [
    "Other Quantitative Biology",
    "Artificial Intelligence"
  ],
  "author": "Jinge Wang",
  "date": "2024-03-22T15:16:23",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.720Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.720Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f5d"
  },
  "title": "Generalized Centroid Estimators in Bioinformatics",
  "abstract": "In a number of estimation problems in bioinformatics, accuracy measures of the target problem are usually given, and it is important to design estimators that are suitable to those accuracy measures. However, there is often a discrepancy between an employed estimator and a given accuracy measure of the problem. In this study, we introduce a general class of efficient estimators for estimation problems on high-dimensional binary spaces, which representmany fundamental problems in bioinformatics. Theoretical analysis reveals that the proposed estimators generally fit with commonly-used accuracy measures (e.g. sensitivity, PPV, MCC and F-score) as well as it can be computed efficiently in many cases, and cover a wide range of problems in bioinformatics from the viewpoint of the principle of maximum expected accuracy (MEA). It is also shown that some important algorithms in bioinformatics can be interpreted in a unified manner. Not only the concept presented in this paper gives a useful framework to design MEA-based estimators but also it is highly extendable and sheds new light on many problems in bioinformatics.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Michiaki Hamada",
  "date": "2013-05-19T07:50:14",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.721Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.721Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f5e"
  },
  "title": "Cellular Automata and Its Applications in Bioinformatics: A Review",
  "abstract": "This paper aims at providing a survey on the problems that can be easily addressed by cellular automata in bioinformatics. Some of the authors have proposed algorithms for addressing some problems in bioinformatics but the application of cellular automata in bioinformatics is a virgin field in research. None of the researchers has tried to relate the major problems in bioinformatics and find a common solution. Extensive literature surveys were conducted. We have considered some papers in various journals and conferences for conduct of our research. This paper provides intuition towards relating various problems in bioinformatics logically and tries to attain a common frame work for addressing the same.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Pokkuluri Kiran Sree",
  "date": "2014-04-02T04:18:06",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.722Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.722Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f5f"
  },
  "title": "An Evaluation of Large Language Models in Bioinformatics Research",
  "abstract": "Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics.",
  "tags": [
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Hengchuang Yin",
  "date": "2024-02-21T11:27:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.723Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.723Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f60"
  },
  "title": "Diffusion Models in Bioinformatics: A New Wave of Deep Learning Revolution in Action",
  "abstract": "Denoising diffusion models have emerged as one of the most powerful generative models in recent years. They have achieved remarkable success in many fields, such as computer vision, natural language processing (NLP), and bioinformatics. Although there are a few excellent reviews on diffusion models and their applications in computer vision and NLP, there is a lack of an overview of their applications in bioinformatics. This review aims to provide a rather thorough overview of the applications of diffusion models in bioinformatics to aid their further development in bioinformatics and computational biology. We start with an introduction of the key concepts and theoretical foundations of three cornerstone diffusion modeling frameworks (denoising diffusion probabilistic models, noise-conditioned scoring networks, and stochastic differential equations), followed by a comprehensive description of diffusion models employed in the different domains of bioinformatics, including cryo-EM data enhancement, single-cell data analysis, protein design and generation, drug and small molecule design, and protein-ligand interaction. The review is concluded with a summary of the potential new development and applications of diffusion models in bioinformatics.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence"
  ],
  "author": "Zhiye Guo",
  "date": "2023-02-13T15:37:23",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.724Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.724Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f61"
  },
  "title": "Fighting against uncertainty: An essential issue in bioinformatics",
  "abstract": "Many bioinformatics problems, such as sequence alignment, gene prediction, phylogenetic tree estimation and RNA secondary structure prediction, are often affected by the \"uncertainty\" of a solution; that is, the probability of the solution is extremely small. This situation arises for estimation problems on high-dimensional discrete spaces in which the number of possible discrete solutions is immense. In the analysis of biological data or the development of prediction algorithms, this uncertainty should be handled carefully and appropriately. In this review, I will explain several methods to combat this uncertainty, presenting a number of examples in bioinformatics. The methods include (i) avoiding point estimation, (ii) maximum expected accuracy (MEA) estimations, and (iii) several strategies to design a pipeline involving several prediction methods. I believe that the basic concepts and ideas described in this review will be generally useful for estimation problems in various areas of bioinformatics.",
  "tags": [
    "Biomolecules"
  ],
  "author": "Michiaki Hamada",
  "date": "2013-05-15T23:49:59",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.725Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.725Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f62"
  },
  "title": "Multiple Attractor Cellular Automata (MACA) for Addressing Major Problems in Bioinformatics",
  "abstract": "CA has grown as potential classifier for addressing major problems in bioinformatics. Lot of bioinformatics problems like predicting the protein coding region, finding the promoter region, predicting the structure of protein and many other problems in bioinformatics can be addressed through Cellular Automata. Even though there are some prediction techniques addressing these problems, the approximate accuracy level is very less. An automated procedure was proposed with MACA (Multiple Attractor Cellular Automata) which can address all these problems. The genetic algorithm is also used to find rules with good fitness values. Extensive experiments are conducted for reporting the accuracy of the proposed tool. The average accuracy of MACA when tested with ENCODE, BG570, HMR195, Fickett and Tongue, ASP67 datasets is 78%.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Pokkuluri Kiran Sree",
  "date": "2013-10-16T15:01:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.726Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.726Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f63"
  },
  "title": "An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated MACA based Clonal Classifier for Protein Coding and Promoter Region Prediction)",
  "abstract": "This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA has created good impact on solving major problems in bioinformatics like protein region identification and promoter region prediction with less time (Pokkuluri Kiran Sree, 2014). This AIS-INMACA is now came with several variations (Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformatics for solving many problems in bioinformatics. So this paper will be very much useful for so many researchers who are working in the domain of bioinformatics with cellular automata.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Pokkuluri Kiran Sree",
  "date": "2014-03-06T03:46:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.727Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.727Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f64"
  },
  "title": "Toward Scalable Machine Learning and Data Mining: the Bioinformatics Case",
  "abstract": "In an effort to overcome the data deluge in computational biology and bioinformatics and to facilitate bioinformatics research in the era of big data, we identify some of the most influential algorithms that have been widely used in the bioinformatics community. These top data mining and machine learning algorithms cover classification, clustering, regression, graphical model-based learning, and dimensionality reduction. The goal of this study is to guide the focus of scalable computing experts in the endeavor of applying new storage and scalable computation designs to bioinformatics algorithms that merit their attention most, following the engineering maxim of \"optimize the common case\".",
  "tags": [
    "Distributed Computing",
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Faraz Faghri",
  "date": "2017-09-29T22:29:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.728Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.728Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f65"
  },
  "title": "Preface to Introduction to Protein Structural Bioinformatics",
  "abstract": "While many good textbooks are available on Protein Structure, Molecular Simulations, Thermodynamics and Bioinformatics methods in general, there is no good introductory level book for the field of Structural Bioinformatics. This book aims to give an introduction into Structural Bioinformatics, which is where the previous topics meet to explore three dimensional protein structures through computational analysis. We provide an overview of existing computational techniques, to validate, simulate, predict and analyse protein structures. More importantly, it will aim to provide practical knowledge about how and when to use such techniques. We will consider proteins from three major vantage points: Protein structure quantification, Protein structure prediction, and Protein simulation & dynamics.",
  "tags": [
    "Biomolecules"
  ],
  "author": "K. Anton Feenstra",
  "date": "2018-01-29T10:54:27",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.729Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.729Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f66"
  },
  "title": "Bioinformatics and Medicine in the Era of Deep Learning",
  "abstract": "Many of the current scientific advances in the life sciences have their origin in the intensive use of data for knowledge discovery. In no area this is so clear as in bioinformatics, led by technological breakthroughs in data acquisition technologies. It has been argued that bioinformatics could quickly become the field of research generating the largest data repositories, beating other data-intensive areas such as high-energy physics or astroinformatics. Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large. In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Davide Bacciu",
  "date": "2018-02-27T09:41:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.730Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.730Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f67"
  },
  "title": "Applying Cognitive Tutoring in the use of Bioinformatics Tools",
  "abstract": "With the proliferation of simple and complex bioinformatics tools, there is the need to teach researchers how to use these tools effectively. To evaluate the potential of cognitive tutoring in the wide-scale adoption of several bioinformatics tools, we designed a simple prototype. We embedded a cognitive tutor, built with the Cognitive Tutor Authoring Tool, on a preexisting platform, the Gene Adjacency Program, developed by the University of Ibadan Bioinformatics group. Our preliminary tests show that researchers who used the platform with the cognitive tutor embedded showed higher levels of competence and efficiency. These results indicate that cognitive tutors have the potential to teach bioinformatics researchers employing new tools how to efficiently use them and accurately make sense of their results.",
  "tags": [
    "Computers and Society"
  ],
  "author": "Angela U. Makolo",
  "date": "2018-11-20T17:56:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.731Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.731Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f68"
  },
  "title": "The theoretical analysis of sequencing bioinformatics algorithms and beyond",
  "abstract": "The theoretical analysis of performance has been an important tool in the engineering of algorithms in many application domains. Its goals are to predict the empirical performance of an algorithm and to be a yardstick that drives the design of novel algorithms that perform well in practice. While these goals have been achieved in many instances, they have not been achieved ubiquitously across crucial application domains. I provide a case study in the area of sequencing bioinformatics, an inter-disciplinary field that uses algorithms to extract biological meaning from genome sequencing data. In particular, I give three concrete examples: two showing how theoretical analysis has failed to achieve its goals and one showing how it has been successful. I will then catalog some of the challenges of applying theoretical analysis to sequencing bioinformatics, argue why empirical analysis is not enough, and give a vision for improving the relevance of theoretical analysis to sequencing bioinformatics. By recognizing the problem, understanding its roots, and providing potential solutions, this work can hopefully be a crucial first step towards making theoretical analysis more relevant in sequencing bioinformatics and potentially other fast-paced application domains.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Paul Medvedev",
  "date": "2022-05-03T21:17:53",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.732Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.732Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f69"
  },
  "title": "The Use of Agricultural Robots in Orchard Management",
  "abstract": "Book chapter that summarizes recent research on agricultural robotics in orchard management, including Robotic pruning, Robotic thinning, Robotic spraying, Robotic harvesting, Robotic fruit transportation, and future trends.",
  "tags": [
    "Robotics"
  ],
  "author": "Qin Zhang",
  "date": "2019-07-30T17:56:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.733Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.733Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f6a"
  },
  "title": "Robotics in Snow and Ice",
  "abstract": "Definition: The terms \"robotics in snow and ice\" refers to robotic systems being studied, developed, and used in areas where water can be found in its solid state. This specialized branch of field robotics investigates the impact of extreme conditions related to cold environments on autonomous vehicles.",
  "tags": [
    "Robotics"
  ],
  "author": "François Pomerleau",
  "date": "2022-08-10T01:02:57",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.734Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.734Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f6b"
  },
  "title": "Robot Accident Investigation: a case study in Responsible Robotics",
  "abstract": "Robot accidents are inevitable. Although rare, they have been happening since assembly-line robots were first introduced in the 1960s. But a new generation of social robots are now becoming commonplace. Often with sophisticated embedded artificial intelligence (AI) social robots might be deployed as care robots to assist elderly or disabled people to live independently. Smart robot toys offer a compelling interactive play experience for children and increasingly capable autonomous vehicles (AVs) the promise of hands-free personal transport and fully autonomous taxis. Unlike industrial robots which are deployed in safety cages, social robots are designed to operate in human environments and interact closely with humans; the likelihood of robot accidents is therefore much greater for social robots than industrial robots. This paper sets out a draft framework for social robot accident investigation; a framework which proposes both the technology and processes that would allow social robot accidents to be investigated with no less rigour than we expect of air or rail accident investigations. The paper also places accident investigation within the practice of responsible robotics, and makes the case that social robotics without accident investigation would be no less irresponsible than aviation without air accident investigation.",
  "tags": [
    "Robotics"
  ],
  "author": "Alan F. T. Winfield",
  "date": "2020-05-15T11:31:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.735Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.735Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f6c"
  },
  "title": "Pattern Formation for Asynchronous Robots without Agreement in Chirality",
  "abstract": "This paper presents a deterministic algorithm for forming a given asymmetric pattern in finite time by a set of autonomous, homogeneous, oblivious mobile robots under the CORDA model. The robots are represented as points on the 2D plane. There is no explicit communication between the robots. The robots coordinate among themselves by observing the positions of the other robots on the plane. Initially all the robots are assumed to be stationary. The robots have local coordinate systems defined by Sense of Direction (SoD), orientation or chirality and scale. Initially the robots are in asymmetric configuration. We show that these robots can form any given asymmetric pattern in finite time.",
  "tags": [
    "Distributed Computing",
    "Robotics"
  ],
  "author": "Sruti Gan Chaudhuri",
  "date": "2014-03-11T16:12:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.736Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.736Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f6d"
  },
  "title": "Formation of General Position by Asynchronous Mobile Robots",
  "abstract": "The traditional distributed model of autonomous, homogeneous, mobile point robots usually assumes that the robots do not create any visual obstruction for the other robots, i.e., the robots are see through. In this paper, we consider a slightly more realistic model, by incorporating the notion of obstructed visibility (i.e., robots are not see through) for other robots. Under the new model of visibility, a robot may not have the full view of its surroundings. Many of the existing algorithms demand that each robot should have the complete knowledge of the positions of other robots. Since, vision is the only mean of their communication, it is required that the robots are in general position (i.e., no three robots are collinear). We consider asynchronous robots. They also do not have common chirality (or any agreement on a global coordinate system). In this paper, we present a distributed algorithm for obtaining a general position for the robots in finite time from any arbitrary configuration. The algorithm also assures collision free motion for each robot. This algorithm may also be used as a preprocessing module for many other subsequent tasks performed by the robots.",
  "tags": [
    "Distributed Computing",
    "Robotics"
  ],
  "author": "S. Bhagat",
  "date": "2014-08-09T07:43:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.737Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.737Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f6e"
  },
  "title": "A review of cuspidal serial and parallel manipulators",
  "abstract": "Cuspidal robots can move from one inverse or direct kinematic solution to another without ever passing through a singularity. These robots have remained unknown because almost all industrial robots do not have this feature. However, in fact, industrial robots are the exceptions. Some robots appeared recently in the industrial market can be shown to be cuspidal but, surprisingly, almost nobody knows it and robot users meet difficulties in planning trajectories with these robots. This paper proposes a review on the fundamental and application aspects of cuspidal robots. It addresses the important issues raised by these robots for the design and planning of trajectories. The identification of all cuspidal robots is still an open issue. This paper recalls in details the case of serial robots with three joints but it also addresses robots with more complex architectures such as 6-revolute-jointed robot and parallel robots. We hope that this paper will help disseminate more widely knowledge on cuspidal robots.",
  "tags": [
    "Robotics"
  ],
  "author": "Philippe Wenger",
  "date": "2022-10-11T07:19:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.738Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.738Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f6f"
  },
  "title": "Optimal Dispersion of Silent Robots in a Ring",
  "abstract": "Given a set of co-located mobile robots in an unknown anonymous graph, the robots must relocate themselves in distinct graph nodes to solve the dispersion problem. In this paper, we consider the dispersion problem for silent robots \\cite{gorain2024collaborative}, i.e., no direct, explicit communication between any two robots placed in the nodes of an oriented $n$ node ring network. The robots operate in synchronous rounds. The dispersion problem for silent mobile robots has been studied in arbitrary graphs where the robots start from a single source. In this paper, we focus on the dispersion problem for silent mobile robots where robots can start from multiple sources. The robots have unique labels from a range $[0,\\;L]$ for some positive integer $L$. Any two co-located robots do not have the information about the label of the other robot. The robots have weak multiplicity detection capability, which means they can determine if it is alone on a node. The robots are assumed to be able to identify an increase or decrease in the number of robots present on a node in a particular round. However, the robots can not get the exact number of increase or decrease in the number of robots. We have proposed a deterministic distributed algorithm that solves the dispersion of $k$ robots in an oriented ring in $O(\\log L+k)$ synchronous rounds with $O(\\log L)$ bits of memory for each robot. A lower bound $\\Omega(\\log L+k)$ on time for the dispersion of $k$ robots on a ring network is presented to establish the optimality of the proposed algorithm.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Bibhuti Das",
  "date": "2024-08-10T08:43:07",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.739Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.739Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f70"
  },
  "title": "Artificial Intelligence and Systems Theory: Applied to Cooperative Robots",
  "abstract": "This paper describes an approach to the design of a population of cooperative robots based on concepts borrowed from Systems Theory and Artificial Intelligence. The research has been developed under the SocRob project, carried out by the Intelligent Systems Laboratory at the Institute for Systems and Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the project stands both for \"Society of Robots\" and \"Soccer Robots\", the case study where we are testing our population of robots. Designing soccer robots is a very challenging problem, where the robots must act not only to shoot a ball towards the goal, but also to detect and avoid static (walls, stopped robots) and dynamic (moving robots) obstacles. Furthermore, they must cooperate to defeat an opposing team. Our past and current research in soccer robotics includes cooperative sensor fusion for world modeling, object recognition and tracking, robot navigation, multi-robot distributed task planning and coordination, including cooperative reinforcement learning in cooperative and adversarial environments, and behavior-based architectures for real time task execution of cooperating robot teams.",
  "tags": [
    "Robotics",
    "Artificial Intelligence"
  ],
  "author": "Pedro U. Lima",
  "date": "2004-11-08T20:41:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.740Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.740Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f71"
  },
  "title": "Medical robotics: where we come from, where we are and where we could go",
  "abstract": "This short note presents a viewpoint about medical robotics.",
  "tags": [
    "Robotics"
  ],
  "author": "Jocelyne Troccaz",
  "date": "2008-08-12T13:21:52",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.741Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.741Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f72"
  },
  "title": "Game-Theoretic Modeling of Human Adaptation in Human-Robot Collaboration",
  "abstract": "In human-robot teams, humans often start with an inaccurate model of the robot capabilities. As they interact with the robot, they infer the robot's capabilities and partially adapt to the robot, i.e., they might change their actions based on the observed outcomes and the robot's actions, without replicating the robot's policy. We present a game-theoretic model of human partial adaptation to the robot, where the human responds to the robot's actions by maximizing a reward function that changes stochastically over time, capturing the evolution of their expectations of the robot's capabilities. The robot can then use this model to decide optimally between taking actions that reveal its capabilities to the human and taking the best action given the information that the human currently has. We prove that under certain observability assumptions, the optimal policy can be computed efficiently. We demonstrate through a human subject experiment that the proposed model significantly improves human-robot team performance, compared to policies that assume complete adaptation of the human to the robot.",
  "tags": [
    "Robotics"
  ],
  "author": "Stefanos Nikolaidis",
  "date": "2017-01-26T17:45:47",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.742Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.742Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f73"
  },
  "title": "Animation Techniques in Human-Robot Interaction User Studies: a Systematic Literature Review",
  "abstract": "There are many different ways a robot can move in Human-Robot Interaction. One way is to use techniques from film animation to instruct the robot to move. This article is a systematic literature review of human-robot trials, pilots, and evaluations that have applied techniques from animation to move a robot. Through 27 articles, we find that animation techniques improves individual's interaction with robots, improving individual's perception of qualities of a robot, understanding what a robot intends to do, and showing the robot's state, or possible emotion. Animation techniques also help people relate to robots that do not resemble a human or robot. The studies in the articles show further areas for research, such as applying animation principles in other types of robots and situations, combining animation techniques with other modalities, and testing robots moving with animation techniques over the long term.",
  "tags": [
    "Robotics",
    "Human-Computer Interaction"
  ],
  "author": "Trenton Schulz",
  "date": "2018-12-17T14:21:37",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.743Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.743Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f74"
  },
  "title": "Robots that Take Advantage of Human Trust",
  "abstract": "Humans often assume that robots are rational. We believe robots take optimal actions given their objective; hence, when we are uncertain about what the robot's objective is, we interpret the robot's actions as optimal with respect to our estimate of its objective. This approach makes sense when robots straightforwardly optimize their objective, and enables humans to learn what the robot is trying to achieve. However, our insight is that---when robots are aware that humans learn by trusting that the robot actions are rational---intelligent robots do not act as the human expects; instead, they take advantage of the human's trust, and exploit this trust to more efficiently optimize their own objective. In this paper, we formally model instances of human-robot interaction (HRI) where the human does not know the robot's objective using a two-player game. We formulate different ways in which the robot can model the uncertain human, and compare solutions of this game when the robot has conservative, optimistic, rational, and trusting human models. In an offline linear-quadratic case study and a real-time user study, we show that trusting human models can naturally lead to communicative robot behavior, which influences end-users and increases their involvement.",
  "tags": [
    "Robotics"
  ],
  "author": "Dylan P. Losey",
  "date": "2019-09-12T16:16:21",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.744Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.744Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f75"
  },
  "title": "Loosely Coupled Payload Transport System with Robot Replacement",
  "abstract": "In this work, we present an algorithm for robot replacement to increase the operational time of a multi-robot payload transport system. Our system comprises a group of nonholonomic wheeled mobile robots traversing on a known trajectory. We design a multi-robot system with loosely coupled robots that ensures the system lasts much longer than the battery life of an individual robot. A system level optimization is presented, to decide on the operational state (charging or discharging) of each robot in the system. The charging state implies that the robot is not in a formation and is kept on charge whereas the discharging state implies that the robot is a part of the formation. Robot battery recharge hubs are present along the trajectory. Robots in the formation can be replaced at these hub locations with charged robots using a replacement mechanism. We showcase the efficacy of the proposed scheduling framework through simulations and experiments with real robots.",
  "tags": [
    "Robotics"
  ],
  "author": "Pulkit Verma",
  "date": "2019-04-05T13:17:27",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.745Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.745Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f76"
  },
  "title": "Robot Vitals and Robot Health: Towards Systematically Quantifying Runtime Performance Degradation in Robots Under Adverse Conditions",
  "abstract": "This paper addresses the problem of automatically detecting and quantifying performance degradation in remote mobile robots during task execution. A robot may encounter a variety of uncertainties and adversities during task execution, which can impair its ability to carry out tasks effectively and cause its performance to degrade. Such situations can be mitigated or averted by timely detection and intervention (e.g., by a remote human supervisor taking over control in teleoperation mode). Inspired by patient triaging systems in hospitals, we introduce the framework of \"robot vitals\" for estimating overall \"robot health\". A robot's vitals are a set of indicators that estimate the extent of performance degradation faced by a robot at a given point in time. Robot health is a metric that combines robot vitals into a single scalar value estimate of performance degradation. Experiments, both in simulation and on a real mobile robot, demonstrate that the proposed robot vitals and robot health can be used effectively to estimate robot performance degradation during runtime.",
  "tags": [
    "Robotics",
    "Artificial Intelligence",
    "Human-Computer Interaction"
  ],
  "author": "Aniketh Ramesh",
  "date": "2022-07-04T19:26:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.746Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.746Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f77"
  },
  "title": "Come Closer: The Effects of Robot Personality on Human Proxemics Behaviours",
  "abstract": "Social Robots in human environments need to be able to reason about their physical surroundings while interacting with people. Furthermore, human proxemics behaviours around robots can indicate how people perceive the robots and can inform robot personality and interaction design. Here, we introduce Charlie, a situated robot receptionist that can interact with people using verbal and non-verbal communication in a dynamic environment, where users might enter or leave the scene at any time. The robot receptionist is stationary and cannot navigate. Therefore, people have full control over their personal space as they are the ones approaching the robot. We investigated the influence of different apparent robot personalities on the proxemics behaviours of the humans. The results indicate that different types of robot personalities, specifically introversion and extroversion, can influence human proxemics behaviours. Participants maintained shorter distances with the introvert robot receptionist, compared to the extrovert robot. Interestingly, we observed that human-robot proxemics were not the same as typical human-human interpersonal distances, as defined in the literature. We therefore propose new proxemics zones for human-robot interaction.",
  "tags": [
    "Robotics"
  ],
  "author": "Meriam Moujahid",
  "date": "2023-09-06T13:24:45",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.747Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.747Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f78"
  },
  "title": "Soft is Safe: Human-Robot Interaction for Soft Robots",
  "abstract": "With the presence of robots increasing in the society, the need for interacting with robots is becoming necessary. The field of Human-Robot Interaction (HRI) has emerged important since more repetitive and tiresome jobs are being done by robots. In the recent times, the field of soft robotics has seen a boom in the field of research and commercialization. The Industry 5.0 focuses on human robot collaboration which also spurs the field of soft robotics. However the HRI for soft robotics is still in the nascent stage. In this work we review and then discuss how HRI is done for soft robots. We first discuss the control, design, materials and manufacturing of soft robots. This will provide an understanding of what is being interacted with. Then we discuss about the various input and output modalities that are used in HRI. The applications where the HRI for soft robots are found in the literature are discussed in detail. Then the limitations of HRI for soft robots and various research opportunities that exist in this field are discussed in detail. It is concluded that there is a huge scope for development for HRI for soft robots.",
  "tags": [
    "Robotics"
  ],
  "author": "Rajashekhar V S",
  "date": "2025-02-03T11:26:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.748Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.748Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f79"
  },
  "title": "Modular Robots: extending the capabilities of one robot",
  "abstract": "For a robot to be perfect and enter the everyday life of humans,like computers did, it needs to move from special-purpose robots to general-purpose. So, the idea of modularity is considered in this project.Thus, any type of task that falls in the 4 D's of Robotization: Dull, Dirty, Dangerous and Dear can be achieved by adding a module to the robot.",
  "tags": [
    "Robotics"
  ],
  "author": "Aymen Rachdi",
  "date": "2022-10-24T13:26:18",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.749Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.749Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f7a"
  },
  "title": "Cuspidal Robots",
  "abstract": "This chapter is dedicated to the so-called cuspidal robots, i.e. those robots that can move from one inverse geometric solution to another without meeting a singular confuguration. This feature was discovered quite recently and has then been fascinating a lot of researchers. After a brief history of cuspidal robots, the chapter provides the main features of cuspidal robots: explanation of the non-singular change of posture, uniqueness domains, regions of feasible paths, identification and classification of cuspidal robots. The chapter focuses on 3-R orthogonal serial robots. The case of 6-dof robots and parallel robots is discussed in the end of this chapter.",
  "tags": [
    "Robotics"
  ],
  "author": "Philippe Wenger",
  "date": "2016-10-13T13:58:59",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.750Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.750Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f7b"
  },
  "title": "Graph Neural Networks for Learning Robot Team Coordination",
  "abstract": "This paper shows how Graph Neural Networks can be used for learning distributed coordination mechanisms in connected teams of robots. We capture the relational aspect of robot coordination by modeling the robot team as a graph, where each robot is a node, and edges represent communication links. During training, robots learn how to pass messages and update internal states, so that a target behavior is reached. As a proxy for more complex problems, this short paper considers the problem where each robot must locally estimate the algebraic connectivity of the team's network topology.",
  "tags": [
    "Robotics",
    "Machine Learning",
    "Multiagent Systems"
  ],
  "author": "Amanda Prorok",
  "date": "2018-05-09T21:24:50",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.751Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.751Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f7c"
  },
  "title": "On Robot Revolution and Taxation",
  "abstract": "Advances in artificial intelligence are resulting in the rapid automation of the work force. The tools that are used to automate are called robots. Bill Gates proposed that in order to deal with the problem of the loss of jobs and reduction of the tax revenue we ought to tax the robots. The problem with taxing the robots is that it is not easy to know what a robot is. This article studies the definition of a robot and the implication of advances in robotics on taxation. It is evident from this article that it is a difficult task to establish what a robot is and what is not a robot. It concludes that taxing robots is the same as increasing corporate tax.",
  "tags": [
    "Computers and Society"
  ],
  "author": "Tshilidzi Marwala",
  "date": "2018-08-05T18:26:34",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.752Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.752Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f7d"
  },
  "title": "Recent Advances in Human-Robot Collaboration Towards Joint Action",
  "abstract": "Robots existed as separate entities till now, but the horizons of a symbiotic human-robot partnership are impending. Despite all the recent technical advances in terms of hardware, robots are still not endowed with desirable relational skills that ensure a social component in their existence. This article draws from our experience as roboticists in Human-Robot Collaboration (HRC) with humanoid robots and presents some of the recent advances made towards realizing intuitive robot behaviors and partner-aware control involving physical interactions.",
  "tags": [
    "Robotics"
  ],
  "author": "Yeshasvi Tirupachuri",
  "date": "2020-01-02T12:26:20",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.755Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.755Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f7e"
  },
  "title": "SENSAR: A Visual Tool for Intelligent Robots for Collaborative Human-Robot Interaction",
  "abstract": "Establishing common ground between an intelligent robot and a human requires communication of the robot's intention, behavior, and knowledge to the human to build trust and assure safety in a shared environment. This paper introduces SENSAR (Seeing Everything iN Situ with Augmented Reality), an augmented reality robotic system that enables robots to communicate their sensory and cognitive data in context over the real-world with rendered graphics, allowing a user to understand, correct, and validate the robot's perception of the world. Our system aims to support human-robot interaction research by establishing common ground where the perceptions of the human and the robot align.",
  "tags": [
    "Robotics"
  ],
  "author": "Andre Cleaver",
  "date": "2020-11-09T15:50:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.756Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.756Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f7f"
  },
  "title": "Effects of Interruptibility-Aware Robot Behavior",
  "abstract": "As robots become increasingly prevalent in human environments, there will inevitably be times when a robot needs to interrupt a human to initiate an interaction. Our work introduces the first interruptibility-aware mobile robot system, and evaluates the effects of interruptibility-awareness on human task performance, robot task performance, and on human interpretation of the robot's social aptitude. Our results show that our robot is effective at predicting interruptibility at high accuracy, allowing it to interrupt at more appropriate times. Results of a large-scale user study show that while participants are able to maintain task performance even in the presence of interruptions, interruptibility-awareness improves the robot's task performance and improves participant social perception of the robot.",
  "tags": [
    "Robotics",
    "Human-Computer Interaction"
  ],
  "author": "Siddhartha Banerjee",
  "date": "2018-04-17T17:26:30",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.757Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.757Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f80"
  },
  "title": "A Survey on End-User Robot Programming",
  "abstract": "As robots interact with a broader range of end-users, end-user robot programming has helped democratize robot programming by empowering end-users who may not have experience in robot programming to customize robots to meet their individual contextual needs. This article surveys work on end-user robot programming, with a focus on end-user program specification. It describes the primary domains, programming phases, and design choices represented by the end-user robot programming literature. The survey concludes by highlighting open directions for further investigation to enhance and widen the reach of end-user robot programming systems.",
  "tags": [
    "Robotics",
    "Human-Computer Interaction"
  ],
  "author": "Gopika Ajaykumar",
  "date": "2021-05-04T20:55:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.758Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.758Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f81"
  },
  "title": "Utilising Explanations to Mitigate Robot Conversational Failures",
  "abstract": "This paper presents an overview of robot failure detection work from HRI and adjacent fields using failures as an opportunity to examine robot explanation behaviours. As humanoid robots remain experimental tools in the early 2020s, interactions with robots are situated overwhelmingly in controlled environments, typically studying various interactional phenomena. Such interactions suffer from real-world and large-scale experimentation and tend to ignore the 'imperfectness' of the everyday user. Robot explanations can be used to approach and mitigate failures, by expressing robot legibility and incapability, and within the perspective of common-ground. In this paper, I discuss how failures present opportunities for explanations in interactive conversational robots and what the potentials are for the intersection of HRI and explainability research.",
  "tags": [
    "Human-Computer Interaction",
    "Robotics"
  ],
  "author": "Dimosthenis Kontogiorgos",
  "date": "2023-07-10T10:20:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.759Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.759Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f82"
  },
  "title": "The Rise of Quantum Internet Computing",
  "abstract": "This article highlights quantum Internet computing as referring to distributed quantum computing over the quantum Internet, analogous to (classical) Internet computing involving (classical) distributed computing over the (classical) Internet. Relevant to quantum Internet computing would be areas of study such as quantum protocols for distributed nodes using quantum information for computations, quantum cloud computing, delegated verifiable blind or private computing, non-local gates, and distributed quantum applications, over Internet-scale distances.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Seng W. Loke",
  "date": "2022-08-01T10:36:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.760Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.760Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f83"
  },
  "title": "Google Quantum AI's Quest for Error-Corrected Quantum Computers",
  "abstract": "Quantum computers stand at the forefront of technological innovation, offering exponential computational speed-ups that challenge classical computing capabilities. At the cutting edge of this transformation is Google Quantum AI, a leader in driving forward the development of practical quantum computers. This article provides a comprehensive review of Google Quantum AI's pivotal role in the quantum computing landscape over the past decade, emphasizing their significant strides towards achieving quantum computational supremacy. By exploring their advancements and contributions in quantum hardware, quantum software, error correction, and quantum algorithms, this study highlights the transformative impact of Google Quantum AI's initiatives in shaping the future of quantum computing technology.",
  "tags": [
    "Architecture"
  ],
  "author": "M. AbuGhanem",
  "date": "2024-09-23T15:56:14",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.761Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.761Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f84"
  },
  "title": "Optimal Stochastic Resource Allocation for Distributed Quantum Computing",
  "abstract": "With the advent of interconnected quantum computers, i.e., distributed quantum computing (DQC), multiple quantum computers can now collaborate via quantum networks to perform massively complex computational tasks. However, DQC faces problems sharing quantum information because it cannot be cloned or duplicated between quantum computers. Thanks to advanced quantum mechanics, quantum computers can teleport quantum information across quantum networks. However, challenges to utilizing efficiently quantum resources, e.g., quantum computers and quantum channels, arise in DQC due to their capabilities and properties, such as uncertain qubit fidelity and quantum channel noise. In this paper, we propose a resource allocation scheme for DQC based on stochastic programming to minimize the total deployment cost for quantum resources. Essentially, the two-stage stochastic programming model is formulated to handle the uncertainty of quantum computing demands, computing power, and fidelity in quantum networks. The performance evaluation demonstrates the effectiveness and ability of the proposed scheme to balance the utilization of quantum computers and on-demand quantum computers while minimizing the overall cost of provisioning under uncertainty.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Napat Ngoenriang",
  "date": "2022-09-16T02:37:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.762Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.762Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f85"
  },
  "title": "Quantum Computers and Quantum Computer Languages: Quantum Assembly Language and Quantum C Language",
  "abstract": "We show a representation of Quantum Computers defines Quantum Turing Machines with associated Quantum Grammars. We then create examples of Quantum Grammars. Lastly we develop an algebraic approach to high level Quantum Languages using Quantum Assembly language and Quantum C language as examples.",
  "tags": [
    "Programming Languages"
  ],
  "author": "Stephen Blaha",
  "date": "2002-01-18T15:08:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.765Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.765Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f86"
  },
  "title": "IBM Quantum Computers: Evolution, Performance, and Future Directions",
  "abstract": "Quantum computers represent a transformative frontier in computational technology, promising exponential speedups beyond classical computing limits. IBM Quantum has led significant advancements in both hardware and software, providing access to quantum hardware via IBM Cloud since 2016, achieving a milestone with the world's first accessible quantum computer. This article explores IBM's quantum computing journey, focusing on the development of practical quantum computers. We summarize the evolution and advancements of IBM Quantum's processors across generations, including their recent breakthrough surpassing the 1,000-qubit barrier. The paper reviews detailed performance metrics across various hardware, tracing their evolution over time and highlighting IBM Quantum's transition from the noisy intermediate-scale quantum (NISQ) computing era towards fault-tolerant quantum computing capabilities.",
  "tags": [
    "Artificial Intelligence",
    "Architecture"
  ],
  "author": "M. AbuGhanem",
  "date": "2024-09-17T07:50:50",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.766Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.766Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f87"
  },
  "title": "From Distributed Quantum Computing to Quantum Internet Computing: an Overview",
  "abstract": "The possibility of quantum computing has been proposed decades ago, at least as far back as the 1980s, and distributed quantum computing has been studied around two decades ago. Recent times have seen experimental successes and advances in quantum computer hardware and in quantum networking, leading towards the quantum Internet. We provide in this paper an overview of concepts and ideas in distributed quantum computing since over two decades ago as well as look at recent efforts in the area, and consider how, with the development of the quantum Internet, distributed quantum computing is evolving into quantum Internet computing.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Seng W. Loke",
  "date": "2022-08-22T07:58:59",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.767Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.767Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f88"
  },
  "title": "Quantum Cybernetics and Complex Quantum Systems Science - A Quantum Connectionist Exploration",
  "abstract": "Quantum cybernetics and its connections to complex quantum systems science is addressed from the perspective of complex quantum computing systems. In this way, the notion of an autonomous quantum computing system is introduced in regards to quantum artificial intelligence, and applied to quantum artificial neural networks, considered as autonomous quantum computing systems, which leads to a quantum connectionist framework within quantum cybernetics for complex quantum computing systems. Several examples of quantum feedforward neural networks are addressed in regards to Boolean functions' computation, multilayer quantum computation dynamics, entanglement and quantum complementarity. The examples provide a framework for a reflection on the role of quantum artificial neural networks as a general framework for addressing complex quantum systems that perform network-based quantum computation, possible consequences are drawn regarding quantum technologies, as well as fundamental research in complex quantum systems science and quantum biology.",
  "tags": [
    "Neural and Evolutionary Computing"
  ],
  "author": "Carlos Pedro Gonçalves",
  "date": "2014-02-05T19:48:24",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.768Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.768Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f89"
  },
  "title": "Quantum Computing for Multi Period Asset Allocation",
  "abstract": "Portfolio construction has been a long-standing topic of research in finance. The computational complexity and the time taken both increase rapidly with the number of investments in the portfolio. It becomes difficult, even impossible for classic computers to solve. Quantum computing is a new way of computing which takes advantage of quantum superposition and entanglement. It changes how such problems are approached and is not constrained by some of the classic computational complexity. Studies have shown that quantum computing can offer significant advantages over classical computing in many fields. The application of quantum computing has been constrained by the unavailability of actual quantum computers. In the past decade, there has been the rapid development of the large-scale quantum computer. However, software development for quantum computing is slow in many fields. In our study, we apply quantum computing to a multi-asset portfolio simulation. The simulation is based on historic data, covariance, and expected returns, all calculated using quantum computing. Although technically a solvable problem for classical computing, we believe the software development is important to the future application of quantum computing in finance. We conducted this study through simulation of a quantum computer and the use of Rensselaer Polytechnic Institute's IBM quantum computer.",
  "tags": [
    "Computational Finance"
  ],
  "author": "Queenie Sun",
  "date": "2024-10-15T19:04:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.769Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.769Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f8a"
  },
  "title": "A Uniform Quantum Computing Model Based on Virtual Quantum Processors",
  "abstract": "Quantum Computers, one fully realized, can represent an exponential boost in computing power. However, the computational power of the current quantum computers, referred to as Noisy Internediate Scale Quantum, or NISQ, is severely limited because of environmental and intrinsic noise, as well as the very low connectivity between qubits compared to their total amount. We propose a virtual quantum processor that emulates a generic hybrid quantum machine which can serve as a logical version of quantum computing hardware. This hybrid classical quantum machine powers quantum-logical computations which are substitutable by future native quantum processors.",
  "tags": [
    "Artificial Intelligence",
    "Architecture"
  ],
  "author": "George Gesek",
  "date": "2023-02-24T17:07:37",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.770Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.770Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f8b"
  },
  "title": "Financial Market Trend Forecasting and Performance Analysis Using LSTM",
  "abstract": "The financial market trend forecasting method is emerging as a hot topic in financial markets today. Many challenges still currently remain, and various researches related thereto have been actively conducted. Especially, recent research of neural network-based financial market trend prediction has attracted much attention. However, previous researches do not deal with the financial market forecasting method based on LSTM which has good performance in time series data. There is also a lack of comparative analysis in the performance of neural network-based prediction techniques and traditional prediction techniques. In this paper, we propose a financial market trend forecasting method using LSTM and analyze the performance with existing financial market trend forecasting methods through experiments. This method prepares the input data set through the data preprocessing process so as to reflect all the fundamental data, technical data and qualitative data used in the financial data analysis, and makes comprehensive financial market analysis through LSTM. In this paper, we experiment and compare performances of existing financial market trend forecasting models, and performance according to the financial market environment. In addition, we implement the proposed method using open sources and platform and forecast financial market trends using various financial data indicators.",
  "tags": [
    "Statistical Finance",
    "Machine Learning"
  ],
  "author": "Jonghyeon Min",
  "date": "2020-03-31T01:30:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.772Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.772Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f8c"
  },
  "title": "Large Language Models for Financial Aid in Financial Time-series Forecasting",
  "abstract": "Considering the difficulty of financial time series forecasting in financial aid, much of the current research focuses on leveraging big data analytics in financial services. One modern approach is to utilize \"predictive analysis\", analogous to forecasting financial trends. However, many of these time series data in Financial Aid (FA) pose unique challenges due to limited historical datasets and high dimensional financial information, which hinder the development of effective predictive models that balance accuracy with efficient runtime and memory usage. Pre-trained foundation models are employed to address these challenging tasks. We use state-of-the-art time series models including pre-trained LLMs (GPT-2 as the backbone), transformers, and linear models to demonstrate their ability to outperform traditional approaches, even with minimal (\"few-shot\") or no fine-tuning (\"zero-shot\"). Our benchmark study, which includes financial aid with seven other time series tasks, shows the potential of using LLMs for scarce financial datasets.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence"
  ],
  "author": "Md Khairul Islam",
  "date": "2024-10-24T12:41:47",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.774Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.774Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f8d"
  },
  "title": "The Role of AI in Financial Forecasting: ChatGPT's Potential and Challenges",
  "abstract": "The outlook for the future of artificial intelligence (AI) in the financial sector, especially in financial forecasting, the challenges and implications. The dynamics of AI technology, including deep learning, reinforcement learning, and integration with blockchAIn and the Internet of Things, also highlight the continued improvement in data processing capabilities. Explore how AI is reshaping financial services with precisely tAIlored services that can more precisely meet the diverse needs of individual investors. The integration of AI challenges regulatory and ethical issues in the financial sector, as well as the implications for data privacy protection. Analyze the limitations of current AI technology in financial forecasting and its potential impact on the future financial industry landscape, including changes in the job market, the emergence of new financial institutions, and user interface innovations. Emphasizing the importance of increasing investor understanding and awareness of AI and looking ahead to future trends in AI tools for user experience to drive wider adoption of AI in financial decision making. The huge potential, challenges, and future directions of AI in the financial sector highlight the critical role of AI technology in driving transformation and innovation in the financial sector",
  "tags": [
    "Statistical Finance",
    "Artificial Intelligence",
    "Computers and Society"
  ],
  "author": "Shuochen Bi",
  "date": "2024-11-07T15:35:16",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.775Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.775Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f8e"
  },
  "title": "Assessing Text Mining and Technical Analyses on Forecasting Financial Time Series",
  "abstract": "Forecasting financial time series (FTS) is an essential field in finance and economics that anticipates market movements in financial markets. This paper investigates the accuracy of text mining and technical analyses in forecasting financial time series. It focuses on the S&P500 stock market index during the pandemic, which tracks the performance of the largest publicly traded companies in the US. The study compares two methods of forecasting the future price of the S&P500: text mining, which uses NLP techniques to extract meaningful insights from financial news, and technical analysis, which uses historical price and volume data to make predictions. The study examines the advantages and limitations of both methods and analyze their performance in predicting the S&P500. The FinBERT model outperforms other models in terms of S&P500 price prediction, as evidenced by its lower RMSE value, and has the potential to revolutionize financial analysis and prediction using financial news data. Keywords: ARIMA, BERT, FinBERT, Forecasting Financial Time Series, GARCH, LSTM, Technical Analysis, Text Mining JEL classifications: G4, C8",
  "tags": [
    "Econometrics"
  ],
  "author": "Ali Lashgari",
  "date": "2023-04-27T21:52:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.776Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.776Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f8f"
  },
  "title": "Advancing Financial Forecasting: A Comparative Analysis of Neural Forecasting Models N-HiTS and N-BEATS",
  "abstract": "In the rapidly evolving field of financial forecasting, the application of neural networks presents a compelling advancement over traditional statistical models. This research paper explores the effectiveness of two specific neural forecasting models, N-HiTS and N-BEATS, in predicting financial market trends. Through a systematic comparison with conventional models, this study demonstrates the superior predictive capabilities of neural approaches, particularly in handling the non-linear dynamics and complex patterns inherent in financial time series data. The results indicate that N-HiTS and N-BEATS not only enhance the accuracy of forecasts but also boost the robustness and adaptability of financial predictions, offering substantial advantages in environments that require real-time decision-making. The paper concludes with insights into the practical implications of neural forecasting in financial markets and recommendations for future research directions.",
  "tags": [
    "Computational Finance"
  ],
  "author": "Mohit Apte",
  "date": "2024-08-31T15:26:30",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.777Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.777Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f90"
  },
  "title": "FinTSBridge: A New Evaluation Suite for Real-world Financial Prediction with Advanced Time Series Models",
  "abstract": "Despite the growing attention to time series forecasting in recent years, many studies have proposed various solutions to address the challenges encountered in time series prediction, aiming to improve forecasting performance. However, effectively applying these time series forecasting models to the field of financial asset pricing remains a challenging issue. There is still a need for a bridge to connect cutting-edge time series forecasting models with financial asset pricing. To bridge this gap, we have undertaken the following efforts: 1) We constructed three datasets from the financial domain; 2) We selected over ten time series forecasting models from recent studies and validated their performance in financial time series; 3) We developed new metrics, msIC and msIR, in addition to MSE and MAE, to showcase the time series correlation captured by the models; 4) We designed financial-specific tasks for these three datasets and assessed the practical performance and application potential of these forecasting models in important financial problems. We hope the developed new evaluation suite, FinTSBridge, can provide valuable insights into the effectiveness and robustness of advanced forecasting models in finanical domains.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Yanlong Wang",
  "date": "2025-03-10T05:19:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.778Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.778Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f91"
  },
  "title": "Artificial intelligence-based blockchain-driven financial default prediction",
  "abstract": "With the rapid development of technology, blockchain and artificial intelligence technology are playing a huge role in all walks of life. In the financial sector, blockchain solves many security problems in data storage and management in traditional systems with its advantages of decentralization and security. And artificial intelligence has huge advantages in financial forecasting and risk management through its powerful algorithmic modeling capabilities. In financial default prediction using blockchain and artificial intelligence technology is a very powerful application. Blockchain technology guarantees the credibility of data and consistency on all nodes, and machine learning builds a high-level default prediction model through detailed analysis of big data. This study offers financial institutions new thoughts on financial technology in terms of credit risk mitigation and financial system stabilization.",
  "tags": [
    "Cryptography and Security",
    "Artificial Intelligence"
  ],
  "author": "Junjun Huang",
  "date": "2024-09-27T17:51:48",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.779Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.779Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f92"
  },
  "title": "Forecasting The JSE Top 40 Using Long Short-Term Memory Networks",
  "abstract": "As a result of the greater availability of big data, as well as the decreasing costs and increasing power of modern computing, the use of artificial neural networks for financial time series forecasting is once again a major topic of discussion and research in the financial world. Despite this academic focus, there are still contrasting opinions and bodies of literature on which artificial neural networks perform the best and whether or not they outperform the forecasting capabilities of conventional time series models. This paper uses a long-short term memory network to perform financial time series forecasting on the return data of the JSE Top 40 index. Furthermore, the forecasting performance of the long-short term memory network is compared to the forecasting performance of a seasonal autoregressive integrated moving average model. This paper evaluates the varying approaches presented in the existing literature and ultimately, compares the results to that existing literature. The paper concludes that the long short-term memory network outperforms the seasonal autoregressive integrated moving average model when forecasting intraday directional movements as well as when forecasting the index close price.",
  "tags": [
    "Statistical Machine Learning",
    "Machine Learning"
  ],
  "author": "Adam Balusik",
  "date": "2021-04-20T09:39:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.780Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.780Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f93"
  },
  "title": "Measuring Consistency in Text-based Financial Forecasting Models",
  "abstract": "Financial forecasting has been an important and active area of machine learning research, as even the most modest advantage in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models -- their invariance under meaning-preserving alternations in input -- is a crucial property for building user trust. Despite this, current financial forecasting methods do not consider consistency. To address this problem, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that current text-based methods are not suitable for robustly predicting market information. All resources are available at https://github.com/yingpengma/fintrust.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Linyi Yang",
  "date": "2023-05-15T10:32:26",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.781Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.781Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f94"
  },
  "title": "Financial Time Series Forecasting with Deep Learning : A Systematic Literature Review: 2005-2019",
  "abstract": "Financial time series forecasting is, without a doubt, the top choice of computational intelligence for finance researchers from both academia and financial industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers came up with various models and a vast number of studies have been published accordingly. As such, a significant amount of surveys exist covering ML for financial time series forecasting studies. Lately, Deep Learning (DL) models started appearing within the field, with results that significantly outperform traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting research, there is a lack of review papers that were solely focused on DL for finance. Hence, our motivation in this paper is to provide a comprehensive literature review on DL studies for financial time series forecasting implementations. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, commodity forecasting, but also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Long-Short Term Memory (LSTM). We also tried to envision the future for the field by highlighting the possible setbacks and opportunities, so the interested researchers can benefit.",
  "tags": [
    "Machine Learning",
    "Computational Finance",
    "Statistical Machine Learning"
  ],
  "author": "Omer Berat Sezer",
  "date": "2019-11-29T18:43:18",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.782Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.782Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f95"
  },
  "title": "Numeral Understanding in Financial Tweets for Fine-grained Crowd-based Forecasting",
  "abstract": "Numerals that contain much information in financial documents are crucial for financial decision making. They play different roles in financial analysis processes. This paper is aimed at understanding the meanings of numerals in financial tweets for fine-grained crowd-based forecasting. We propose a taxonomy that classifies the numerals in financial tweets into 7 categories, and further extend some of these categories into several subcategories. Neural network-based models with word and character-level encoders are proposed for 7-way classification and 17-way classification. We perform backtest to confirm the effectiveness of the numeric opinions made by the crowd. This work is the first attempt to understand numerals in financial social media data, and we provide the first comparison of fine-grained opinion of individual investors and analysts based on their forecast price. The numeral corpus used in our experiments, called FinNum 1.0 , is available for research purposes.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Chung-Chi Chen",
  "date": "2018-09-14T11:11:37",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.783Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.783Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f96"
  },
  "title": "Few-Shot Learning Patterns in Financial Time-Series for Trend-Following Strategies",
  "abstract": "Forecasting models for systematic trading strategies do not adapt quickly when financial market conditions rapidly change, as was seen in the advent of the COVID-19 pandemic in 2020, causing many forecasting models to take loss-making positions. To deal with such situations, we propose a novel time-series trend-following forecaster that can quickly adapt to new market conditions, referred to as regimes. We leverage recent developments from the deep learning community and use few-shot learning. We propose the Cross Attentive Time-Series Trend Network -- X-Trend -- which takes positions attending over a context set of financial time-series regimes. X-Trend transfers trends from similar patterns in the context set to make forecasts, then subsequently takes positions for a new distinct target regime. By quickly adapting to new financial regimes, X-Trend increases Sharpe ratio by 18.9% over a neural forecaster and 10-fold over a conventional Time-series Momentum strategy during the turbulent market period from 2018 to 2023. Our strategy recovers twice as quickly from the COVID-19 drawdown compared to the neural-forecaster. X-Trend can also take zero-shot positions on novel unseen financial assets obtaining a 5-fold Sharpe ratio increase versus a neural time-series trend forecaster over the same period. Furthermore, the cross-attention mechanism allows us to interpret the relationship between forecasts and patterns in the context set.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Kieran Wood",
  "date": "2023-10-16T15:20:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.784Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.784Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f97"
  },
  "title": "Scenario Analysis with Multivariate Bayesian Machine Learning Models",
  "abstract": "We present an econometric framework that adapts tools for scenario analysis, such as variants of conditional forecasts and impulse response functions, for use with dynamic nonparametric multivariate models. We demonstrate the utility of our approach with simulated data and three real-world applications: (1) scenario-based conditional forecasts aligned with Federal Reserve stress test assumptions, measuring (2) macroeconomic risk under varying financial conditions, and (3) asymmetric effects of US-based financial shocks and their international spillovers. Our results indicate the importance of nonlinearities and asymmetries in dynamic relationships between macroeconomic and financial variables.",
  "tags": [
    "Econometrics"
  ],
  "author": "Michael Pfarrhofer",
  "date": "2025-02-12T14:30:57",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.785Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.785Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f98"
  },
  "title": "A K-means Algorithm for Financial Market Risk Forecasting",
  "abstract": "Financial market risk forecasting involves applying mathematical models, historical data analysis and statistical methods to estimate the impact of future market movements on investments. This process is crucial for investors to develop strategies, financial institutions to manage assets and regulators to formulate policy. In today's society, there are problems of high error rate and low precision in financial market risk prediction, which greatly affect the accuracy of financial market risk prediction. K-means algorithm in machine learning is an effective risk prediction technique for financial market. This study uses K-means algorithm to develop a financial market risk prediction system, which significantly improves the accuracy and efficiency of financial market risk prediction. Ultimately, the outcomes of the experiments confirm that the K-means algorithm operates with user-friendly simplicity and achieves a 94.61% accuracy rate",
  "tags": [
    "Statistical Finance",
    "Machine Learning"
  ],
  "author": "Jinxin Xu",
  "date": "2024-05-21T02:24:46",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.786Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.786Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f99"
  },
  "title": "Comparative Analysis of Machine Learning, Hybrid, and Deep Learning Forecasting Models Evidence from European Financial Markets and Bitcoins",
  "abstract": "This study analyzes the transmission of market uncertainty on key European financial markets and the cryptocurrency market over an extended period, encompassing the pre, during, and post-pandemic periods. Daily financial market indices and price observations are used to assess the forecasting models. We compare statistical, machine learning, and deep learning forecasting models to evaluate the financial markets, such as the ARIMA, hybrid ETS-ANN, and kNN predictive models. The study results indicate that predicting financial market fluctuations is challenging, and the accuracy levels are generally low in several instances. ARIMA and hybrid ETS-ANN models perform better over extended periods compared to the kNN model, with ARIMA being the best-performing model in 2018-2021 and the hybrid ETS-ANN model being the best-performing model in most of the other subperiods. Still, the kNN model outperforms the others in several periods, depending on the observed accuracy measure. Researchers have advocated using parametric and non-parametric modeling combinations to generate better results. In this study, the results suggest that the hybrid ETS-ANN model is the best-performing model despite its moderate level of accuracy. Thus, the hybrid ETS-ANN model is a promising financial time series forecasting approach. The findings offer financial analysts an additional source that can provide valuable insights for investment decisions.",
  "tags": [
    "Statistical Finance",
    "Econometrics",
    "Risk Management"
  ],
  "author": "Apostolos Ampountolas",
  "date": "2023-07-17T21:25:00",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.787Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.787Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f9a"
  },
  "title": "DAM: A Universal Dual Attention Mechanism for Multimodal Timeseries Cryptocurrency Trend Forecasting",
  "abstract": "In the distributed systems landscape, Blockchain has catalyzed the rise of cryptocurrencies, merging enhanced security and decentralization with significant investment opportunities. Despite their potential, current research on cryptocurrency trend forecasting often falls short by simplistically merging sentiment data without fully considering the nuanced interplay between financial market dynamics and external sentiment influences. This paper presents a novel Dual Attention Mechanism (DAM) for forecasting cryptocurrency trends using multimodal time-series data. Our approach, which integrates critical cryptocurrency metrics with sentiment data from news and social media analyzed through CryptoBERT, addresses the inherent volatility and prediction challenges in cryptocurrency markets. By combining elements of distributed systems, natural language processing, and financial forecasting, our method outperforms conventional models like LSTM and Transformer by up to 20\\% in prediction accuracy. This advancement deepens the understanding of distributed systems and has practical implications in financial markets, benefiting stakeholders in cryptocurrency and blockchain technologies. Moreover, our enhanced forecasting approach can significantly support decentralized science (DeSci) by facilitating strategic planning and the efficient adoption of blockchain technologies, improving operational efficiency and financial risk management in the rapidly evolving digital asset domain, thus ensuring optimal resource allocation.",
  "tags": [
    "Computation and Language",
    "Cryptography and Security",
    "Computational Finance"
  ],
  "author": "Yihang Fu",
  "date": "2024-05-01T13:58:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.788Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.788Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f9b"
  },
  "title": "Volatility Forecasting in Global Financial Markets Using TimeMixer",
  "abstract": "Predicting volatility in financial markets, including stocks, index ETFs, foreign exchange, and cryptocurrencies, remains a challenging task due to the inherent complexity and non-linear dynamics of these time series. In this study, I apply TimeMixer, a state-of-the-art time series forecasting model, to predict the volatility of global financial assets. TimeMixer utilizes a multiscale-mixing approach that effectively captures both short-term and long-term temporal patterns by analyzing data across different scales. My empirical results reveal that while TimeMixer performs exceptionally well in short-term volatility forecasting, its accuracy diminishes for longer-term predictions, particularly in highly volatile markets. These findings highlight TimeMixer's strength in capturing short-term volatility, making it highly suitable for practical applications in financial risk management, where precise short-term forecasts are critical. However, the model's limitations in long-term forecasting point to potential areas for further refinement.",
  "tags": [
    "Statistical Finance",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Alex Li",
  "date": "2024-09-27T17:35:28",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.789Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.789Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f9c"
  },
  "title": "Forecasting in multivariate irregularly sampled time series with missing values",
  "abstract": "Sparse and irregularly sampled multivariate time series are common in clinical, climate, financial and many other domains. Most recent approaches focus on classification, regression or forecasting tasks on such data. In forecasting, it is necessary to not only forecast the right value but also to forecast when that value will occur in the irregular time series. In this work, we present an approach to forecast not only the values but also the time at which they are expected to occur.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Shivam Srivastava",
  "date": "2020-04-06T01:49:46",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.791Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.791Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f9d"
  },
  "title": "Enhancing Financial Market Predictions: Causality-Driven Feature Selection",
  "abstract": "This paper introduces the FinSen dataset that revolutionizes financial market analysis by integrating economic and financial news articles from 197 countries with stock market data. The dataset's extensive coverage spans 15 years from 2007 to 2023 with temporal information, offering a rich, global perspective with 160,000 records on financial market news. Our study leverages causally validated sentiment scores and LSTM models to enhance market forecast accuracy and reliability. Utilizing the FinSen dataset, we introduce an innovative Focal Calibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent with the DAN 3 model. This not only improves prediction accuracy but also aligns probabilistic forecasts closely with real outcomes, crucial for the financial sector where predicted probability is paramount. Our approach demonstrates the effectiveness of combining sentiment analysis with precise calibration techniques for trustworthy financial forecasting where the cost of misinterpretation can be high. Finsen Data can be found at [this github URL](https://github.com/EagleAdelaide/FinSen_Dataset.git).",
  "tags": [
    "Machine Learning",
    "Computation and Language"
  ],
  "author": "Wenhao Liang",
  "date": "2024-08-02T04:40:15",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.792Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.792Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f9e"
  },
  "title": "An adaptive volatility method for probabilistic forecasting and its application to the M6 financial forecasting competition",
  "abstract": "In this paper, we address the problem of probabilistic forecasting using an adaptive volatility method rooted in classical time-varying volatility models and leveraging online stochastic optimization algorithms. These principles were successfully applied in the M6 forecasting competition under the team named AdaGaussMC. Our approach takes a unique path by embracing the Efficient Market Hypothesis (EMH) instead of trying to beat the market directly. We focus on evaluating the efficient market, emphasizing the importance of online forecasting in adapting to the dynamic nature of financial markets. The three key points of our approach are: (a) apply the univariate time-varying volatility model AdaVol, (b) obtain probabilistic forecasts of future returns, and (c) optimize the competition metrics using stochastic gradient-based algorithms. We contend that the simplicity of our approach contributes to its robustness and consistency. Remarkably, our performance in the M6 competition resulted in an overall 7th ranking, with a noteworthy 5th position in the forecasting task. This achievement, considering the perceived simplicity of our approach, underscores the efficacy of our adaptive volatility method in the realm of probabilistic forecasting.",
  "tags": [
    "Statistical Finance"
  ],
  "author": "Joseph de Vilmarest",
  "date": "2023-03-03T11:18:21",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.793Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.793Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006f9f"
  },
  "title": "The Governance of Physical Artificial Intelligence",
  "abstract": "Physical artificial intelligence can prove to be one of the most important challenges of the artificial intelligence. The governance of physical artificial intelligence would define its responsible intelligent application in the society.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Yingbo Li",
  "date": "2023-04-06T08:26:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.794Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.794Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa0"
  },
  "title": "Does an artificial intelligence perform market manipulation with its own discretion? -- A genetic algorithm learns in an artificial market simulation",
  "abstract": "Who should be charged with responsibility for an artificial intelligence performing market manipulation have been discussed. In this study, I constructed an artificial intelligence using a genetic algorithm that learns in an artificial market simulation, and investigated whether the artificial intelligence discovers market manipulation through learning with an artificial market simulation despite a builder of artificial intelligence has no intention of market manipulation. As a result, the artificial intelligence discovered market manipulation as an optimal investment strategy. This result suggests necessity of regulation, such as obligating builders of artificial intelligence to prevent artificial intelligence from performing market manipulation.",
  "tags": [
    "Computational Finance",
    "Risk Management"
  ],
  "author": "Takanobu Mizuta",
  "date": "2020-05-21T07:00:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.795Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.795Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa1"
  },
  "title": "The case for psychometric artificial general intelligence",
  "abstract": "A short review of the literature on measurement and detection of artificial general intelligence is made. Proposed benchmarks and tests for artificial general intelligence are critically evaluated against multiple criteria. Based on the findings, the most promising approaches are identified and some useful directions for future work are proposed.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Mark McPherson",
  "date": "2020-12-27T23:45:03",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.799Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.799Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa2"
  },
  "title": "Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence (1997)",
  "abstract": "This is the Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Dan Geiger",
  "date": "2013-04-13T20:44:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.800Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.800Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa3"
  },
  "title": "Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (1993)",
  "abstract": "This is the Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence, which was held in Washington, DC, July 9-11, 1993",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "David Heckerman",
  "date": "2013-04-13T21:03:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.801Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.801Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa4"
  },
  "title": "Proceedings of the Second Conference on Uncertainty in Artificial Intelligence (1986)",
  "abstract": "This is the Proceedings of the Second Conference on Uncertainty in Artificial Intelligence, which was held in Philadelphia, PA, August 8-10, 1986",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Laveen Kanal",
  "date": "2013-04-13T21:37:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.802Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.802Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa5"
  },
  "title": "Artificial Intelligence in Humans",
  "abstract": "In this paper, I put forward that in many instances, thinking mechanisms are equivalent to artificial intelligence modules programmed into the human mind.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Michael Swan Laufer",
  "date": "2013-10-30T14:19:49",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.803Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.803Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa6"
  },
  "title": "AAAI FSS-18: Artificial Intelligence in Government and Public Sector Proceedings",
  "abstract": "Proceedings of the AAAI Fall Symposium on Artificial Intelligence in Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Frank Stein",
  "date": "2018-10-14T11:40:30",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.804Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.804Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa7"
  },
  "title": "Watershed of Artificial Intelligence: Human Intelligence, Machine Intelligence, and Biological Intelligence",
  "abstract": "This article reviews the \"Once learning\" mechanism that was proposed 23 years ago and the subsequent successes of \"One-shot learning\" in image classification and \"You Only Look Once - YOLO\" in objective detection. Analyzing the current development of Artificial Intelligence (AI), the proposal is that AI should be clearly divided into the following categories: Artificial Human Intelligence (AHI), Artificial Machine Intelligence (AMI), and Artificial Biological Intelligence (ABI), which will also be the main directions of theory and application development for AI. As a watershed for the branches of AI, some classification standards and methods are discussed: 1) Human-oriented, machine-oriented, and biological-oriented AI R&D; 2) Information input processed by Dimensionality-up or Dimensionality-reduction; 3) The use of one/few or large samples for knowledge learning.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Li Weigang",
  "date": "2021-04-27T13:03:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.805Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.805Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa8"
  },
  "title": "Perspective: Purposeful Failure in Artificial Life and Artificial Intelligence",
  "abstract": "Complex systems fail. I argue that failures can be a blueprint characterizing living organisms and biological intelligence, a control mechanism to increase complexity in evolutionary simulations, and an alternative to classical fitness optimization. Imitating biological successes in Artificial Life and Artificial Intelligence can be misleading; imitating failures offers a path towards understanding and emulating life it in artificial systems.",
  "tags": [
    "Artificial Intelligence",
    "Computers and Society",
    "Machine Learning",
    "Neural and Evolutionary Computing"
  ],
  "author": "Lana Sinapayen",
  "date": "2021-02-24T05:43:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.806Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.806Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fa9"
  },
  "title": "Comprehensible Artificial Intelligence on Knowledge Graphs: A survey",
  "abstract": "Artificial Intelligence applications gradually move outside the safe walls of research labs and invade our daily lives. This is also true for Machine Learning methods on Knowledge Graphs, which has led to a steady increase in their application since the beginning of the 21st century. However, in many applications, users require an explanation of the Artificial Intelligences decision. This led to increased demand for Comprehensible Artificial Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible Artificial Intelligence, due to their ability to display connected data, i.e. knowledge, in a human- as well as machine-readable way. This survey gives a short history to Comprehensible Artificial Intelligence on Knowledge Graphs. Furthermore, we contribute by arguing that the concept Explainable Artificial Intelligence is overloaded and overlapping with Interpretable Machine Learning. By introducing the parent concept Comprehensible Artificial Intelligence, we provide a clear-cut distinction of both concepts while accounting for their similarities. Thus, we provide in this survey a case for Comprehensible Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine Learning on Knowledge Graphs and Explainable Artificial Intelligence on Knowledge Graphs. This leads to the introduction of a novel taxonomy for Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a comprehensive overview of the research on Comprehensible Artificial Intelligence on Knowledge Graphs is presented and put into the context of the taxonomy. Finally, research gaps in the field of Comprehensible Artificial Intelligence on Knowledge Graphs are identified for future research.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Simon Schramm",
  "date": "2024-04-04T14:57:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.807Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.807Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006faa"
  },
  "title": "Human $\\neq$ AGI",
  "abstract": "Terms Artificial General Intelligence (AGI) and Human-Level Artificial Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail of Artificial Intelligence (AI) research, creation of a machine capable of achieving goals in a wide range of environments. However, widespread implicit assumption of equivalence between capabilities of AGI and HLAI appears to be unjustified, as humans are not general intelligences. In this paper, we will prove this distinction.",
  "tags": [
    "Computers and Society",
    "Artificial Intelligence"
  ],
  "author": "Roman V. Yampolskiy",
  "date": "2020-07-11T14:06:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.808Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.808Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fab"
  },
  "title": "Artificial Intelligence Technology analysis using Artificial Intelligence patent through Deep Learning model and vector space model",
  "abstract": "Thanks to rapid development of artificial intelligence technology in recent years, the current artificial intelligence technology is contributing to many part of society. Education, environment, medical care, military, tourism, economy, politics, etc. are having a very large impact on society as a whole. For example, in the field of education, there is an artificial intelligence tutoring system that automatically assigns tutors based on student's level. In the field of economics, there are quantitative investment methods that automatically analyze large amounts of data to find investment laws to create investment models or predict changes in financial markets. As such, artificial intelligence technology is being used in various fields. So, it is very important to know exactly what factors have an important influence on each field of artificial intelligence technology and how the relationship between each field is connected. Therefore, it is necessary to analyze artificial intelligence technology in each field. In this paper, we analyze patent documents related to artificial intelligence technology. We propose a method for keyword analysis within factors using artificial intelligence patent data sets for artificial intelligence technology analysis. This is a model that relies on feature engineering based on deep learning model named KeyBERT, and using vector space model. A case study of collecting and analyzing artificial intelligence patent data was conducted to show how the proposed model can be applied to real world problems.",
  "tags": [
    "Information Retrieval",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Yongmin Yoo",
  "date": "2021-11-08T00:10:49",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.809Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.809Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fac"
  },
  "title": "Three IQs of AI Systems and their Testing Methods",
  "abstract": "The rapid development of artificial intelligence has brought the artificial intelligence threat theory as well as the problem about how to evaluate the intelligence level of intelligent products. Both need to find a quantitative method to evaluate the intelligence level of intelligence systems, including human intelligence. Based on the standard intelligence system and the extended Von Neumann architecture, this paper proposes General IQ, Service IQ and Value IQ evaluation methods for intelligence systems, depending on different evaluation purposes. Among them, the General IQ of intelligence systems is to answer the question of whether the artificial intelligence can surpass the human intelligence, which is reflected in putting the intelligence systems on an equal status and conducting the unified evaluation. The Service IQ and Value IQ of intelligence systems are used to answer the question of how the intelligent products can better serve the human, reflecting the intelligence and required cost of each intelligence system as a product in the process of serving human.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Feng Liu",
  "date": "2017-12-14T17:49:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.810Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.810Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fad"
  },
  "title": "Examining correlation between trust and transparency with explainable artificial intelligence",
  "abstract": "Trust between humans and artificial intelligence(AI) is an issue which has implications in many fields of human computer interaction. The current issue with artificial intelligence is a lack of transparency into its decision making, and literature shows that increasing transparency increases trust. Explainable artificial intelligence has the ability to increase transparency of AI, which could potentially increase trust for humans. This paper attempts to use the task of predicting yelp review star ratings with assistance from an explainable and non explainable artificial intelligence to see if trust is increased with increased transparency. Results show that for these tasks, explainable artificial intelligence provided significant increase in trust as a measure of influence.",
  "tags": [
    "Human-Computer Interaction"
  ],
  "author": "Arnav Kartikeya",
  "date": "2021-08-10T16:24:30",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.811Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.811Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fae"
  },
  "title": "The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence",
  "abstract": "We attempt to define what is necessary to construct an Artificial Scientist, explore and evaluate several approaches to artificial general intelligence (AGI) which may facilitate this, conclude that a unified or hybrid approach is necessary and explore two theories that satisfy this requirement to some degree.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Michael Timothy Bennett",
  "date": "2021-10-05T05:58:23",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.812Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.812Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006faf"
  },
  "title": "Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence (2011)",
  "abstract": "This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Fabio Cozman",
  "date": "2012-05-11T18:35:50",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.813Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.813Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb0"
  },
  "title": "Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (2010)",
  "abstract": "This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11 2010.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Peter Grunwald",
  "date": "2012-05-11T18:40:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.814Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.814Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb1"
  },
  "title": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (2009)",
  "abstract": "This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21 2009.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Jeff Bilmes",
  "date": "2012-06-13T16:43:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.815Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.815Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb2"
  },
  "title": "Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (2008)",
  "abstract": "This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "David McAllester",
  "date": "2012-08-25T18:22:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.816Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.816Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb3"
  },
  "title": "Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence (2005)",
  "abstract": "This is the Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29 2005.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Fahiem Bacchus",
  "date": "2012-08-25T18:44:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.817Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.817Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb4"
  },
  "title": "Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (2006)",
  "abstract": "This is the Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence, which was held in Cambridge, MA, July 13 - 16 2006.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Rina Dechter",
  "date": "2012-08-25T18:45:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.818Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.818Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb5"
  },
  "title": "Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (2004)",
  "abstract": "This is the Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence, which was held in Banff, Canada, July 7 - 11 2004.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Max Chickering",
  "date": "2012-08-25T18:48:34",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.819Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.819Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb6"
  },
  "title": "Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence (2012)",
  "abstract": "This is the Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence, which was held on Catalina Island, CA August 14-18 2012.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Nando de Freitas",
  "date": "2013-01-19T22:32:52",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.820Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.820Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb7"
  },
  "title": "Some Insights into Lifelong Reinforcement Learning Systems",
  "abstract": "A lifelong reinforcement learning system is a learning system that has the ability to learn through trail-and-error interaction with the environment over its lifetime. In this paper, I give some arguments to show that the traditional reinforcement learning paradigm fails to model this type of learning system. Some insights into lifelong reinforcement learning are provided, along with a simplistic prototype lifelong reinforcement learning system.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Changjian Li",
  "date": "2020-01-27T07:26:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.821Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.821Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb8"
  },
  "title": "Counterexample-Guided Repair of Reinforcement Learning Systems Using Safety Critics",
  "abstract": "Naively trained Deep Reinforcement Learning agents may fail to satisfy vital safety constraints. To avoid costly retraining, we may desire to repair a previously trained reinforcement learning agent to obviate unsafe behaviour. We devise a counterexample-guided repair algorithm for repairing reinforcement learning systems leveraging safety critics. The algorithm jointly repairs a reinforcement learning agent and a safety critic using gradient-based constrained optimisation.",
  "tags": [
    "Machine Learning"
  ],
  "author": "David Boetius",
  "date": "2024-05-24T10:56:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.822Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.822Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fb9"
  },
  "title": "Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey",
  "abstract": "Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Artificial Intelligence"
  ],
  "author": "Ngan Le",
  "date": "2021-08-25T23:01:48",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.823Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.823Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fba"
  },
  "title": "Causal Reinforcement Learning: A Survey",
  "abstract": "Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence"
  ],
  "author": "Zhihong Deng",
  "date": "2023-07-04T03:00:43",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.824Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.824Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fbb"
  },
  "title": "Distributed Deep Reinforcement Learning: A Survey and A Multi-Player Multi-Agent Learning Toolbox",
  "abstract": "With the breakthrough of AlphaGo, deep reinforcement learning becomes a recognized technique for solving sequential decision-making problems. Despite its reputation, data inefficiency caused by its trial and error learning mechanism makes deep reinforcement learning hard to be practical in a wide range of areas. Plenty of methods have been developed for sample efficient deep reinforcement learning, such as environment modeling, experience transfer, and distributed modifications, amongst which, distributed deep reinforcement learning has shown its potential in various applications, such as human-computer gaming, and intelligent transportation. In this paper, we conclude the state of this exciting field, by comparing the classical distributed deep reinforcement learning methods, and studying important components to achieve efficient distributed learning, covering single player single agent distributed deep reinforcement learning to the most complex multiple players multiple agents distributed deep reinforcement learning. Furthermore, we review recently released toolboxes that help to realize distributed deep reinforcement learning without many modifications of their non-distributed versions. By analyzing their strengths and weaknesses, a multi-player multi-agent distributed deep reinforcement learning toolbox is developed and released, which is further validated on Wargame, a complex environment, showing usability of the proposed toolbox for multiple players and multiple agents distributed deep reinforcement learning under complex games. Finally, we try to point out challenges and future trends, hoping this brief review can provide a guide or a spark for researchers who are interested in distributed deep reinforcement learning.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Multiagent Systems"
  ],
  "author": "Qiyue Yin",
  "date": "2022-12-01T03:39:24",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.827Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.827Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fbc"
  },
  "title": "Transfer Learning in Deep Reinforcement Learning: A Survey",
  "abstract": "Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics from the reinforcement learning perspective and explore their potential challenges that await future research progress.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Statistical Machine Learning"
  ],
  "author": "Zhuangdi Zhu",
  "date": "2020-09-16T18:38:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.828Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.828Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fbd"
  },
  "title": "Memory-two strategies forming symmetric mutual reinforcement learning equilibrium in repeated prisoners' dilemma game",
  "abstract": "We investigate symmetric equilibria of mutual reinforcement learning when both players alternately learn the optimal memory-two strategies against the opponent in the repeated prisoners' dilemma game. We provide a necessary condition for memory-two deterministic strategies to form symmetric equilibria. We then provide three examples of memory-two deterministic strategies which form symmetric mutual reinforcement learning equilibria. We also prove that mutual reinforcement learning equilibria formed by memory-two strategies are also mutual reinforcement learning equilibria when both players use reinforcement learning of memory-$n$ strategies with $n>2$.",
  "tags": [
    "Physics and Society"
  ],
  "author": "Masahiko Ueda",
  "date": "2021-08-05T04:59:18",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.829Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.829Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fbe"
  },
  "title": "Implementing Online Reinforcement Learning with Temporal Neural Networks",
  "abstract": "A Temporal Neural Network (TNN) architecture for implementing efficient online reinforcement learning is proposed and studied via simulation. The proposed T-learning system is composed of a frontend TNN that implements online unsupervised clustering and a backend TNN that implements online reinforcement learning. The reinforcement learning paradigm employs biologically plausible neo-Hebbian three-factor learning rules. As a working example, a prototype implementation of the cart-pole problem (balancing an inverted pendulum) is studied via simulation.",
  "tags": [
    "Neural and Evolutionary Computing"
  ],
  "author": "James E. Smith",
  "date": "2022-04-11T23:10:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.830Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.830Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fbf"
  },
  "title": "Deep Reinforcement Learning for Conversational AI",
  "abstract": "Deep reinforcement learning is revolutionizing the artificial intelligence field. Currently, it serves as a good starting point for constructing intelligent autonomous systems which offer a better knowledge of the visual world. It is possible to scale deep reinforcement learning with the use of deep learning and do amazing tasks such as use of pixels in playing video games. In this paper, key concepts of deep reinforcement learning including reward function, differences between reinforcement learning and supervised learning and models for implementation of reinforcement are discussed. Key challenges related to the implementation of reinforcement learning in conversational AI domain are identified as well as discussed in detail. Various conversational models which are based on deep reinforcement learning (as well as deep learning) are also discussed. In summary, this paper discusses key aspects of deep reinforcement learning which are crucial for designing an efficient conversational AI.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Mahipal Jadeja",
  "date": "2017-09-15T06:18:33",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.831Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.831Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc0"
  },
  "title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
  "abstract": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.",
  "tags": [
    "Information Retrieval",
    "Artificial Intelligence"
  ],
  "author": "Xiaocong Chen",
  "date": "2023-08-22T10:28:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.832Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.832Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc1"
  },
  "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
  "abstract": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Statistical Machine Learning"
  ],
  "author": "Ezgi Korkmaz",
  "date": "2024-01-04T16:45:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.833Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.833Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc2"
  },
  "title": "Reinforcement Teaching",
  "abstract": "Machine learning algorithms learn to solve a task, but are unable to improve their ability to learn. Meta-learning methods learn about machine learning algorithms and improve them so that they learn more quickly. However, existing meta-learning methods are either hand-crafted to improve one specific component of an algorithm or only work with differentiable algorithms. We develop a unifying meta-learning framework, called Reinforcement Teaching, to improve the learning process of \\emph{any} algorithm. Under Reinforcement Teaching, a teaching policy is learned, through reinforcement, to improve a student's learning algorithm. To learn an effective teaching policy, we introduce the parametric-behavior embedder that learns a representation of the student's learnable parameters from its input/output behavior. We further use learning progress to shape the teacher's reward, allowing it to more quickly maximize the student's performance. To demonstrate the generality of Reinforcement Teaching, we conduct experiments in which a teacher learns to significantly improve both reinforcement and supervised learning algorithms. Reinforcement Teaching outperforms previous work using heuristic reward functions and state representations, as well as other parameter representations.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Calarina Muslimani",
  "date": "2022-04-25T18:04:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.834Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.834Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc3"
  },
  "title": "Generative Adversarial Imitation Learning",
  "abstract": "Consider learning a policy from example expert behavior, without interaction with the expert or access to reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data, as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between imitation learning and generative adversarial networks, from which we derive a model-free imitation learning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence"
  ],
  "author": "Jonathan Ho",
  "date": "2016-06-10T20:51:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.835Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.835Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc4"
  },
  "title": "Two-Memory Reinforcement Learning",
  "abstract": "While deep reinforcement learning has shown important empirical success, it tends to learn relatively slow due to slow propagation of rewards information and slow update of parametric neural networks. Non-parametric episodic memory, on the other hand, provides a faster learning alternative that does not require representation learning and uses maximum episodic return as state-action values for action selection. Episodic memory and reinforcement learning both have their own strengths and weaknesses. Notably, humans can leverage multiple memory systems concurrently during learning and benefit from all of them. In this work, we propose a method called Two-Memory reinforcement learning agent (2M) that combines episodic memory and reinforcement learning to distill both of their strengths. The 2M agent exploits the speed of the episodic memory part and the optimality and the generalization capacity of the reinforcement learning part to complement each other. Our experiments demonstrate that the 2M agent is more data efficient and outperforms both pure episodic memory and pure reinforcement learning, as well as a state-of-the-art memory-augmented RL agent. Moreover, the proposed approach provides a general framework that can be used to combine any episodic memory agent with other off-policy reinforcement learning algorithms.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence"
  ],
  "author": "Zhao Yang",
  "date": "2023-04-20T05:39:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.836Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.836Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc5"
  },
  "title": "Recruitment-imitation Mechanism for Evolutionary Reinforcement Learning",
  "abstract": "Reinforcement learning, evolutionary algorithms and imitation learning are three principal methods to deal with continuous control tasks. Reinforcement learning is sample efficient, yet sensitive to hyper-parameters setting and needs efficient exploration; Evolutionary algorithms are stable, but with low sample efficiency; Imitation learning is both sample efficient and stable, however it requires the guidance of expert data. In this paper, we propose Recruitment-imitation Mechanism (RIM) for evolutionary reinforcement learning, a scalable framework that combines advantages of the three methods mentioned above. The core of this framework is a dual-actors and single critic reinforcement learning agent. This agent can recruit high-fitness actors from the population of evolutionary algorithms, which instructs itself to learn from experience replay buffer. At the same time, low-fitness actors in the evolutionary population can imitate behavior patterns of the reinforcement learning agent and improve their adaptability. Reinforcement and imitation learners in this framework can be replaced with any off-policy actor-critic reinforcement learner or data-driven imitation learner. We evaluate RIM on a series of benchmarks for continuous control tasks in Mujoco. The experimental results show that RIM outperforms prior evolutionary or reinforcement learning methods. The performance of RIM's components is significantly better than components of previous evolutionary reinforcement learning algorithm, and the recruitment using soft update enables reinforcement learning agent to learn faster than that using hard update.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Neural and Evolutionary Computing"
  ],
  "author": "Shuai Lü",
  "date": "2019-12-13T03:26:14",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.837Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.837Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc6"
  },
  "title": "Accelerate Reinforcement Learning with PID Controllers in the Pendulum Simulations",
  "abstract": "We propose a Proportional Integral Derivative (PID) controller-based coaching scheme to expedite reinforcement learning (RL).",
  "tags": [
    "Systems and Control (Electrical Engineering)",
    "Machine Learning",
    "Systems and Control"
  ],
  "author": "Liping Bai",
  "date": "2022-10-03T08:59:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.838Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.838Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc7"
  },
  "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement Learning",
  "abstract": "This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.",
  "tags": [
    "Statistical Machine Learning",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Nick Erickson",
  "date": "2017-06-19T00:16:24",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.840Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.840Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc8"
  },
  "title": "Interpretable Reinforcement Learning with Ensemble Methods",
  "abstract": "We propose to use boosted regression trees as a way to compute human-interpretable solutions to reinforcement learning problems. Boosting combines several regression trees to improve their accuracy without significantly reducing their inherent interpretability. Prior work has focused independently on reinforcement learning and on interpretable machine learning, but there has been little progress in interpretable reinforcement learning. Our experimental results show that boosted regression trees compute solutions that are both interpretable and match the quality of leading reinforcement learning methods.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Alexander Brown",
  "date": "2018-09-19T03:23:35",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.841Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.841Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fc9"
  },
  "title": "Unsupervised Meta-Learning for Reinforcement Learning",
  "abstract": "Meta-learning algorithms use past experience to learn to quickly solve new tasks. In the context of reinforcement learning, meta-learning algorithms acquire reinforcement learning procedures to solve new problems more efficiently by utilizing experience from prior tasks. The performance of meta-learning algorithms depends on the tasks available for meta-training: in the same way that supervised learning generalizes best to test points drawn from the same distribution as the training points, meta-learning methods generalize best to tasks from the same distribution as the meta-training tasks. In effect, meta-reinforcement learning offloads the design burden from algorithm design to task design. If we can automate the process of task design as well, we can devise a meta-learning algorithm that is truly automated. In this work, we take a step in this direction, proposing a family of unsupervised meta-learning algorithms for reinforcement learning. We motivate and describe a general recipe for unsupervised meta-reinforcement learning, and present an instantiation of this approach. Our conceptual and theoretical contributions consist of formulating the unsupervised meta-reinforcement learning problem and describing how task proposals based on mutual information can be used to train optimal meta-learners. Our experimental results indicate that unsupervised meta-reinforcement learning effectively acquires accelerated reinforcement learning procedures without the need for manual task design and these procedures exceed the performance of learning from scratch.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Statistical Machine Learning"
  ],
  "author": "Abhishek Gupta",
  "date": "2018-06-12T16:48:52",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.842Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.842Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fca"
  },
  "title": "Lineage Evolution Reinforcement Learning",
  "abstract": "We propose a general agent population learning system, and on this basis, we propose lineage evolution reinforcement learning algorithm. Lineage evolution reinforcement learning is a kind of derivative algorithm which accords with the general agent population learning system. We take the agents in DQN and its related variants as the basic agents in the population, and add the selection, mutation and crossover modules in the genetic algorithm to the reinforcement learning algorithm. In the process of agent evolution, we refer to the characteristics of natural genetic behavior, add lineage factor to ensure the retention of potential performance of agent, and comprehensively consider the current performance and lineage value when evaluating the performance of agent. Without changing the parameters of the original reinforcement learning algorithm, lineage evolution reinforcement learning can optimize different reinforcement learning algorithms. Our experiments show that the idea of evolution with lineage improves the performance of original reinforcement learning algorithm in some games in Atari 2600.",
  "tags": [
    "Neural and Evolutionary Computing",
    "Artificial Intelligence",
    "Machine Learning",
    "Multiagent Systems"
  ],
  "author": "Zeyu Zhang",
  "date": "2020-09-26T11:58:16",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.843Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.843Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fcb"
  },
  "title": "Robust Reinforcement Learning with Distributional Risk-averse formulation",
  "abstract": "Robust Reinforcement Learning tries to make predictions more robust to changes in the dynamics or rewards of the system. This problem is particularly important when the dynamics and rewards of the environment are estimated from the data. In this paper, we approximate the Robust Reinforcement Learning constrained with a $\\Phi$-divergence using an approximate Risk-Averse formulation. We show that the classical Reinforcement Learning formulation can be robustified using standard deviation penalization of the objective. Two algorithms based on Distributional Reinforcement Learning, one for discrete and one for continuous action spaces are proposed and tested in a classical Gym environment to demonstrate the robustness of the algorithms.",
  "tags": [
    "Machine Learning",
    "Optimization and Control",
    "Statistical Machine Learning"
  ],
  "author": "Pierre Clavier",
  "date": "2022-06-14T13:33:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.844Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.844Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fcc"
  },
  "title": "A survey of benchmarking frameworks for reinforcement learning",
  "abstract": "Reinforcement learning has recently experienced increased prominence in the machine learning community. There are many approaches to solving reinforcement learning problems with new techniques developed constantly. When solving problems using reinforcement learning, there are various difficult challenges to overcome. To ensure progress in the field, benchmarks are important for testing new algorithms and comparing with other approaches. The reproducibility of results for fair comparison is therefore vital in ensuring that improvements are accurately judged. This paper provides an overview of different contributions to reinforcement learning benchmarking and discusses how they can assist researchers to address the challenges facing reinforcement learning. The contributions discussed are the most used and recent in the literature. The paper discusses the contributions in terms of implementation, tasks and provided algorithm implementations with benchmarks. The survey aims to bring attention to the wide range of reinforcement learning benchmarking tasks available and to encourage research to take place in a standardised manner. Additionally, this survey acts as an overview for researchers not familiar with the different tasks that can be used to develop and test new reinforcement learning algorithms.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Belinda Stapelberg",
  "date": "2020-11-27T06:32:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.845Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.845Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fcd"
  },
  "title": "Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents",
  "abstract": "Spiking neural network (SNN), compared with depth neural network (DNN), has faster processing speed, lower energy consumption and more biological interpretability, which is expected to approach Strong AI. Reinforcement learning is similar to learning in biology. It is of great significance to study the combination of SNN and RL. We propose the reinforcement learning method of spike distillation network (SDN) with STBP. This method uses distillation to effectively avoid the weakness of STBP, which can achieve SOTA performance in classification, and can obtain a smaller, faster convergence and lower power consumption SNN reinforcement learning model. Experiments show that our method can converge faster than traditional SNN reinforcement learning and DNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN 200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which proves that SDN has lower power consumption than DNN, and the power consumption of SDN is more than 600 times lower than DNN on large-scale devices. SDN provides a new way of SNN reinforcement learning, and can achieve SOTA performance, which proves the possibility of further development of SNN reinforcement learning.",
  "tags": [
    "Neural and Evolutionary Computing",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Ling Zhang",
  "date": "2021-08-05T07:48:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.846Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.846Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fce"
  },
  "title": "A Survey on Offline Model-Based Reinforcement Learning",
  "abstract": "Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by the field, and suggest possible directions for future work.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Systems and Control",
    "Systems and Control (Electrical Engineering)"
  ],
  "author": "Haoyang He",
  "date": "2023-05-05T08:23:56",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.847Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.847Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fcf"
  },
  "title": "Genome-on-Diet: Taming Large-Scale Genomic Analyses via Sparsified Genomics",
  "abstract": "Searching for similar genomic sequences is an essential and fundamental step in biomedical research and an overwhelming majority of genomic analyses. State-of-the-art computational methods performing such comparisons fail to cope with the exponential growth of genomic sequencing data. We introduce the concept of sparsified genomics where we systematically exclude a large number of bases from genomic sequences and enable much faster and more memory-efficient processing of the sparsified, shorter genomic sequences, while providing similar or even higher accuracy compared to processing non-sparsified sequences. Sparsified genomics provides significant benefits to many genomic analyses and has broad applicability. We show that sparsifying genomic sequences greatly accelerates the state-of-the-art read mapper (minimap2) by 2.57-5.38x, 1.13-2.78x, and 3.52-6.28x using real Illumina, HiFi, and ONT reads, respectively, while providing up to 2.1x smaller memory footprint, 2x smaller index size, and more truly detected small and structural variations compared to minimap2. Sparsifying genomic sequences makes containment search through very large genomes and large databases 72.7-75.88x faster and 723.3x more storage-efficient than searching through non-sparsified genomic sequences (with CMash and KMC3). Sparsifying genomic sequences enables robust microbiome discovery by providing 54.15-61.88x faster and 720x more storage-efficient taxonomic profiling of metagenomic samples over the state-of-the-art tool (Metalign). We design and open-source a framework called Genome-on-Diet as an example tool for sparsified genomics, which can be freely downloaded from https://github.com/CMU-SAFARI/Genome-on-Diet.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Mohammed Alser",
  "date": "2022-11-15T14:09:39",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.848Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.848Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd0"
  },
  "title": "Hypergraph covering problems motivated by genome assembly questions",
  "abstract": "The Consecutive-Ones Property (C1P) is a classical concept in discrete mathematics that has been used in several genomics applications, from physical mapping of contemporary genomes to the assembly of ancient genomes. A common issue in genome assembly concerns repeats, genomic sequences that appear in several locations of a genome. Handling repeats leads to a variant of the C1P, the C1P with multiplicity (mC1P), that can also be seen as the problem of covering edges of hypergraphs by linear and circular walks. In the present work, we describe variants of the mC1P that address specific issues of genome assembly, and polynomial time or fixed-parameter algorithms to solve them.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Cedric Chauve",
  "date": "2013-06-18T21:01:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.849Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.849Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd1"
  },
  "title": "Accelerating Genome Analysis via Algorithm-Architecture Co-Design",
  "abstract": "High-throughput sequencing (HTS) technologies have revolutionized the field of genomics, enabling rapid and cost-effective genome analysis for various applications. However, the increasing volume of genomic data generated by HTS technologies presents significant challenges for computational techniques to effectively analyze genomes. To address these challenges, several algorithm-architecture co-design works have been proposed, targeting different steps of the genome analysis pipeline. These works explore emerging technologies to provide fast, accurate, and low-power genome analysis.   This paper provides a brief review of the recent advancements in accelerating genome analysis, covering the opportunities and challenges associated with the acceleration of the key steps of the genome analysis pipeline. Our analysis highlights the importance of integrating multiple steps of genome analysis using suitable architectures to unlock significant performance improvements and reduce data movement and energy consumption. We conclude by emphasizing the need for novel strategies and techniques to address the growing demands of genomic data generation and analysis.",
  "tags": [
    "Architecture"
  ],
  "author": "Onur Mutlu",
  "date": "2023-04-30T14:25:53",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.850Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.850Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd2"
  },
  "title": "Efficient and Scalable Fine-Tune of Language Models for Genome Understanding",
  "abstract": "Although DNA foundation models have advanced the understanding of genomes, they still face significant challenges in the limited scale and diversity of genomic data. This limitation starkly contrasts with the success of natural language foundation models, which thrive on substantially larger scales. Furthermore, genome understanding involves numerous downstream genome annotation tasks with inherent data heterogeneity, thereby necessitating more efficient and robust fine-tuning methods tailored for genomics. Here, we present \\textsc{Lingo}: \\textsc{L}anguage prefix f\\textsc{In}e-tuning for \\textsc{G}en\\textsc{O}mes. Unlike DNA foundation models, \\textsc{Lingo} strategically leverages natural language foundation models' contextual cues, recalibrating their linguistic knowledge to genomic sequences. \\textsc{Lingo} further accommodates numerous, heterogeneous downstream fine-tune tasks by an adaptive rank sampling method that prunes and stochastically reintroduces pruned singular vectors within small computational budgets. Adaptive rank sampling outperformed existing fine-tuning methods on all benchmarked 14 genome understanding tasks, while requiring fewer than 2\\% of trainable parameters as genomic-specific adapters. Impressively, applying these adapters on natural language foundation models matched or even exceeded the performance of DNA foundation models. \\textsc{Lingo} presents a new paradigm of efficient and scalable genome understanding via genomic-specific adapters on language models.",
  "tags": [
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Huixin Zhan",
  "date": "2024-02-12T21:40:45",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.851Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.851Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd3"
  },
  "title": "Progressive Mauve: Multiple alignment of genomes with gene flux and rearrangement",
  "abstract": "Multiple genome alignment remains a challenging problem. Effects of recombination including rearrangement, segmental duplication, gain, and loss can create a mosaic pattern of homology even among closely related organisms. We describe a method to align two or more genomes that have undergone large-scale recombination, particularly genomes that have undergone substantial amounts of gene gain and loss (gene flux). The method utilizes a novel alignment objective score, referred to as a sum-of-pairs breakpoint score. We also apply a probabilistic alignment filtering method to remove erroneous alignments of unrelated sequences, which are commonly observed in other genome alignment methods. We describe new metrics for quantifying genome alignment accuracy which measure the quality of rearrangement breakpoint predictions and indel predictions. The progressive genome alignment algorithm demonstrates markedly improved accuracy over previous approaches in situations where genomes have undergone realistic amounts of genome rearrangement, gene gain, loss, and duplication. We apply the progressive genome alignment algorithm to a set of 23 completely sequenced genomes from the genera Escherichia, Shigella, and Salmonella. The 23 enterobacteria have an estimated 2.46Mbp of genomic content conserved among all taxa and total unique content of 15.2Mbp. We document substantial population-level variability among these organisms driven by homologous recombination, gene gain, and gene loss. Free, open-source software implementing the described genome alignment approach is available from http://gel.ahabs.wisc.edu/mauve .",
  "tags": [
    "Biomolecules"
  ],
  "author": "Aaron E. Darling",
  "date": "2009-10-30T04:59:46",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.852Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.852Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd4"
  },
  "title": "Deep Learning for Genomics: A Concise Overview",
  "abstract": "Advancements in genomic research such as high-throughput sequencing techniques have driven modern genomic studies into \"big data\" disciplines. This data explosion is constantly challenging conventional methods used in genomics. In parallel with the urgent demand for robust algorithms, deep learning has succeeded in a variety of fields such as vision, speech, and text processing. Yet genomics entails unique challenges to deep learning since we are expecting from deep learning a superhuman intelligence that explores beyond our knowledge to interpret the genome. A powerful deep learning model should rely on insightful utilization of task-specific knowledge. In this paper, we briefly discuss the strengths of different deep learning models from a genomic perspective so as to fit each particular task with a proper deep architecture, and remark on practical considerations of developing modern deep learning architectures for genomics. We also provide a concise review of deep learning applications in various aspects of genomic research, as well as pointing out potential opportunities and obstacles for future genomics applications.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Tianwei Yue",
  "date": "2018-02-02T12:50:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.853Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.853Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd5"
  },
  "title": "GDC 2: Compression of large collections of genomes",
  "abstract": "The fall of prices of the high-throughput genome sequencing changes the landscape of modern genomics. A number of large scale projects aimed at sequencing many human genomes are in progress. Genome sequencing also becomes an important aid in the personalized medicine. One of the significant side effects of this change is a necessity of storage and transfer of huge amounts of genomic data. In this paper we deal with the problem of compression of large collections of complete genomic sequences. We propose an algorithm that is able to compress the collection of 1092 human diploid genomes about 9,500 times. This result is about 4 times better than what is offered by the other existing compressors. Moreover, our algorithm is very fast as it processes the data with speed 200MB/s on a modern workstation. In a consequence the proposed algorithm allows storing the complete genomic collections at low cost, e.g., the examined collection of 1092 human genomes needs only about 700MB when compressed, what can be compared to about 6.7 TB of uncompressed FASTA files. The source code is available at http://sun.aei.polsl.pl/REFRESH/index.php?page=projects&project=gdc&subpage=about.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Sebastian Deorowicz",
  "date": "2015-03-05T12:38:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.854Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.854Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd6"
  },
  "title": "A Compressed Self-Index for Genomic Databases",
  "abstract": "Advances in DNA sequencing technology will soon result in databases of thousands of genomes. Within a species, individuals' genomes are almost exact copies of each other; e.g., any two human genomes are 99.9% the same. Relative Lempel-Ziv (RLZ) compression takes advantage of this property: it stores the first genome uncompressed or as an FM-index, then compresses the other genomes with a variant of LZ77 that copies phrases only from the first genome. RLZ achieves good compression and supports fast random access; in this paper we show how to support fast search as well, thus obtaining an efficient compressed self-index.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Travis Gagie",
  "date": "2011-11-05T21:53:33",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.855Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.855Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd7"
  },
  "title": "copMEM: Finding maximal exact matches via sampling both genomes",
  "abstract": "Genome-to-genome comparisons require designating anchor points, which are given by Maximum Exact Matches (MEMs) between their sequences. For large genomes this is a challenging problem and the performance of existing solutions, even in parallel regimes, is not quite satisfactory. We present a new algorithm, copMEM, that allows to sparsely sample both input genomes, with sampling steps being coprime. Despite being a single-threaded implementation, copMEM computes all MEMs of minimum length 100 between the human and mouse genomes in less than 2 minutes, using less than 10 GB of RAM memory.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Szymon Grabowski",
  "date": "2018-05-22T18:58:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.856Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.856Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd8"
  },
  "title": "A Review of Image Mosaicing Techniques",
  "abstract": "Image Mosaicing is a method of constructing multiple images of the same scene into a larger image. The output of the image mosaic will be the union of two input images. Image-mosaicing algorithms are used to get mosaiced image. Image Mosaicing processed is basically divided in to 5 phases. Which includes; Feature point extraction, Image registration, Homography computation, Warping and Blending if Image. Various corner detection algorithm is being used for Feature extraction. This corner produces an efficient and informative output mosaiced image. Image mosaicing is widely used in creating 3D images, medical imaging, computer vision, data from satellites, and military automatic target recognition.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Dushyant Vaghela",
  "date": "2014-05-11T15:13:56",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.857Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.857Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fd9"
  },
  "title": "Recognition-Oriented Low-Light Image Enhancement based on Global and Pixelwise Optimization",
  "abstract": "In this paper, we propose a novel low-light image enhancement method aimed at improving the performance of recognition models. Despite recent advances in deep learning, the recognition of images under low-light conditions remains a challenge. Although existing low-light image enhancement methods have been developed to improve image visibility for human vision, they do not specifically focus on enhancing recognition model performance. Our proposed low-light image enhancement method consists of two key modules: the Global Enhance Module, which adjusts the overall brightness and color balance of the input image, and the Pixelwise Adjustment Module, which refines image features at the pixel level. These modules are trained to enhance input images to improve downstream recognition model performance effectively. Notably, the proposed method can be applied as a frontend filter to improve low-light recognition performance without requiring retraining of downstream recognition models. Experimental results demonstrate that our method improves the performance of pretrained recognition models under low-light conditions and its effectiveness.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Seitaro Ono",
  "date": "2025-01-08T01:09:49",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.858Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.858Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fda"
  },
  "title": "Overview Of Satellite Image Recognition Models",
  "abstract": "In this article, the analysis of existing models of satellite image recognition was carried out, the problems in the field of satellite image recognition as a source of information were considered and analyzed, deep learning methods were compared, and existing image recognition methods were analyzed. The results obtained will be used as a basis for the prospective development of a fire recognition model based on satellite images and the use of recognition results as input data for a cognitive model of forecasting the macro-economic situation based on fuzzy cognitive maps.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Alexey Averkin",
  "date": "2022-12-07T15:33:43",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.859Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.859Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fdb"
  },
  "title": "A Topological Code for Plane Images",
  "abstract": "It is proposed a new code for contours of plane images. This code was applied for optical character recognition of printed and handwritten characters. One can apply it to recognition of any visual images.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Evgeny Shchepin",
  "date": "2012-12-04T18:39:14",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.860Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.860Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fdc"
  },
  "title": "Logical methods of object recognition on satellite images using spatial constraints",
  "abstract": "A logical approach to object recognition on image is proposed. The main idea of the approach is to perform the object recognition as a logical inference on a set of rules describing an object shape.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "R. K. Fedorov",
  "date": "2010-04-27T13:22:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.861Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.861Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fdd"
  },
  "title": "Smart Novel Computer-based Analytical Tool for Image Forgery Authentication",
  "abstract": "This paper presents an integration of image forgery detection with image facial recognition using black propagation neural network (BPNN). We observed that facial image recognition by itself will always give a matching output or closest possible output image for every input image irrespective of the authenticity or otherwise not of the testing input image. Based on this, we are proposing the combination of the blind but powerful automation image forgery detection for entire input images for the BPNN recognition program. Hence, an input image must first be authenticated before being fed into the recognition program. Thus, an image security identification and authentication requirement, any image that fails the authentication/verification stage are not to be used as an input/test image. In addition, the universal smart GUI tool is proposed and designed to perform image forgery detection with the high accuracy of 2% error rate.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Rozita Teymourzadeh",
  "date": "2018-06-10T14:07:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.862Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.862Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fde"
  },
  "title": "Geometric Surface Image Prediction for Image Recognition Enhancement",
  "abstract": "This work presents a method to predict a geometric surface image from a photograph to assist in image recognition. To recognize objects, several images from different conditions are required for training a model or fine-tuning a pre-trained model. In this work, a geometric surface image is introduced as a better representation than its color image counterpart to overcome lighting conditions. The surface image is predicted from a color image. To do so, the geometric surface image together with its color photographs are firstly trained with Generative Adversarial Networks (GAN) model. The trained generator model is then used to predict the geometric surface image from the input color image. The evaluation on a case study of an amulet recognition shows that the predicted geometric surface images contain less ambiguity than their color images counterpart under different lighting conditions and can be used effectively for assisting in image recognition task.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Graphics"
  ],
  "author": "Tanasai Sucontphunt",
  "date": "2020-12-15T17:44:37",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.863Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.863Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fdf"
  },
  "title": "URIE: Universal Image Enhancement for Visual Recognition in the Wild",
  "abstract": "Despite the great advances in visual recognition, it has been witnessed that recognition models trained on clean images of common datasets are not robust against distorted images in the real world. To tackle this issue, we present a Universal and Recognition-friendly Image Enhancement network, dubbed URIE, which is attached in front of existing recognition models and enhances distorted input to improve their performance without retraining them. URIE is universal in that it aims to handle various factors of image degradation and to be incorporated with any arbitrary recognition models. Also, it is recognition-friendly since it is optimized to improve the robustness of following recognition models, instead of perceptual quality of output image. Our experiments demonstrate that URIE can handle various and latent image distortions and improve the performance of existing models for five diverse recognition tasks when input images are degraded.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Taeyoung Son",
  "date": "2020-07-17T13:45:56",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.864Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.864Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe0"
  },
  "title": "Face Reconstruction from Face Embeddings using Adapter to a Face Foundation Model",
  "abstract": "Face recognition systems extract embedding vectors from face images and use these embeddings to verify or identify individuals. Face reconstruction attack (also known as template inversion) refers to reconstructing face images from face embeddings and using the reconstructed face image to enter a face recognition system. In this paper, we propose to use a face foundation model to reconstruct face images from the embeddings of a blackbox face recognition model. The foundation model is trained with 42M images to generate face images from the facial embeddings of a fixed face recognition model. We propose to use an adapter to translate target embeddings into the embedding space of the foundation model. The generated images are evaluated on different face recognition models and different datasets, demonstrating the effectiveness of our method to translate embeddings of different face recognition models. We also evaluate the transferability of reconstructed face images when attacking different face recognition models. Our experimental results show that our reconstructed face images outperform previous reconstruction attacks against face recognition models.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Hatef Otroshi Shahreza",
  "date": "2024-11-06T14:45:41",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.865Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.865Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe1"
  },
  "title": "Image Coding for Machines with Object Region Learning",
  "abstract": "Compression technology is essential for efficient image transmission and storage. With the rapid advances in deep learning, images are beginning to be used for image recognition as well as for human vision. For this reason, research has been conducted on image coding for image recognition, and this field is called Image Coding for Machines (ICM). There are two main approaches in ICM: the ROI-based approach and the task-loss-based approach. The former approach has the problem of requiring an ROI-map as input in addition to the input image. The latter approach has the problems of difficulty in learning the task-loss, and lack of robustness because the specific image recognition model is used to compute the loss function. To solve these problems, we propose an image compression model that learns object regions. Our model does not require additional information as input, such as an ROI-map, and does not use task-loss. Therefore, it is possible to compress images for various image recognition models. In the experiments, we demonstrate the versatility of the proposed method by using three different image recognition models and three different datasets. In addition, we verify the effectiveness of our model by comparing it with previous methods.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Takahiro Shindo",
  "date": "2023-08-27T01:54:03",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.866Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.866Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe2"
  },
  "title": "Ablation study of self-supervised learning for image classification",
  "abstract": "This project focuses on the self-supervised training of convolutional neural networks (CNNs) and transformer networks for the task of image recognition. A simple siamese network with different backbones is used in order to maximize the similarity of two augmented transformed images from the same source image. In this way, the backbone is able to learn visual information without supervision. Finally, the method is evaluated on three image recognition datasets.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Ilias Papastratis",
  "date": "2021-12-04T09:59:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.867Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.867Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe3"
  },
  "title": "Synthesizing brain tumor images and annotations by combining progressive growing GAN and SPADE",
  "abstract": "Training segmentation networks requires large annotated datasets, but manual annotation is time consuming and costly. We here investigate if the combination of a noise-to-image GAN and an image-to-image GAN can be used to synthesize realistic brain tumor images as well as the corresponding tumor annotations (labels), to substantially increase the number of training images. The noise-to-image GAN is used to synthesize new label images, while the image-to-image GAN generates the corresponding MR image from the label image. Our results indicate that the two GANs can synthesize label images and MR images that look realistic, and that adding synthetic images improves the segmentation performance, although the effect is small.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Mehdi Foroozandeh",
  "date": "2020-09-13T07:56:10",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.868Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.868Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe4"
  },
  "title": "A comparative study of generative adversarial networks for image recognition algorithms based on deep learning and traditional methods",
  "abstract": "In this paper, an image recognition algorithm based on the combination of deep learning and generative adversarial network (GAN) is studied, and compared with traditional image recognition methods. The purpose of this study is to evaluate the advantages and application prospects of deep learning technology, especially GAN, in the field of image recognition. Firstly, this paper reviews the basic principles and techniques of traditional image recognition methods, including the classical algorithms based on feature extraction such as SIFT, HOG and their combination with support vector machine (SVM), random forest, and other classifiers. Then, the working principle, network structure, and unique advantages of GAN in image generation and recognition are introduced. In order to verify the effectiveness of GAN in image recognition, a series of experiments are designed and carried out using multiple public image data sets for training and testing. The experimental results show that compared with traditional methods, GAN has excellent performance in processing complex images, recognition accuracy, and anti-noise ability. Specifically, Gans are better able to capture high-dimensional features and details of images, significantly improving recognition performance. In addition, Gans shows unique advantages in dealing with image noise, partial missing information, and generating high-quality images.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Machine Learning"
  ],
  "author": "Yihao Zhong",
  "date": "2024-08-07T06:11:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.869Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.869Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe5"
  },
  "title": "Illegible Text to Readable Text: An Image-to-Image Transformation using Conditional Sliced Wasserstein Adversarial Networks",
  "abstract": "Automatic text recognition from ancient handwritten record images is an important problem in the genealogy domain. However, critical challenges such as varying noise conditions, vanishing texts, and variations in handwriting make the recognition task difficult. We tackle this problem by developing a handwritten-to-machine-print conditional Generative Adversarial network (HW2MP-GAN) model that formulates handwritten recognition as a text-Image-to-text-Image translation problem where a given image, typically in an illegible form, is converted into another image, close to its machine-print form. The proposed model consists of three-components including a generator, and word-level and character-level discriminators. The model incorporates Sliced Wasserstein distance (SWD) and U-Net architectures in HW2MP-GAN for better quality image-to-image transformation. Our experiments reveal that HW2MP-GAN outperforms state-of-the-art baseline cGAN models by almost 30 in Frechet Handwritten Distance (FHD), 0.6 on average Levenshtein distance and 39% in word accuracy for image-to-image translation on IAM database. Further, HW2MP-GAN improves handwritten recognition word accuracy by 1.3% compared to baseline handwritten recognition models on the IAM database.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Mostafa Karimi",
  "date": "2019-10-11T22:01:24",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.870Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.870Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe6"
  },
  "title": "Feature-based Recognition Framework for Super-resolution Images",
  "abstract": "In practical application, the performance of recognition network usually decreases when being applied on super-resolution images. In this paper, we propose a feature-based recognition network combined with GAN (FGAN). Our network improves the recognition accuracy by extracting more features that benefit recognition from SR images. In the experiment, we build three datasets using three different super-resolution algorithm, and our network increases the recognition accuracy by more than 6% comparing with ReaNet50 and DenseNet121.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Jing Hu",
  "date": "2021-12-04T07:30:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.871Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.871Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe7"
  },
  "title": "Deep Learning for Finger Vein Recognition: A Brief Survey of Recent Trend",
  "abstract": "Finger vein image recognition technology plays an important role in biometric recognition and has been successfully applied in many fields. Because veins are buried beneath the skin tissue, finger vein image recognition has an unparalleled advantage, which is not easily disturbed by external factors. This review summarizes 46 papers about deep learning for finger vein image recognition from 2017 to 2021. These papers are summarized according to the tasks of deep neural networks. Besides, we present the challenges and potential development directions of finger vein image recognition.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Artificial Intelligence"
  ],
  "author": "Renye Zhang",
  "date": "2022-07-05T16:14:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.872Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.872Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe8"
  },
  "title": "Pairwise Decomposition of Image Sequences for Active Multi-View Recognition",
  "abstract": "A multi-view image sequence provides a much richer capacity for object recognition than from a single image. However, most existing solutions to multi-view recognition typically adopt hand-crafted, model-based geometric methods, which do not readily embrace recent trends in deep learning. We propose to bring Convolutional Neural Networks to generic multi-view recognition, by decomposing an image sequence into a set of image pairs, classifying each pair independently, and then learning an object classifier by weighting the contribution of each pair. This allows for recognition over arbitrary camera trajectories, without requiring explicit training over the potentially infinite number of camera paths and lengths. Building these pairwise relationships then naturally extends to the next-best-view problem in an active recognition framework. To achieve this, we train a second Convolutional Neural Network to map directly from an observed image to next viewpoint. Finally, we incorporate this into a trajectory optimisation task, whereby the best recognition confidence is sought for a given trajectory length. We present state-of-the-art results in both guided and unguided multi-view recognition on the ModelNet dataset, and show how our method can be used with depth images, greyscale images, or both.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Robotics"
  ],
  "author": "Edward Johns",
  "date": "2016-05-26T16:44:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.873Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.873Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fe9"
  },
  "title": "Image Recognition Using Scale Recurrent Neural Networks",
  "abstract": "Convolutional Neural Network(CNN) has been widely used for image recognition with great success. However, there are a number of limitations of the current CNN based image recognition paradigm. First, the receptive field of CNN is generally fixed, which limits its recognition capacity when the input image is very large. Second, it lacks the computational scalability for dealing with images with different sizes. Third, it is quite different from human visual system for image recognition, which involves both feadforward and recurrent proprocessing. This paper proposes a different paradigm of image recognition, which can take advantages of variable scales of the input images, has more computational scalabilities, and is more similar to image recognition by human visual system. It is based on recurrent neural network (RNN) defined on image scale with an embeded base CNN, which is named Scale Recurrent Neural Network(SRNN). This RNN based approach makes it easier to deal with images with variable sizes, and allows us to borrow existing RNN techniques, such as LSTM and GRU, to further enhance the recognition accuracy. Our experiments show that the recognition accuracy of a base CNN can be significantly boosted using the proposed SRNN models. It also significantly outperforms the scale ensemble method, which integrate the results of performing CNN to the input image at different scales, although the computational overhead of using SRNN is negligible.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Dong-Qing Zhang",
  "date": "2018-03-25T09:16:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.875Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.875Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fea"
  },
  "title": "One Model for Two Tasks: Cooperatively Recognizing and Recovering Low-Resolution Scene Text Images by Iterative Mutual Guidance",
  "abstract": "Scene text recognition (STR) from high-resolution (HR) images has been significantly successful, however text reading on low-resolution (LR) images is still challenging due to insufficient visual information. Therefore, recently many scene text image super-resolution (STISR) models have been proposed to generate super-resolution (SR) images for the LR ones, then STR is done on the SR images, which thus boosts recognition performance. Nevertheless, these methods have two major weaknesses. On the one hand, STISR approaches may generate imperfect or even erroneous SR images, which mislead the subsequent recognition of STR models. On the other hand, as the STISR and STR models are jointly optimized, to pursue high recognition accuracy, the fidelity of SR images may be spoiled. As a result, neither the recognition performance nor the fidelity of STISR models are desirable. Then, can we achieve both high recognition performance and good fidelity? To this end, in this paper we propose a novel method called IMAGE (the abbreviation of Iterative MutuAl GuidancE) to effectively recognize and recover LR scene text images simultaneously. Concretely, IMAGE consists of a specialized STR model for recognition and a tailored STISR model to recover LR images, which are optimized separately. And we develop an iterative mutual guidance mechanism, with which the STR model provides high-level semantic information as clue to the STISR model for better super-resolution, meanwhile the STISR model offers essential low-level pixel clue to the STR model for more accurate recognition. Extensive experiments on two LR datasets demonstrate the superiority of our method over the existing works on both recognition performance and super-resolution fidelity.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Minyi Zhao",
  "date": "2024-09-22T15:05:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.876Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.876Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006feb"
  },
  "title": "Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events",
  "abstract": "Recognizing objects from sparse and noisy events becomes extremely difficult when paired images and category labels do not exist. In this paper, we study label-free event-based object recognition where category labels and paired images are not available. To this end, we propose a joint formulation of object recognition and image reconstruction in a complementary manner. Our method first reconstructs images from events and performs object recognition through Contrastive Language-Image Pre-training (CLIP), enabling better recognition through a rich context of images. Since the category information is essential in reconstructing images, we propose category-guided attraction loss and category-agnostic repulsion loss to bridge the textual features of predicted categories and the visual features of reconstructed images using CLIP. Moreover, we introduce a reliable data sampling strategy and local-global reconstruction consistency to boost joint learning of two tasks. To enhance the accuracy of prediction and quality of reconstruction, we also propose a prototype-based approach using unpaired images. Extensive experiments demonstrate the superiority of our method and its extensibility for zero-shot object recognition. Our project code is available at \\url{https://github.com/Chohoonhee/Ev-LaFOR}.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Hoonhee Cho",
  "date": "2023-08-18T08:28:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.877Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.877Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fec"
  },
  "title": "Reinforcing Generated Images via Meta-learning for One-Shot Fine-Grained Visual Recognition",
  "abstract": "One-shot fine-grained visual recognition often suffers from the problem of having few training examples for new fine-grained classes. To alleviate this problem, off-the-shelf image generation techniques based on Generative Adversarial Networks (GANs) can potentially create additional training images. However, these GAN-generated images are often not helpful for actually improving the accuracy of one-shot fine-grained recognition. In this paper, we propose a meta-learning framework to combine generated images with original images, so that the resulting \"hybrid\" training images improve one-shot learning. Specifically, the generic image generator is updated by a few training instances of novel classes, and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. Our experiments demonstrate consistent improvement over baselines on one-shot fine-grained image classification benchmarks. Furthermore, our analysis shows that the reinforced images have more diversity compared to the original and GAN-generated images.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Satoshi Tsutsui",
  "date": "2022-04-22T13:11:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.878Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.878Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fed"
  },
  "title": "Image-level Classification in Hyperspectral Images using Feature Descriptors, with Application to Face Recognition",
  "abstract": "In this paper, we proposed a novel pipeline for image-level classification in the hyperspectral images. By doing this, we show that the discriminative spectral information at image-level features lead to significantly improved performance in a face recognition task. We also explored the potential of traditional feature descriptors in the hyperspectral images. From our evaluations, we observe that SIFT features outperform the state-of-the-art hyperspectral face recognition methods, and also the other descriptors. With the increasing deployment of hyperspectral sensors in a multitude of applications, we believe that our approach can effectively exploit the spectral information in hyperspectral images, thus beneficial to more accurate classification.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Vivek Sharma",
  "date": "2016-05-11T13:18:22",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.879Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.879Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fee"
  },
  "title": "EmoCAM: Toward Understanding What Drives CNN-based Emotion Recognition",
  "abstract": "Convolutional Neural Networks are particularly suited for image analysis tasks, such as Image Classification, Object Recognition or Image Segmentation. Like all Artificial Neural Networks, however, they are \"black box\" models, and suffer from poor explainability. This work is concerned with the specific downstream task of Emotion Recognition from images, and proposes a framework that combines CAM-based techniques with Object Detection on a corpus level to better understand on which image cues a particular model, in our case EmoNet, relies to assign a specific emotion to an image. We demonstrate that the model mostly focuses on human characteristics, but also explore the pronounced effect of specific image modifications.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Artificial Intelligence"
  ],
  "author": "Youssef Doulfoukar",
  "date": "2024-07-19T13:47:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.880Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.880Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fef"
  },
  "title": "Large Vocabulary Spontaneous Speech Recognition for Tigrigna",
  "abstract": "This thesis proposes and describes a research attempt at designing and developing a speaker independent spontaneous automatic speech recognition system for Tigrigna The acoustic model of the Speech Recognition System is developed using Carnegie Mellon University Automatic Speech Recognition development tool (Sphinx) while the SRIM tool is used for the development of the language model.   Keywords Automatic Speech Recognition Tigrigna language",
  "tags": [
    "Machine Learning"
  ],
  "author": "Ataklti Kahsu",
  "date": "2023-10-15T13:07:41",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.881Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.881Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff0"
  },
  "title": "Speech Enhancement Modeling Towards Robust Speech Recognition System",
  "abstract": "Form about four decades human beings have been dreaming of an intelligent machine which can master the natural speech. In its simplest form, this machine should consist of two subsystems, namely automatic speech recognition (ASR) and speech understanding (SU). The goal of ASR is to transcribe natural speech while SU is to understand the meaning of the transcription. Recognizing and understanding a spoken sentence is obviously a knowledge-intensive process, which must take into account all variable information about the speech communication process, from acoustics to semantics and pragmatics. While developing an Automatic Speech Recognition System, it is observed that some adverse conditions degrade the performance of the Speech Recognition System. In this contribution, speech enhancement system is introduced for enhancing speech signals corrupted by additive noise and improving the performance of Automatic Speech Recognizers in noisy conditions. Automatic speech recognition experiments show that replacing noisy speech signals by the corresponding enhanced speech signals leads to an improvement in the recognition accuracies. The amount of improvement varies with the type of the corrupting noise.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Urmila Shrawankar",
  "date": "2013-05-07T07:21:06",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.882Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.882Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff1"
  },
  "title": "Silent versus modal multi-speaker speech recognition from ultrasound and video",
  "abstract": "We investigate multi-speaker speech recognition from ultrasound images of the tongue and video images of the lips. We train our systems on imaging data from modal speech, and evaluate on matched test sets of two speaking modes: silent and modal speech. We observe that silent speech recognition from imaging data underperforms compared to modal speech recognition, likely due to a speaking-mode mismatch between training and testing. We improve silent speech recognition performance using techniques that address the domain mismatch, such as fMLLR and unsupervised model adaptation. We also analyse the properties of silent and modal speech in terms of utterance duration and the size of the articulatory space. To estimate the articulatory space, we compute the convex hull of tongue splines, extracted from ultrasound tongue images. Overall, we observe that the duration of silent speech is longer than that of modal speech, and that silent speech covers a smaller articulatory space than modal speech. Although these two properties are statistically significant across speaking modes, they do not directly correlate with word error rates from speech recognition.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Manuel Sam Ribeiro",
  "date": "2021-02-27T21:34:48",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.883Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.883Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff2"
  },
  "title": "Evaluating Gammatone Frequency Cepstral Coefficients with Neural Networks for Emotion Recognition from Speech",
  "abstract": "Current approaches to speech emotion recognition focus on speech features that can capture the emotional content of a speech signal. Mel Frequency Cepstral Coefficients (MFCCs) are one of the most commonly used representations for audio speech recognition and classification. This paper proposes Gammatone Frequency Cepstral Coefficients (GFCCs) as a potentially better representation of speech signals for emotion recognition. The effectiveness of MFCC and GFCC representations are compared and evaluated over emotion and intensity classification tasks with fully connected and recurrent neural network architectures. The results provide evidence that GFCCs outperform MFCCs in speech emotion recognition.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Gabrielle K. Liu",
  "date": "2018-06-23T17:42:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.884Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.884Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff3"
  },
  "title": "Speech Recognition with Augmented Synthesized Speech",
  "abstract": "Recent success of the Tacotron speech synthesis architecture and its variants in producing natural sounding multi-speaker synthesized speech has raised the exciting possibility of replacing expensive, manually transcribed, domain-specific, human speech that is used to train speech recognizers. The multi-speaker speech synthesis architecture can learn latent embedding spaces of prosody, speaker and style variations derived from input acoustic representations thereby allowing for manipulation of the synthesized speech. In this paper, we evaluate the feasibility of enhancing speech recognition performance using speech synthesis using two corpora from different domains. We explore algorithms to provide the necessary acoustic and lexical diversity needed for robust speech recognition. Finally, we demonstrate the feasibility of this approach as a data augmentation strategy for domain-transfer.   We find that improvements to speech recognition performance is achievable by augmenting training data with synthesized material. However, there remains a substantial gap in performance between recognizers trained on human speech those trained on synthesized speech.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Andrew Rosenberg",
  "date": "2019-09-25T18:32:50",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.885Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.885Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff4"
  },
  "title": "Enhancing Indonesian Automatic Speech Recognition: Evaluating Multilingual Models with Diverse Speech Variabilities",
  "abstract": "An ideal speech recognition model has the capability to transcribe speech accurately under various characteristics of speech signals, such as speaking style (read and spontaneous), speech context (formal and informal), and background noise conditions (clean and moderate). Building such a model requires a significant amount of training data with diverse speech characteristics. Currently, Indonesian data is dominated by read, formal, and clean speech, leading to a scarcity of Indonesian data with other speech variabilities. To develop Indonesian automatic speech recognition (ASR), we present our research on state-of-the-art speech recognition models, namely Massively Multilingual Speech (MMS) and Whisper, as well as compiling a dataset comprising Indonesian speech with variabilities to facilitate our study. We further investigate the models' predictive ability to transcribe Indonesian speech data across different variability groups. The best results were achieved by the Whisper fine-tuned model across datasets with various characteristics, as indicated by the decrease in word error rate (WER) and character error rate (CER). Moreover, we found that speaking style variability affected model performance the most.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Aulia Adila",
  "date": "2024-10-11T14:07:07",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.886Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.886Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff5"
  },
  "title": "AV-CPL: Continuous Pseudo-Labeling for Audio-Visual Speech Recognition",
  "abstract": "Audio-visual speech contains synchronized audio and visual information that provides cross-modal supervision to learn representations for both automatic speech recognition (ASR) and visual speech recognition (VSR). We introduce continuous pseudo-labeling for audio-visual speech recognition (AV-CPL), a semi-supervised method to train an audio-visual speech recognition (AVSR) model on a combination of labeled and unlabeled videos with continuously regenerated pseudo-labels. Our models are trained for speech recognition from audio-visual inputs and can perform speech recognition using both audio and visual modalities, or only one modality. Our method uses the same audio-visual model for both supervised training and pseudo-label generation, mitigating the need for external speech recognition models to generate pseudo-labels. AV-CPL obtains significant improvements in VSR performance on the LRS3 dataset while maintaining practical ASR and AVSR performance. Finally, using visual-only speech data, our method is able to leverage unlabeled visual speech to improve VSR.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Andrew Rouditchenko",
  "date": "2023-09-29T16:57:21",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.887Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.887Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff6"
  },
  "title": "Algorithm of Segment-Syllabic Synthesis in Speech Recognition Problem",
  "abstract": "Speech recognition based on the syllable segment is discussed in this paper. The principal search methods in space of states for the speech recognition problem by segment-syllabic parameters trajectory synthesis are investigated. Recognition as comparison the parameters trajectories in chosen speech units on the sections of the segmented speech is realized. Some experimental results are given and discussed.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Oleg N. Karpov",
  "date": "2007-03-10T23:59:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.888Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.888Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff7"
  },
  "title": "Speech Recognition with no speech or with noisy speech",
  "abstract": "The performance of automatic speech recognition systems(ASR) degrades in the presence of noisy speech. This paper demonstrates that using electroencephalography (EEG) can help automatic speech recognition systems overcome performance loss in the presence of noise. The paper also shows that distillation training of automatic speech recognition systems using EEG features will increase their performance. Finally, we demonstrate the ability to recognize words from EEG with no speech signal on a limited English vocabulary with high accuracy.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Gautam Krishna",
  "date": "2019-03-02T17:53:49",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.889Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.889Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff8"
  },
  "title": "Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition",
  "abstract": "Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)",
  "tags": [
    "Computation and Language"
  ],
  "author": "Jeehyun Lee",
  "date": "2024-02-29T07:29:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.890Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.890Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ff9"
  },
  "title": "Deep Speech Based End-to-End Automated Speech Recognition (ASR) for Indian-English Accents",
  "abstract": "Automated Speech Recognition (ASR) is an interdisciplinary application of computer science and linguistics that enable us to derive the transcription from the uttered speech waveform. It finds several applications in Military like High-performance fighter aircraft, helicopters, air-traffic controller. Other than military speech recognition is used in healthcare, persons with disabilities and many more. ASR has been an active research area. Several models and algorithms for speech to text (STT) have been proposed. One of the most recent is Mozilla Deep Speech, it is based on the Deep Speech research paper by Baidu. Deep Speech is a state-of-art speech recognition system is developed using end-to-end deep learning, it is trained using well-optimized Recurrent Neural Network (RNN) training system utilizing multiple Graphical Processing Units (GPUs). This training is mostly done using American-English accent datasets, which results in poor generalizability to other English accents. India is a land of vast diversity. This can even be seen in the speech, there are several English accents which vary from state to state. In this work, we have used transfer learning approach using most recent Deep Speech model i.e., deepspeech-0.9.3 to develop an end-to-end speech recognition system for Indian-English accents. This work utilizes fine-tuning and data argumentation to further optimize and improve the Deep Speech ASR system. Indic TTS data of Indian-English accents is used for transfer learning and fine-tuning the pre-trained Deep Speech model. A general comparison is made among the untrained model, our trained model and other available speech recognition services for Indian-English Accents.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Priyank Dubey",
  "date": "2022-04-03T03:11:21",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.891Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.891Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ffa"
  },
  "title": "Speech-Driven Text Retrieval: Using Target IR Collections for Statistical Language Model Adaptation in Speech Recognition",
  "abstract": "Speech recognition has of late become a practical technology for real world applications. Aiming at speech-driven text retrieval, which facilitates retrieving information with spoken queries, we propose a method to integrate speech recognition and retrieval methods. Since users speak contents related to a target collection, we adapt statistical language models used for speech recognition based on the target collection, so as to improve both the recognition and retrieval accuracy. Experiments using existing test collections combined with dictated queries showed the effectiveness of our method.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Atsushi Fujii",
  "date": "2002-06-24T10:28:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.892Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.892Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ffb"
  },
  "title": "Speech Synthesis as Augmentation for Low-Resource ASR",
  "abstract": "Speech synthesis might hold the key to low-resource speech recognition. Data augmentation techniques have become an essential part of modern speech recognition training. Yet, they are simple, naive, and rarely reflect real-world conditions. Meanwhile, speech synthesis techniques have been rapidly getting closer to the goal of achieving human-like speech. In this paper, we investigate the possibility of using synthesized speech as a form of data augmentation to lower the resources necessary to build a speech recognizer. We experiment with three different kinds of synthesizers: statistical parametric, neural, and adversarial. Our findings are interesting and point to new research directions for the future.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Deblin Bagchi",
  "date": "2020-12-23T22:19:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.893Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.893Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ffc"
  },
  "title": "Modified Mel Filter Bank to Compute MFCC of Subsampled Speech",
  "abstract": "Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used speech features in most speech and speaker recognition applications. In this work, we propose a modified Mel filter bank to extract MFCCs from subsampled speech. We also propose a stronger metric which effectively captures the correlation between MFCCs of original speech and MFCC of resampled speech. It is found that the proposed method of filter bank construction performs distinguishably well and gives recognition performance on resampled speech close to recognition accuracies on original speech.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Kiran Kumar Bhuvanagiri",
  "date": "2014-10-25T10:00:14",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.894Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.894Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ffd"
  },
  "title": "Unified Speech-Text Pre-training for Speech Translation and Recognition",
  "abstract": "We describe a method to jointly pre-train speech and text in an encoder-decoder modeling framework for speech translation and recognition. The proposed method incorporates four self-supervised and supervised subtasks for cross modality learning. A self-supervised speech subtask leverages unlabelled speech data, and a (self-)supervised text to text subtask makes use of abundant text training data. Two auxiliary supervised speech tasks are included to unify speech and text modeling space. Our contribution lies in integrating linguistic information from the text corpus into the speech pre-training. Detailed analysis reveals learning interference among subtasks. Two pre-training configurations for speech translation and recognition, respectively, are presented to alleviate subtask interference. Our experiments show the proposed method can effectively fuse speech and text information into one model. It achieves between 1.7 and 2.3 BLEU improvement above the state of the art on the MuST-C speech translation dataset and comparable WERs to wav2vec 2.0 on the Librispeech speech recognition task.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Yun Tang",
  "date": "2022-04-11T20:59:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.895Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.895Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006ffe"
  },
  "title": "Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations",
  "abstract": "Children's speech recognition is a vital, yet largely overlooked domain when building inclusive speech technologies. The major challenge impeding progress in this domain is the lack of adequate child speech corpora; however, recent advances in self-supervised learning have created a new opportunity for overcoming this problem of data scarcity. In this paper, we leverage self-supervised adult speech representations and use three well-known child speech corpora to build models for children's speech recognition. We assess the performance of fine-tuning on both native and non-native children's speech, examine the effect of cross-domain child corpora, and investigate the minimum amount of child speech required to fine-tune a model which outperforms a state-of-the-art adult model. We also analyze speech recognition performance across children's ages. Our results demonstrate that fine-tuning with cross-domain child corpora leads to relative improvements of up to 46.08% and 45.53% for native and non-native child speech respectively, and absolute improvements of 14.70% and 31.10%. We also show that with as little as 5 hours of transcribed children's speech, it is possible to fine-tune a children's speech recognition system that outperforms a state-of-the-art adult model fine-tuned on 960 hours of adult speech.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Renee Lu",
  "date": "2022-11-14T22:03:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.896Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.896Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87006fff"
  },
  "title": "Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition",
  "abstract": "Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success. However, the distribution of such data is typically biased towards common accents or typical speech patterns. As a result, those systems often poorly perform on atypical accented speech. In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech. For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering. Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech. Fine-tuning ASR on the mined Indian accent speech using the proposed supervised or unsupervised clustering schemes showed 10.0% and 5.3% relative improvements compared to fine-tuning on the randomly sampled speech, respectively.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Jaeyoung Kim",
  "date": "2024-08-05T16:00:07",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.897Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.897Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007000"
  },
  "title": "Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech",
  "abstract": "Modern speech synthesis systems have improved significantly, with synthetic speech being indistinguishable from real speech. However, efficient and holistic evaluation of synthetic speech still remains a significant challenge. Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due to high costs. Therefore, researchers have developed auxiliary automatic metrics like Word Error Rate (WER) to measure intelligibility. Prior works focus on evaluating synthetic speech based on pre-trained speech recognition models, however, this can be limiting since this approach primarily measures speech intelligibility. In this paper, we propose an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech. Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility. Our proposed metric demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and YourTTS.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Dareen Alharthi",
  "date": "2023-10-01T15:52:48",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.898Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.898Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007001"
  },
  "title": "DeepTalk: Vocal Style Encoding for Speaker Recognition and Speech Synthesis",
  "abstract": "Automatic speaker recognition algorithms typically characterize speech audio using short-term spectral features that encode the physiological and anatomical aspects of speech production. Such algorithms do not fully capitalize on speaker-dependent characteristics present in behavioral speech features. In this work, we propose a prosody encoding network called DeepTalk for extracting vocal style features directly from raw audio data. The DeepTalk method outperforms several state-of-the-art speaker recognition systems across multiple challenging datasets. The speaker recognition performance is further improved by combining DeepTalk with a state-of-the-art physiological speech feature-based speaker recognition system. We also integrate DeepTalk into a current state-of-the-art speech synthesizer to generate synthetic speech. A detailed analysis of the synthetic speech shows that the DeepTalk captures F0 contours essential for vocal style modeling. Furthermore, DeepTalk-based synthetic speech is shown to be almost indistinguishable from real speech in the context of speaker recognition.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Anurag Chowdhury",
  "date": "2020-12-09T14:39:40",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.899Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.899Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007002"
  },
  "title": "Improving Accented Speech Recognition using Data Augmentation based on Unsupervised Text-to-Speech Synthesis",
  "abstract": "This paper investigates the use of unsupervised text-to-speech synthesis (TTS) as a data augmentation method to improve accented speech recognition. TTS systems are trained with a small amount of accented speech training data and their pseudo-labels rather than manual transcriptions, and hence unsupervised. This approach enables the use of accented speech data without manual transcriptions to perform data augmentation for accented speech recognition. Synthetic accented speech data, generated from text prompts by using the TTS systems, are then combined with available non-accented speech data to train automatic speech recognition (ASR) systems. ASR experiments are performed in a self-supervised learning framework using a Wav2vec2.0 model which was pre-trained on large amount of unsupervised accented speech data. The accented speech data for training the unsupervised TTS are read speech, selected from L2-ARCTIC and British Isles corpora, while spontaneous conversational speech from the Edinburgh international accents of English corpus are used as the evaluation data. Experimental results show that Wav2vec2.0 models which are fine-tuned to downstream ASR task with synthetic accented speech data, generated by the unsupervised TTS, yield up to 6.1% relative word error rate reductions compared to a Wav2vec2.0 baseline which is fine-tuned with the non-accented speech data from Librispeech corpus.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Cong-Thanh Do",
  "date": "2024-07-04T16:42:24",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.900Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.900Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007003"
  },
  "title": "A Study of Enhancement, Augmentation, and Autoencoder Methods for Domain Adaptation in Distant Speech Recognition",
  "abstract": "Speech recognizers trained on close-talking speech do not generalize to distant speech and the word error rate degradation can be as large as 40% absolute. Most studies focus on tackling distant speech recognition as a separate problem, leaving little effort to adapting close-talking speech recognizers to distant speech. In this work, we review several approaches from a domain adaptation perspective. These approaches, including speech enhancement, multi-condition training, data augmentation, and autoencoders, all involve a transformation of the data between domains. We conduct experiments on the AMI data set, where these approaches can be realized under the same controlled setting. These approaches lead to different amounts of improvement under their respective assumptions. The purpose of this paper is to quantify and characterize the performance gap between the two domains, setting up the basis for studying adaptation of speech recognizers from close-talking speech to distant speech. Our results also have implications for improving distant speech recognition.",
  "tags": [
    "Computation and Language",
    "Machine Learning"
  ],
  "author": "Hao Tang",
  "date": "2018-06-13T04:07:22",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.901Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.901Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007004"
  },
  "title": "Lecture Notes: Neural Network Architectures",
  "abstract": "These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.",
  "tags": [
    "Machine Learning",
    "Optimization and Control"
  ],
  "author": "Evelyn Herberg",
  "date": "2023-04-11T10:54:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.903Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.903Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007005"
  },
  "title": "Self-Organizing Multilayered Neural Networks of Optimal Complexity",
  "abstract": "The principles of self-organizing the neural networks of optimal complexity is considered under the unrepresentative learning set. The method of self-organizing the multi-layered neural networks is offered and used to train the logical neural networks which were applied to the medical diagnostics.",
  "tags": [
    "Neural and Evolutionary Computing",
    "Artificial Intelligence"
  ],
  "author": "V. Schetinin",
  "date": "2005-04-13T13:59:55",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.904Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.904Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007006"
  },
  "title": "Neural Network Processing Neural Networks: An efficient way to learn higher order functions",
  "abstract": "Functions are rich in meaning and can be interpreted in a variety of ways. Neural networks were proven to be capable of approximating a large class of functions[1]. In this paper, we propose a new class of neural networks called \"Neural Network Processing Neural Networks\" (NNPNNs), which inputs neural networks and numerical values, instead of just numerical values. Thus enabling neural networks to represent and process rich structures.",
  "tags": [
    "Machine Learning",
    "Neural and Evolutionary Computing"
  ],
  "author": "Firat Tuna",
  "date": "2019-11-06T19:15:34",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.905Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.905Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007007"
  },
  "title": "Guaranteed Quantization Error Computation for Neural Network Model Compression",
  "abstract": "Neural network model compression techniques can address the computation issue of deep neural networks on embedded devices in industrial systems. The guaranteed output error computation problem for neural network compression with quantization is addressed in this paper. A merged neural network is built from a feedforward neural network and its quantized version to produce the exact output difference between two neural networks. Then, optimization-based methods and reachability analysis methods are applied to the merged neural network to compute the guaranteed quantization error. Finally, a numerical example is proposed to validate the applicability and effectiveness of the proposed approach.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Neural and Evolutionary Computing"
  ],
  "author": "Wesley Cooke",
  "date": "2023-04-26T20:21:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.906Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.906Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007008"
  },
  "title": "Graph Structure of Neural Networks",
  "abstract": "Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a \"sweet spot\" of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural network's performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.",
  "tags": [
    "Machine Learning",
    "Computer Vision and Pattern Recognition",
    "Social and Information Networks",
    "Statistical Machine Learning"
  ],
  "author": "Jiaxuan You",
  "date": "2020-07-13T17:59:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.907Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.907Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007009"
  },
  "title": "Hybrid Quantum-Classical Neural Networks for Downlink Beamforming Optimization",
  "abstract": "This paper investigates quantum machine learning to optimize the beamforming in a multiuser multiple-input single-output downlink system. We aim to combine the power of quantum neural networks and the success of classical deep neural networks to enhance the learning performance. Specifically, we propose two hybrid quantum-classical neural networks to maximize the sum rate of a downlink system. The first one proposes a quantum neural network employing parameterized quantum circuits that follows a classical convolutional neural network. The classical neural network can be jointly trained with the quantum neural network or pre-trained leading to a fine-tuning transfer learning method. The second one designs a quantum convolutional neural network to better extract features followed by a classical deep neural network. Our results demonstrate the feasibility of the proposed hybrid neural networks, and reveal that the first method can achieve similar sum rate performance compared to a benchmark classical neural network with significantly less training parameters; while the second method can achieve higher sum rate especially in presence of many users still with less training parameters. The robustness of the proposed methods is verified using both software simulators and hardware emulators considering noisy intermediate-scale quantum devices.",
  "tags": [
    "Information Theory"
  ],
  "author": "Juping Zhang",
  "date": "2024-08-08T20:14:39",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.908Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.908Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700700a"
  },
  "title": "Cortex Neural Network: learning with Neural Network groups",
  "abstract": "Neural Network has been successfully applied to many real-world problems, such as image recognition and machine translation. However, for the current architecture of neural networks, it is hard to perform complex cognitive tasks, for example, to process the image and audio inputs together. Cortex, as an important architecture in the brain, is important for animals to perform the complex cognitive task. We view the architecture of Cortex in the brain as a missing part in the design of the current artificial neural network. In this paper, we purpose Cortex Neural Network (CrtxNN). The Cortex Neural Network is an upper architecture of neural networks which motivated from cerebral cortex in the brain to handle different tasks in the same learning system. It is able to identify different tasks and solve them with different methods. In our implementation, the Cortex Neural Network is able to process different cognitive tasks and perform reflection to get a higher accuracy. We provide a series of experiments to examine the capability of the cortex architecture on traditional neural networks. Our experiments proved its ability on the Cortex Neural Network can reach accuracy by 98.32% on MNIST and 62% on CIFAR10 at the same time, which can promisingly reduce the loss by 40%.",
  "tags": [
    "Neural and Evolutionary Computing",
    "Computer Vision and Pattern Recognition",
    "Machine Learning"
  ],
  "author": "Liyao Gao",
  "date": "2018-04-10T02:33:47",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.909Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.909Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700700b"
  },
  "title": "Parametrical Neural Networks and Some Other Similar Architectures",
  "abstract": "A review of works on associative neural networks accomplished during last four years in the Institute of Optical Neural Technologies RAS is given. The presentation is based on description of parametrical neural networks (PNN). For today PNN have record recognizing characteristics (storage capacity, noise immunity and speed of operation). Presentation of basic ideas and principles is accentuated.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Neural and Evolutionary Computing"
  ],
  "author": "Leonid B. Litinskii",
  "date": "2006-08-18T08:28:23",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.910Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.910Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700700c"
  },
  "title": "Assessing Intelligence in Artificial Neural Networks",
  "abstract": "The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network. Both batch normalization and dropout layers were found to increase neural efficiency. Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92.51% even when 75% of the class labels are randomized. These results demonstrate the utility of aIQ and neural efficiency as metrics for balancing network performance and size.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Nicholas J. Schaub",
  "date": "2020-06-03T16:45:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.911Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.911Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700700d"
  },
  "title": "Rational Neural Network Controllers",
  "abstract": "Neural networks have shown great success in many machine learning related tasks, due to their ability to act as general function approximators. Recent work has demonstrated the effectiveness of neural networks in control systems (known as neural feedback loops), most notably by using a neural network as a controller. However, one of the big challenges of this approach is that neural networks have been shown to be sensitive to adversarial attacks. This means that, unless they are designed properly, they are not an ideal candidate for controllers due to issues with robustness and uncertainty, which are pivotal aspects of control systems. There has been initial work on robustness to both analyse and design dynamical systems with neural network controllers. However, one prominent issue with these methods is that they use existing neural network architectures tailored for traditional machine learning tasks. These structures may not be appropriate for neural network controllers and it is important to consider alternative architectures. This paper considers rational neural networks and presents novel rational activation functions, which can be used effectively in robustness problems for neural feedback loops. Rational activation functions are replaced by a general rational neural network structure, which is convex in the neural network's parameters. A method is proposed to recover a stabilising controller from a Sum of Squares feasibility test. This approach is then applied to a refined rational neural network which is more compatible with Sum of Squares programming. Numerical examples show that this method can successfully recover stabilising rational neural network controllers for neural feedback loops with non-linear plants with noise and parametric uncertainty.",
  "tags": [
    "Systems and Control (Electrical Engineering)",
    "Machine Learning",
    "Systems and Control"
  ],
  "author": "Matthew Newton",
  "date": "2023-07-12T16:35:41",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.912Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.912Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700700e"
  },
  "title": "Asymptotic Theory of Expectile Neural Networks",
  "abstract": "Neural networks are becoming an increasingly important tool in applications. However, neural networks are not widely used in statistical genetics. In this paper, we propose a new neural networks method called expectile neural networks. When the size of parameter is too large, the standard maximum likelihood procedures may not work. We use sieve method to constrain parameter space. And we prove its consistency and normality under nonparametric regression framework.",
  "tags": [
    "Statistics Theory"
  ],
  "author": "Jinghang Lin",
  "date": "2020-10-31T01:14:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.913Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.913Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700700f"
  },
  "title": "Combining Recurrent and Convolutional Neural Networks for Relation Classification",
  "abstract": "This paper investigates two different neural architectures for the task of relation classification: convolutional neural networks and recurrent neural networks. For both models, we demonstrate the effect of different architectural choices. We present a new context representation for convolutional neural networks for relation classification (extended middle context). Furthermore, we propose connectionist bi-directional recurrent neural networks and introduce ranking loss for their optimization. Finally, we show that combining convolutional and recurrent neural networks using a simple voting scheme is accurate enough to improve results. Our neural models achieve state-of-the-art results on the SemEval 2010 relation classification task.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Ngoc Thang Vu",
  "date": "2016-05-24T08:20:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.914Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.914Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007010"
  },
  "title": "A Comprehensive Review of Spiking Neural Networks: Interpretation, Optimization, Efficiency, and Best Practices",
  "abstract": "Biological neural networks continue to inspire breakthroughs in neural network performance. And yet, one key area of neural computation that has been under-appreciated and under-investigated is biologically plausible, energy-efficient spiking neural networks, whose potential is especially attractive for low-power, mobile, or otherwise hardware-constrained settings. We present a literature review of recent developments in the interpretation, optimization, efficiency, and accuracy of spiking neural networks. Key contributions include identification, discussion, and comparison of cutting-edge methods in spiking neural network optimization, energy-efficiency, and evaluation, starting from first principles so as to be accessible to new practitioners.",
  "tags": [
    "Neural and Evolutionary Computing",
    "Machine Learning"
  ],
  "author": "Kai Malcolm",
  "date": "2023-03-19T22:07:27",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.915Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.915Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007011"
  },
  "title": "Design and development of opto-neural processors for simulation of neural networks trained in image detection for potential implementation in hybrid robotics",
  "abstract": "Neural networks have been employed for a wide range of processing applications like image processing, motor control, object detection and many others. Living neural networks offer advantages of lower power consumption, faster processing, and biological realism. Optogenetics offers high spatial and temporal control over biological neurons and presents potential in training live neural networks. This work proposes a simulated living neural network trained indirectly by backpropagating STDP based algorithms using precision activation by optogenetics achieving accuracy comparable to traditional neural network training algorithms.",
  "tags": [
    "Artificial Intelligence",
    "Machine Learning",
    "Neural and Evolutionary Computing"
  ],
  "author": "Sanjana Shetty",
  "date": "2024-01-17T04:42:49",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.916Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.916Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007012"
  },
  "title": "Convex Formulation of Overparameterized Deep Neural Networks",
  "abstract": "Analysis of over-parameterized neural networks has drawn significant attention in recentyears. It was shown that such systems behave like convex systems under various restrictedsettings, such as for two-level neural networks, and when learning is only restricted locally inthe so-called neural tangent kernel space around specialized initializations. However, there areno theoretical techniques that can analyze fully trained deep neural networks encountered inpractice. This paper solves this fundamental problem by investigating such overparameterizeddeep neural networks when fully trained. We generalize a new technique called neural feature repopulation, originally introduced in (Fang et al., 2019a) for two-level neural networks, to analyze deep neural networks. It is shown that under suitable representations, overparameterized deep neural networks are inherently convex, and when optimized, the system can learn effective features suitable for the underlying learning task under mild conditions. This new analysis is consistent with empirical observations that deep neural networks are capable of learning efficient feature representations. Therefore, the highly unexpected result of this paper can satisfactorily explain the practical success of deep neural networks. Empirical studies confirm that predictions of our theory are consistent with results observed in practice.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Cong Fang",
  "date": "2019-11-18T13:42:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.917Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.917Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007013"
  },
  "title": "Approximate Bisimulation Relations for Neural Networks and Application to Assured Neural Network Compression",
  "abstract": "In this paper, we propose a concept of approximate bisimulation relation for feedforward neural networks. In the framework of approximate bisimulation relation, a novel neural network merging method is developed to compute the approximate bisimulation error between two neural networks based on reachability analysis of neural networks. The developed method is able to quantitatively measure the distance between the outputs of two neural networks with the same inputs. Then, we apply the approximate bisimulation relation results to perform neural networks model reduction and compute the compression precision, i.e., assured neural networks compression. At last, using the assured neural network compression, we accelerate the verification processes of ACAS Xu neural networks to illustrate the effectiveness and advantages of our proposed approximate bisimulation approach.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Systems and Control",
    "Systems and Control (Electrical Engineering)"
  ],
  "author": "Weiming Xiang",
  "date": "2022-02-02T16:21:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.919Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.919Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007014"
  },
  "title": "Optimal rates of approximation by shallow ReLU$^k$ neural networks and applications to nonparametric regression",
  "abstract": "We study the approximation capacity of some variation spaces corresponding to shallow ReLU$^k$ neural networks. It is shown that sufficiently smooth functions are contained in these spaces with finite variation norms. For functions with less smoothness, the approximation rates in terms of the variation norm are established. Using these results, we are able to prove the optimal approximation rates in terms of the number of neurons for shallow ReLU$^k$ neural networks. It is also shown how these results can be used to derive approximation bounds for deep neural networks and convolutional neural networks (CNNs). As applications, we study convergence rates for nonparametric regression using three ReLU neural network models: shallow neural network, over-parameterized neural network, and CNN. In particular, we show that shallow neural networks can achieve the minimax optimal rates for learning H\\\"older functions, which complements recent results for deep neural networks. It is also proven that over-parameterized (deep or shallow) neural networks can achieve nearly optimal rates for nonparametric regression.",
  "tags": [
    "Statistical Machine Learning",
    "Machine Learning"
  ],
  "author": "Yunfei Yang",
  "date": "2023-04-04T06:35:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.920Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.920Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007015"
  },
  "title": "Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks",
  "abstract": "Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, a vector-valued neural network can be obtained by placing restrictions on a real-valued model to consider the intercorrelation between feature channels. Finally, we show how V-nets, including hypercomplex-valued neural networks, can be implemented in current deep-learning libraries as real-valued networks.",
  "tags": [
    "Machine Learning",
    "Neural and Evolutionary Computing"
  ],
  "author": "Marcos Eduardo Valle",
  "date": "2023-09-14T13:48:16",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.921Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.921Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007016"
  },
  "title": "One weird trick for parallelizing convolutional neural networks",
  "abstract": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.",
  "tags": [
    "Neural and Evolutionary Computing",
    "Distributed Computing",
    "Machine Learning"
  ],
  "author": "Alex Krizhevsky",
  "date": "2014-04-23T22:37:56",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.922Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.922Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007017"
  },
  "title": "Nonlinear Systems Identification Using Deep Dynamic Neural Networks",
  "abstract": "Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data",
  "tags": [
    "Neural and Evolutionary Computing"
  ],
  "author": "Olalekan Ogunmolu",
  "date": "2016-10-05T14:26:27",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.923Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.923Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007018"
  },
  "title": "Neural Networks Architecture Evaluation in a Quantum Computer",
  "abstract": "In this work, we propose a quantum algorithm to evaluate neural networks architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The proposed algorithm is based on a quantum associative memory and the learning algorithm for artificial neural networks. Unlike conventional algorithms for evaluating neural network architectures, QNNAE does not depend on initialization of weights. The proposed algorithm has a binary output and results in 0 with probability proportional to the performance of the network. And its computational cost is equal to the computational cost to train a neural network.",
  "tags": [
    "Neural and Evolutionary Computing"
  ],
  "author": "Adenilton José da Silva",
  "date": "2017-11-13T18:50:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.924Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.924Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007019"
  },
  "title": "Geometric Decomposition of Feed Forward Neural Networks",
  "abstract": "There have been several attempts to mathematically understand neural networks and many more from biological and computational perspectives. The field has exploded in the last decade, yet neural networks are still treated much like a black box. In this work we describe a structure that is inherent to a feed forward neural network. This will provide a framework for future work on neural networks to improve training algorithms, compute the homology of the network, and other applications. Our approach takes a more geometric point of view and is unlike other attempts to mathematically understand neural networks that rely on a functional perspective.",
  "tags": [
    "Neural and Evolutionary Computing"
  ],
  "author": "Sven Cattell",
  "date": "2016-12-08T03:28:10",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.925Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.925Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700701a"
  },
  "title": "Building Compact and Robust Deep Neural Networks with Toeplitz Matrices",
  "abstract": "Deep neural networks are state-of-the-art in a wide variety of tasks, however, they exhibit important limitations which hinder their use and deployment in real-world applications. When developing and training neural networks, the accuracy should not be the only concern, neural networks must also be cost-effective and reliable. Although accurate, large neural networks often lack these properties. This thesis focuses on the problem of training neural networks which are not only accurate but also compact, easy to train, reliable and robust to adversarial examples. To tackle these problems, we leverage the properties of structured matrices from the Toeplitz family to build compact and secure neural networks.",
  "tags": [
    "Machine Learning"
  ],
  "author": "Alexandre Araujo",
  "date": "2021-09-02T13:58:12",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.926Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.926Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700701b"
  },
  "title": "Application of Neural Network in Optimization of Chemical Process",
  "abstract": "Artificial neural network (ANN) has been widely used due to its strong nonlinear mapping ability, fault tolerance and self-learning ability. This article summarizes the development history of artificial neural networks, introduces three common neural network types, BP neural network, RBF neural network and convolutional neural network, and focuses on the practical application in chemical process optimization, especially the results achieved in multi-objective control optimization and process parameter improvement.",
  "tags": [
    "Systems and Control (Electrical Engineering)",
    "Systems and Control"
  ],
  "author": "Fei Liang",
  "date": "2021-10-11T00:31:00",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.927Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.927Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700701c"
  },
  "title": "Compact Matrix Quantum Group Equivariant Neural Networks",
  "abstract": "We derive the existence of a new type of neural network, called a compact matrix quantum group equivariant neural network, that learns from data that has an underlying quantum symmetry. We apply the Woronowicz formulation of Tannaka-Krein duality to characterise the weight matrices that appear in these neural networks for any easy compact matrix quantum group. We show that compact matrix quantum group equivariant neural networks contain, as a subclass, all compact matrix group equivariant neural networks. Moreover, we obtain characterisations of the weight matrices for many compact matrix group equivariant neural networks that have not previously appeared in the machine learning literature.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Edward Pearce-Crump",
  "date": "2023-11-10T19:11:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.928Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.928Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700701d"
  },
  "title": "Lost in Translation: Large Language Models in Non-English Content Analysis",
  "abstract": "In recent years, large language models (e.g., Open AI's GPT-4, Meta's LLaMa, Google's PaLM) have become the dominant approach for building AI systems to analyze and generate language online. However, the automated systems that increasingly mediate our interactions online -- such as chatbots, content moderation systems, and search engines -- are primarily designed for and work far more effectively in English than in the world's other 7,000 languages. Recently, researchers and technology companies have attempted to extend the capabilities of large language models into languages other than English by building what are called multilingual language models.   In this paper, we explain how these multilingual language models work and explore their capabilities and limits. Part I provides a simple technical explanation of how large language models work, why there is a gap in available data between English and other languages, and how multilingual language models attempt to bridge that gap. Part II accounts for the challenges of doing content analysis with large language models in general and multilingual language models in particular. Part III offers recommendations for companies, researchers, and policymakers to keep in mind when considering researching, developing and deploying large and multilingual language models.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Gabriel Nicholas",
  "date": "2023-06-12T19:10:47",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.929Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.929Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700701e"
  },
  "title": "Cedille: A large autoregressive French language model",
  "abstract": "Scaling up the size and training of autoregressive language models has enabled novel ways of solving Natural Language Processing tasks using zero-shot and few-shot learning. While extreme-scale language models such as GPT-3 offer multilingual capabilities, zero-shot learning for languages other than English remain largely unexplored. Here, we introduce Cedille, a large open source auto-regressive language model, specifically trained for the French language. Our results show that Cedille outperforms existing French language models and is competitive with GPT-3 on a range of French zero-shot benchmarks. Furthermore, we provide an in-depth comparison of the toxicity exhibited by these models, showing that Cedille marks an improvement in language model safety thanks to dataset filtering.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Martin Müller",
  "date": "2022-02-07T17:40:43",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.930Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.930Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700701f"
  },
  "title": "How Good are Commercial Large Language Models on African Languages?",
  "abstract": "Recent advancements in Natural Language Processing (NLP) has led to the proliferation of large pretrained language models. These models have been shown to yield good performance, using in-context learning, even on unseen tasks and languages. They have also been exposed as commercial APIs as a form of language-model-as-a-service, with great adoption. However, their performance on African languages is largely unknown. We present a preliminary analysis of commercial large language models on two tasks (machine translation and text classification) across eight African languages, spanning different language families and geographical areas. Our results suggest that commercial language models produce below-par performance on African languages. We also find that they perform better on text classification than machine translation. In general, our findings present a call-to-action to ensure African languages are well represented in commercial large language models, given their growing popularity.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Jessica Ojo",
  "date": "2023-05-11T02:29:53",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.931Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.931Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007020"
  },
  "title": "Goldfish: Monolingual Language Models for 350 Languages",
  "abstract": "For many low-resource languages, the only available language models are large multilingual models trained on many languages simultaneously. However, using FLORES perplexity as a metric, we find that these models perform worse than bigrams for many languages (e.g. 24% of languages in XGLM 4.5B; 43% in BLOOM 7.1B). To facilitate research that focuses on low-resource languages, we pre-train and release Goldfish, a suite of monolingual autoregressive Transformer language models up to 125M parameters for 350 languages. The Goldfish reach lower FLORES perplexities than BLOOM, XGLM, and MaLA-500 on 98 of 204 FLORES languages, despite each Goldfish model being over 10x smaller. However, the Goldfish significantly underperform larger multilingual models on reasoning benchmarks, suggesting that for low-resource languages, multilinguality primarily improves general reasoning abilities rather than basic text generation. We release models trained on 5MB (350 languages), 10MB (288 languages), 100MB (166 languages), and 1GB (83 languages) of text data where available. The Goldfish models are available as baselines, fine-tuning sources, or augmentations to existing models in low-resource NLP research, and they are further useful for crosslinguistic studies requiring maximally comparable models across languages.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Tyler A. Chang",
  "date": "2024-08-19T22:31:21",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.932Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.932Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007021"
  },
  "title": "Modelling Language",
  "abstract": "This paper argues that large language models have a valuable scientific role to play in serving as scientific models of a language. Linguistic study should not only be concerned with the cognitive processes behind linguistic competence, but also with language understood as an external, social entity. Once this is recognized, the value of large language models as scientific models becomes clear. This paper defends this position against a number of arguments to the effect that language models provide no linguistic insight. It also draws upon recent work in philosophy of science to show how large language models could serve as scientific models.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Jumbly Grindrod",
  "date": "2024-04-15T08:40:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.933Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.933Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007022"
  },
  "title": "LLaMA-Reg: Using LLaMA 2 for Unsupervised Medical Image Registration",
  "abstract": "Medical image registration is an essential topic in medical image analysis. In this paper, we propose a method for medical image registration using a pretrained large language model. We find that using the pretrained large language model to encode deep features of the medical images in the registration model can effectively improve image registration accuracy, indicating the great potential of the large language model in medical image registration tasks. We use dual encoders to perform deep feature extraction on image pairs and then input the features into the pretrained large language model. To adapt the large language model to our registration task, the weights of the large language model are frozen in the registration model, and an adapter is utilized to fine-tune the large language model, which aims at (a) mapping the visual tokens to the language space before the large language model computing, (b) project the modeled language tokens output from the large language model to the visual space. Our method combines output features from the fine-tuned large language model with the features output from each encoder layer to gradually generate the deformation fields required for registration in the decoder. To demonstrate the effectiveness of the large prediction model in registration tasks, we conducted experiments on knee and brain MRI and achieved state-of-the-art results.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Mingrui Ma",
  "date": "2024-05-29T05:26:25",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.934Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.934Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007023"
  },
  "title": "A Survey of Large Language Models for European Languages",
  "abstract": "Large Language Models (LLMs) have gained significant attention due to their high performance on a wide range of natural language tasks since the release of ChatGPT. The LLMs learn to understand and generate language by training billions of model parameters on vast volumes of text data. Despite being a relatively new field, LLM research is rapidly advancing in various directions. In this paper, we present an overview of LLM families, including LLaMA, PaLM, GPT, and MoE, and the methods developed to create and enhance LLMs for official European Union (EU) languages. We provide a comprehensive summary of common monolingual and multilingual datasets used for pretraining large language models.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Wazir Ali",
  "date": "2024-08-27T13:10:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.935Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.935Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007024"
  },
  "title": "Beyond the limitations of any imaginable mechanism: large language models and psycholinguistics",
  "abstract": "Large language models are not detailed models of human linguistic processing. They are, however, extremely successful at their primary task: providing a model for language. For this reason and because there are no animal models for language, large language models are important in psycholinguistics: they are useful as a practical tool, as an illustrative comparative, and philosophically, as a basis for recasting the relationship between language and thought.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Conor Houghton",
  "date": "2023-02-28T20:49:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.936Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.936Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007025"
  },
  "title": "Enhance Reasoning Ability of Visual-Language Models via Large Language Models",
  "abstract": "Pre-trained visual language models (VLM) have shown excellent performance in image caption tasks. However, it sometimes shows insufficient reasoning ability. In contrast, large language models (LLMs) emerge with powerful reasoning capabilities. Therefore, we propose a method called TReE, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios. TReE contains three stages: observation, thinking, and re-thinking. Observation stage indicates that VLM obtains the overall information of the relative image. Thinking stage combines the image information and task description as the prompt of the LLM, inference with the rationals. Re-Thinking stage learns from rationale and then inference the final result through VLM.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Yueting Yang",
  "date": "2023-05-22T17:33:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.937Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.937Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007026"
  },
  "title": "Should we Stop Training More Monolingual Models, and Simply Use Machine Translation Instead?",
  "abstract": "Most work in NLP makes the assumption that it is desirable to develop solutions in the native language in question. There is consequently a strong trend towards building native language models even for low-resource languages. This paper questions this development, and explores the idea of simply translating the data into English, thereby enabling the use of pretrained, and large-scale, English language models. We demonstrate empirically that a large English language model coupled with modern machine translation outperforms native language models in most Scandinavian languages. The exception to this is Finnish, which we assume is due to inferior translation quality. Our results suggest that machine translation is a mature technology, which raises a serious counter-argument for training native language models for low-resource languages. This paper therefore strives to make a provocative but important point. As English language models are improving at an unprecedented pace, which in turn improves machine translation, it is from an empirical and environmental stand-point more effective to translate data from low-resource languages into English, than to build language models for such languages.",
  "tags": [
    "Computation and Language",
    "Machine Learning"
  ],
  "author": "Tim Isbister",
  "date": "2021-04-21T10:21:24",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.938Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.938Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007027"
  },
  "title": "Images in Language Space: Exploring the Suitability of Large Language Models for Vision & Language Tasks",
  "abstract": "Large language models have demonstrated robust performance on various language tasks using zero-shot or few-shot learning paradigms. While being actively researched, multimodal models that can additionally handle images as input have yet to catch up in size and generality with language-only models. In this work, we ask whether language-only models can be utilised for tasks that require visual input -- but also, as we argue, often require a strong reasoning component. Similar to some recent related work, we make visual information accessible to the language model using separate verbalisation models. Specifically, we investigate the performance of open-source, open-access language models against GPT-3 on five vision-language tasks when given textually-encoded visual information. Our results suggest that language models are effective for solving vision-language tasks even with limited samples. This approach also enhances the interpretability of a model's output by providing a means of tracing the output back through the verbalised image content.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Sherzod Hakimov",
  "date": "2023-05-23T07:50:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.939Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.939Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007028"
  },
  "title": "The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models",
  "abstract": "The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Jonathan Katzy",
  "date": "2025-01-16T16:48:41",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.940Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.940Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007029"
  },
  "title": "Formal Aspects of Language Modeling",
  "abstract": "Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. Consequently, it is important for both developers and researchers alike to understand the mathematical foundations of large language models, as well as how to implement them. These notes are the accompaniment to the theoretical portion of the ETH Z\\\"urich course on large language models, covering what constitutes a language model from a formal, theoretical perspective.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Ryan Cotterell",
  "date": "2023-11-07T20:21:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.941Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.941Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700702a"
  },
  "title": "When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models",
  "abstract": "Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. Transliterating those languages improves very significantly the ability of large-scale multilingual language models on downstream tasks.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Benjamin Muller",
  "date": "2020-10-24T10:15:03",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.942Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.942Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700702b"
  },
  "title": "Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind",
  "abstract": "Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality. Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression. MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets. Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages. We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression. In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Hongchuan Zeng",
  "date": "2024-04-06T22:16:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.943Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.943Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700702c"
  },
  "title": "Evaluation Methodology for Large Language Models for Multilingual Document Question and Answer",
  "abstract": "With the widespread adoption of Large Language Models (LLMs), in this paper we investigate the multilingual capability of these models. Our preliminary results show that, translating the native language context, question and answer into a high resource language produced the best results.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Adar Kahana",
  "date": "2024-02-01T23:46:05",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.944Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.944Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700702d"
  },
  "title": "Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback",
  "abstract": "Current representations used in reasoning steps of large language models can mostly be categorized into two main types: (1) natural language, which is difficult to verify; and (2) non-natural language, usually programming code, which is difficult for people who are unfamiliar with coding to read. In this paper, we propose to use a semi-structured form to represent reasoning steps of large language models. Specifically, we use relation tuples, which are not only human-readable but also machine-friendly and easier to verify than natural language. We implement a framework that includes three main components: (1) introducing relation tuples into the reasoning steps of large language models; (2) implementing an automatic verification process of reasoning steps with a local code interpreter based on relation tuples; and (3) integrating a simple and effective dynamic feedback mechanism, which we found helpful for self-improvement of large language models. The experimental results on various arithmetic datasets demonstrate the effectiveness of our method in improving the arithmetic reasoning ability of large language models. The source code is available at https://github.com/gpgg/art.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence"
  ],
  "author": "Zhongtao Miao",
  "date": "2024-06-25T18:21:00",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.945Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.945Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700702e"
  },
  "title": "Large Language Models are not Models of Natural Language: they are Corpus Models",
  "abstract": "Natural Language Processing (NLP) has become one of the leading application areas in the current Artificial Intelligence boom. Transfer learning has enabled large deep learning neural networks trained on the language modeling task to vastly improve performance in almost all downstream language tasks. Interestingly, when the language models are trained with data that includes software code, they demonstrate remarkable abilities in generating functioning computer code from natural language specifications. We argue that this creates a conundrum for the claim that eliminative neural models are a radical restructuring in our understanding of cognition in that they eliminate the need for symbolic abstractions like generative phrase structure grammars. Because the syntax of programming languages is by design determined by phrase structure grammars, neural models that produce syntactic code are apparently uninformative about the theoretical foundations of programming languages. The demonstration that neural models perform well on tasks that involve clearly symbolic systems, proves that they cannot be used as an argument that language and other cognitive systems are not symbolic. Finally, we argue as a corollary that the term language model is misleading and propose the adoption of the working term corpus model instead, which better reflects the genesis and contents of the model.",
  "tags": [
    "Computation and Language",
    "Machine Learning"
  ],
  "author": "Csaba Veres",
  "date": "2021-12-13T22:39:46",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.946Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.946Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700702f"
  },
  "title": "Dynamic Fusion: Attentional Language Model for Neural Machine Translation",
  "abstract": "Neural Machine Translation (NMT) can be used to generate fluent output. As such, language models have been investigated for incorporation with NMT. In prior investigations, two models have been used: a translation model and a language model. The translation model's predictions are weighted by the language model with a hand-crafted ratio in advance. However, these approaches fail to adopt the language model weighting with regard to the translation history. In another line of approach, language model prediction is incorporated into the translation model by jointly considering source and target information. However, this line of approach is limited because it largely ignores the adequacy of the translation output.   Accordingly, this work employs two mechanisms, the translation model and the language model, with an attentive architecture to the language model as an auxiliary element of the translation model. Compared with previous work in English--Japanese machine translation using a language model, the experimental results obtained with the proposed Dynamic Fusion mechanism improve BLEU and Rank-based Intuitive Bilingual Evaluation Scores (RIBES) scores. Additionally, in the analyses of the attention and predictivity of the language model, the Dynamic Fusion mechanism allows predictive language modeling that conforms to the appropriate grammatical structure.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Michiki Kurosawa",
  "date": "2019-09-11T07:14:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.948Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.948Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007030"
  },
  "title": "BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages",
  "abstract": "Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with ChatGPT and Google Translate in many languages and even outperforms ChatGPT in 8 language pairs. We release the BigTranslate model and hope it can advance the research progress.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Wen Yang",
  "date": "2023-05-29T14:07:52",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.949Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.949Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007031"
  },
  "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
  "abstract": "Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. In this paper, we set out to investigate calibrating the pruning of multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. Our results offer practical suggestions, for example, calibrating in the target language can efficiently retain the language modeling capability but does not necessarily benefit downstream tasks. Through further analysis of latent subspaces, pruning masks, and individual neurons within pruned models, we find that while pruning generally preserves strong language-specific features, it may fail to retain language-specific neuron activation patterns and subtle, language-agnostic features associated with knowledge and reasoning that are needed for complex tasks.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Simon Kurz",
  "date": "2024-08-26T16:29:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.950Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.950Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007032"
  },
  "title": "Can Large Language Models design a Robot?",
  "abstract": "Large Language Models can lead researchers in the design of robots.",
  "tags": [
    "Robotics"
  ],
  "author": "Francesco Stella",
  "date": "2023-03-15T09:41:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.951Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.951Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007033"
  },
  "title": "Still \"Talking About Large Language Models\": Some Clarifications",
  "abstract": "My paper \"Talking About Large Language Models\" has more than once been interpreted as advocating a reductionist stance towards large language models. But the paper was not intended that way, and I do not endorse such positions. This short note situates the paper in the context of a larger philosophical project that is concerned with the (mis)use of words rather than metaphysics, in the spirit of Wittgenstein's later writing.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Murray Shanahan",
  "date": "2024-12-13T17:21:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.952Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.952Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007034"
  },
  "title": "Scientific Computing with Large Language Models",
  "abstract": "We provide an overview of the emergence of large language models for scientific computing applications. We highlight use cases that involve natural language processing of scientific documents and specialized languages designed to describe physical systems. For the former, chatbot style applications appear in medicine, mathematics and physics and can be used iteratively with domain experts for problem solving. We also review specialized languages within molecular biology, the languages of molecules, proteins, and DNA where language models are being used to predict properties and even create novel physical systems at much faster rates than traditional computing methods.",
  "tags": [
    "Computation and Language",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Christopher Culver",
  "date": "2024-06-11T13:39:07",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.953Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.953Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007035"
  },
  "title": "Cybersecurity Dynamics",
  "abstract": "We explore the emerging field of {\\em Cybersecurity Dynamics}, a candidate foundation for the Science of Cybersecurity.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Shouhuai Xu",
  "date": "2015-02-18T01:59:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.955Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.955Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007036"
  },
  "title": "Novel Approach for Cybersecurity Workforce Development: A Course in Secure Design",
  "abstract": "Training the future cybersecurity workforce to respond to emerging threats requires introduction of novel educational interventions into the cybersecurity curriculum. To be effective, these interventions have to incorporate trending knowledge from cybersecurity and other related domains while allowing for experiential learning through hands-on experimentation. To date, the traditional interdisciplinary approach for cybersecurity training has infused political science, law, economics or linguistics knowledge into the cybersecurity curriculum, allowing for limited experimentation. Cybersecurity students were left with little opportunity to acquire knowledge, skills, and abilities in domains outside of these. Also, students in outside majors had no options to get into cybersecurity. With this in mind, we developed an interdisciplinary course for experiential learning in the fields of cybersecurity and interaction design. The inaugural course teaches students from cybersecurity, user interaction design, and visual design the principles of designing for secure use - or secure design - and allows them to apply them for prototyping of Internet-of-Things (IoT) products for smart homes. This paper elaborates on the concepts of secure design and how our approach enhances the training of the future cybersecurity workforce.",
  "tags": [
    "Computers and Society",
    "Cryptography and Security"
  ],
  "author": "Filipo Sharevski",
  "date": "2018-06-04T16:39:46",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.956Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.956Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007037"
  },
  "title": "Building a Resilient Cybersecurity Posture: A Framework for Leveraging Prevent, Detect and Respond Functions and Law Enforcement Collaboration",
  "abstract": "This research paper proposes a framework for building a resilient cybersecurity posture that leverages prevent, detect, and respond functions and law enforcement collaboration. The Cybersecurity Resilience and Law Enforcement Collaboration (CyRLEC) Framework is designed to provide a comprehensive and integrated approach to cybersecurity that emphasizes collaboration with law enforcement agencies to mitigate cyber threats. The paper compares and contrasts the CyRLEC Framework with the NIST Cybersecurity Framework and highlights the critical differences between the two frameworks. While the NIST framework focuses on managing cybersecurity risk, the CyRLEC Framework takes a broader view of cybersecurity, including proactive prevention, early detection, rapid response to cyber-attacks, and close collaboration with law enforcement agencies to investigate and prosecute cybercriminals. The paper also provides a case study of a simulated real-world implementation of the CyRLEC Framework and evaluates its effectiveness in improving an organization's cybersecurity posture. The research findings demonstrate the value of the CyRLEC Framework in enhancing cybersecurity resilience and promoting effective collaboration with law enforcement agencies. Overall, this research paper contributes to the growing knowledge of cybersecurity frameworks and provides practical insights for organizations seeking to improve their cybersecurity posture.",
  "tags": [
    "Computers and Society",
    "Cryptography and Security"
  ],
  "author": "Francesco Schiliro",
  "date": "2023-03-20T05:16:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.957Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.957Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007038"
  },
  "title": "Cybersecurity Dynamics: A Foundation for the Science of Cybersecurity",
  "abstract": "Cybersecurity Dynamics is new concept that aims to achieve the modeling, analysis, quantification, and management of cybersecurity from a holistic perspective, rather than from a building-blocks perspective. It is centered at modeling and analyzing the attack-defense interactions in cyberspace, which cause a ``natural'' phenomenon -- the evolution of the global cybersecurity state. In this Chapter, we systematically introduce and review the Cybersecurity Dynamics foundation for the Science of Cybersecurity. We review the core concepts, technical approaches, research axes, and results that have been obtained in this endeavor. We outline a research roadmap towards the ultimate research goal, including a systematic set of technical barriers.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Shouhuai Xu",
  "date": "2020-10-09T04:09:43",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.958Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.958Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007039"
  },
  "title": "Adopting the Cybersecurity Concepts into Curriculum The Potential Effects on Students Cybersecurity Knowledge",
  "abstract": "This study examines the effect of adopting cybersecurity concepts on the IT curriculum and determines the potential effect on students' knowledge of cybersecurity practices and level of awareness. To this end, a pilot study was first conducted to measure the current level of cybersecurity awareness. The results revealed that students do not have much knowledge of Cybersecurity. Thus, a four-step approach was proposed to infuse the relevant cybersecurity topics in five matched courses based on the latest Cybersecurity curricular guidelines (CSEC2017). A sample of 42 students was selected purposively without prior knowledge of Cybersecurity and divided identically into experimental and control groups. Students in the experimental group were asked to take five consecutive courses over five semesters. In each course, groups went through a pre-test for the infused topics. Then, the experimental group taught the corresponding infused topics. A post-test was administered to both groups at the end of each course, and the t-test was conducted. The results found significant differences between marks of prior and post-tests for 11 out of 14 infused topics. These satisfactory results would encourage universities to infuse cybersecurity concepts into their curriculum",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Mohammad Azzeh",
  "date": "2022-09-12T16:00:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.959Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.959Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700703a"
  },
  "title": "Practical Cybersecurity Ethics: Mapping CyBOK to Ethical Concerns",
  "abstract": "Research into the ethics of cybersecurity is an established and growing topic of investigation, however the translation of this research into practice is lacking: there exists a small number of professional codes of ethics or codes of practice in cybersecurity, however these are very broad and do not offer much insight into the ethical dilemmas that can be faced while performing specific cybersecurity activities. In order to address this gap, we leverage ongoing work on the Cyber Security Body of Knowledge (CyBOK) to help elicit and document the responsibilities and ethics of the profession. Based on a literature review of the ethics of cybersecurity, we use CyBOK to frame the exploration of ethical challenges in the cybersecurity profession through a series of 15 interviews with cybersecurity experts. Our approach is qualitative and exploratory, aiming to answer the research question \"What ethical challenges, insights, and solutions arise in different areas of cybersecurity?\". Our findings indicate that there are broad ethical challenges across the whole of cybersecurity, but also that different areas of cybersecurity can face specific ethical considerations for which more detailed guidance can help professionals in those areas. In particular, our findings indicate that security decision-making is expected of all security professionals, but that this requires them to balance a complex mix of technical, objective and subjective points of view, and that resolving conflicts raises challenging ethical dilemmas. We conclude that more work is needed to explore, map, and integrate ethical considerations into cybersecurity practice; the urgent need to conduct further research into the ethics of cybersecurity AI; and highlight the importance of this work for individuals and professional bodies who seek to develop and mature the cybersecurity profession in a responsible manner.",
  "tags": [
    "Cryptography and Security",
    "Human-Computer Interaction"
  ],
  "author": "Ivan Flechais",
  "date": "2023-11-16T19:44:03",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.960Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.960Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700703b"
  },
  "title": "Toward a Blockchain-based Platform to Manage Cybersecurity Certification of IoT devices",
  "abstract": "The goal of this paper is to propose a blockchain-based platform to enhance transparency and traceability of cybersecurity certification information motivated by the recently adopted EU Cybersecurity Act. The proposed platform is generic and intended to support the trusted exchange of cybersecurity certification information for any electronic product, service, or process. However, for the purposes of this paper, we focus on the case study of the cybersecurity certification of IoT devices, which are explicitly referenced in the recently adopted Cybersecurity Act as one of the main domains where it is highlighted the need for an increased level of trust.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Ricardo Neisse",
  "date": "2019-09-16T07:42:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.961Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.961Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700703c"
  },
  "title": "Assessing and Improving Cybersecurity Maturity for SMEs: Standardization aspects",
  "abstract": "SMEs constitute a very large part of the economy in every country and they play an important role in economic growth and social development. SMEs are frequent targets of cybersecurity attacks similar to large enterprises. However, unlike large enterprises, SMEs mostly have limited capabilities regarding cybersecurity practices. Given the increasing cybersecurity risks and the large impact that the risks may bring to the SMEs, assessing and improving the cybersecurity capabilities is crucial for SMEs for sustainability. This research aims to provide an approach for SMEs for assessing and improving their cybersecurity capabilities by integrating key elements from existing industry standards.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Bilge Yigit Ozkan",
  "date": "2020-07-03T15:18:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.962Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.962Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700703d"
  },
  "title": "ICAR, a categorical framework to connect vulnerability, threat and asset managements",
  "abstract": "We present ICAR, a mathematical framework derived from category theory for representing cybersecurity NIST and MITRE's ontologies. Designed for cybersecurity, ICAR is a category whose objects are cybersecurity knowledge (weakness, vulnerability, impacted product, attack technique, etc.) and whose morphisms are relations between this knowledge, that make sense for cybersecurity. Within this rigorous and unified framework, we obtain a knowledge graph capable of identifying the attack and weakness structures of an IS, at the interface between description logics, database theory and cybersecurity. We then define ten cybersecurity queries to help understand the risks incurred by IS and organise their defence.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Arnaud Valence",
  "date": "2023-06-21T12:59:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.963Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.963Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700703e"
  },
  "title": "Collaborative Approaches to Enhancing Smart Vehicle Cybersecurity by AI-Driven Threat Detection",
  "abstract": "The introduction sets the stage for exploring collaborative approaches to bolstering smart vehicle cybersecurity through AI-driven threat detection. As the automotive industry increasingly adopts connected and automated vehicles (CAVs), the need for robust cybersecurity measures becomes paramount. With the emergence of new vulnerabilities and security requirements, the integration of advanced technologies such as 5G networks, blockchain, and quantum computing presents promising avenues for enhancing CAV cybersecurity . Additionally, the roadmap for cybersecurity in autonomous vehicles emphasizes the importance of efficient intrusion detection systems and AI-based techniques, along with the integration of secure hardware, software stacks, and advanced threat intelligence to address cybersecurity challenges in future autonomous vehicles.",
  "tags": [
    "Cryptography and Security",
    "Artificial Intelligence"
  ],
  "author": "Syed Atif Ali",
  "date": "2024-12-31T04:08:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.964Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.964Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700703f"
  },
  "title": "Cybersecurity in the AWS Cloud",
  "abstract": "This paper re-examines the content of a standard advanced course in Cybersecurity from the perspective of Cloud Computing. More precisely, we review the core concepts of Cybersecurity, as presented in a senior undergraduate or graduate class, in light of the Amazon Web Services (AWS) cloud.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Michael Soltys",
  "date": "2020-03-28T22:45:28",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.965Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.965Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007040"
  },
  "title": "Ten AI Stepping Stones for Cybersecurity",
  "abstract": "With the turmoil in cybersecurity and the mind-blowing advances in AI, it is only natural that cybersecurity practitioners consider further employing learning techniques to help secure their organizations and improve the efficiency of their security operation centers. But with great fears come great opportunities for both the good and the evil, and a myriad of bad deals. This paper discusses ten issues in cybersecurity that hopefully will make it easier for practitioners to ask detailed questions about what they want from an AI system in their cybersecurity operations. We draw on the state of the art to provide factual arguments for a discussion on well-established AI in cybersecurity issues, including the current scope of AI and its application to cybersecurity, the impact of privacy concerns on the cybersecurity data that can be collected and shared externally to the organization, how an AI decision can be explained to the person running the operations center, and the implications of the adversarial nature of cybersecurity in the learning techniques. We then discuss the use of AI by attackers on a level playing field including several issues in an AI battlefield, and an AI perspective on the old cat-and-mouse game including how the adversary may assess your AI power.",
  "tags": [
    "Cryptography and Security",
    "Artificial Intelligence"
  ],
  "author": "Ricardo Morla",
  "date": "2019-12-14T09:54:36",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.967Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.967Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007041"
  },
  "title": "A Systems Thinking for Cybersecurity Modeling",
  "abstract": "Solving cybersecurity issues requires a holistic understanding of components, factors, structures and their interactions in cyberspace, but conventional modeling approaches view the field of cybersecurity by their boundaries so that we are still not clear to cybersecurity and its changes. In this paper, we attempt to discuss the application of systems thinking approaches to cybersecurity modeling. This paper reviews the systems thinking approaches and provides the systems theories and methods for tackling cybersecurity challenges, regarding relevant fields, associated impact factors and their interactions. Moreover, an illustrative example of systems thinking frameworks for cybersecurity modeling is developed to help broaden the mind in methodology, theory, technology and practice. This article concludes that systems thinking can be considered as one of the powerful tools of cybersecurity modeling to find, characterize, understand, evaluate and predict cybersecurity.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Dingyu Yan",
  "date": "2020-01-16T10:44:43",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.968Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.968Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007042"
  },
  "title": "Classifying SMEs for Approaching Cybersecurity Competence and Awareness",
  "abstract": "Cybersecurity is increasingly a concern for small and medium-sized enterprises (SMEs), and there exist many awareness training programs and tools for them. The literature mainly studies SMEs as a unitary type of company and provides one-size-fits-all recommendations and solutions. However, SMEs are not homogeneous. They are diverse with different vulnerabilities, cybersecurity needs, and competencies. Few studies considered such differences in standards and certificates for security tools adoption and cybersecurity tailoring for these SMEs. This study proposes a classification framework with an outline of cybersecurity improvement needs for each class. The framework suggests five SME types based on their characteristics and specific security needs: cybersecurity abandoned SME, unskilled SME, expert-connected SME, capable SME, and cybersecurity provider SME. In addition to describing the five classes, the study explains the framework's usage in sampled SMEs. The framework proposes solutions for each class to approach cybersecurity awareness and competence more consistent with SME needs. The final publication is available at ACM Digital Library via this https URL https://doi.org/10.1145/3465481.3469200",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Alireza Shojaifar",
  "date": "2021-10-11T15:59:43",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.969Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.969Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007043"
  },
  "title": "A Review of Quantum Cybersecurity: Threats, Risks and Opportunities",
  "abstract": "The promise of quantum computing is not speeding up conventional computing rather delivering an exponential advantage for certain classes of problems, with profound implications for cybersecurity for instance. With the advent and development of quantum computers, cyberspace security can surely become the most critical problem for the Internet in near future. On contrary, prosaic quantum technology can be promising to transform cybersecurity. This research aims to synthesize basic and fundamental studies concerning quantum cybersecurity that can be emerged both as a threat and solution to critical cybersecurity issues based on a systematic study. We provide a comprehensive, illustrative description of the current state-of-the-art quantum computing and cybersecurity and present the proposed approaches to date. Findings in quantum computing cybersecurity suggest that quantum computing can be adopted for the betterment of cybersecurity threats while it poses the most unexpected threats to cybersecurity. The focus and depth of this systematic survey not only provide quantum and cybersecurity practitioners and researchers with a consolidated body of knowledge about current trends in this area but also underpins a starting point for further research in this field.",
  "tags": [
    "Cryptography and Security",
    "Distributed Computing"
  ],
  "author": "Md Jobair Hossain Faruk",
  "date": "2022-07-07T18:57:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.970Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.970Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007044"
  },
  "title": "Ontological Approach toward Cybersecurity in Cloud Computing",
  "abstract": "Widespread deployment of the Internet enabled building of an emerging IT delivery model, i.e., cloud computing. Albeit cloud computing-based services have rapidly developed, their security aspects are still at the initial stage of development. In order to preserve cybersecurity in cloud computing, cybersecurity information that will be exchanged within it needs to be identified and discussed. For this purpose, we propose an ontological approach to cybersecurity in cloud computing. We build an ontology for cybersecurity operational information based on actual cybersecurity operations mainly focused on non-cloud computing. In order to discuss necessary cybersecurity information in cloud computing, we apply the ontology to cloud computing. Through the discussion, we identify essential changes in cloud computing such as data-asset decoupling and clarify the cybersecurity information required by the changes such as data provenance and resource dependency information.",
  "tags": [
    "Cryptography and Security",
    "Distributed Computing"
  ],
  "author": "Takeshi Takahashi",
  "date": "2014-04-01T08:10:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.971Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.971Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007045"
  },
  "title": "Emergent Behavior in Cybersecurity",
  "abstract": "We argue that emergent behavior is inherent to cybersecurity.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Shouhuai Xu",
  "date": "2015-02-18T02:05:02",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.972Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.972Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007046"
  },
  "title": "Cybersecurity Cost of Quality: Managing the Costs of Cybersecurity Risk Management",
  "abstract": "There is no standard yet for measuring and controlling the costs associated with implementing cybersecurity programs. To advance research and practice towards this end, we develop a mapping using the well-known concept of quality costs and the Framework Core within the Cybersecurity Framework produced by the National Institute of Standards and Technology (NIST) in response to the Cybersecurity Enhancement Act of 2014. This mapping can be easily adopted by organizations that are already using the NIST CSF for cybersecurity risk management to plan, manage, and continually improve cybersecurity operations. If an organization is not using the NIST CSF, this mapping may still be useful for linking elements in accounting systems that are associated with cybersecurity operations and risk management to a quality cost model.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Nicole M. Radziwill",
  "date": "2017-07-09T22:42:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.973Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.973Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007047"
  },
  "title": "Explaining Cybersecurity with Films and the Arts (Extended Abstract)",
  "abstract": "Explaining Cybersecurity with Films and the Arts",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Luca Viganò",
  "date": "2019-05-05T18:47:11",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.974Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.974Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007048"
  },
  "title": "A Value Driven Framework for Cybersecurity Innovation in Transportation & Infrastructure",
  "abstract": "This paper introduces a value-driven cybersecurity innovation framework for the transportation and infrastructure sectors, as opposed to the traditional market-centric approaches that have dominated the field. Recontextualizing innovation categories into sustaining, incremental, disruptive, and transformative, we aim to foster a culture of self-innovation within organizations, enabling a strategic focus on cybersecurity measures that directly contribute to business value and strategic goals. This approach enhances operational effectiveness and efficiency of cyber defences primarily, while also aligns cybersecurity initiatives with mission-critical objectives. We detail a practical method for evaluating the business value of cybersecurity innovations and present a pragmatic approach for organizations to funnel innovative ideas in a structured and repeatable manner. The framework is designed to reinforce cybersecurity capabilities against an evolving cyber threat landscape while maintaining infrastructural integrity. Shifting the focus from general market appeal to sector-specific needs, our framework provides cybersecurity leaders with the strategic cyber-foresight necessary for prioritizing impactful initiatives, thereby making cybersecurity a core business enabler rather than a burden.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Lampis Alevizos",
  "date": "2024-05-12T18:45:11",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.975Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.975Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007049"
  },
  "title": "Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education",
  "abstract": "Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students. However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education. This study addresses this gap using the Theory of Planned Behavior as a theoretical framework. A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution. Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed. A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted. This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Steve Goliath",
  "date": "2024-11-05T16:09:37",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.976Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.976Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700704a"
  },
  "title": "A Review of Topological Data Analysis for Cybersecurity",
  "abstract": "In cybersecurity it is often the case that malicious or anomalous activity can only be detected by combining many weak indicators of compromise, any one of which may not raise suspicion when taken alone. The path that such indicators take can also be critical. This makes the problem of analysing cybersecurity data particularly well suited to Topological Data Analysis (TDA), a field that studies the high level structure of data using techniques from algebraic topology, both for exploratory analysis and as part of a machine learning workflow. By introducing TDA and reviewing the work done on its application to cybersecurity, we hope to highlight to researchers a promising new area with strong potential to improve cybersecurity data science.",
  "tags": [
    "Cryptography and Security",
    "Artificial Intelligence"
  ],
  "author": "Thomas Davies",
  "date": "2022-02-16T13:03:52",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.977Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.977Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700704b"
  },
  "title": "Cybersecurity Entity Alignment via Masked Graph Attention Networks",
  "abstract": "Cybersecurity vulnerability information is often recorded by multiple channels, including government vulnerability repositories, individual-maintained vulnerability-gathering platforms, or vulnerability-disclosure email lists and forums. Integrating vulnerability information from different channels enables comprehensive threat assessment and quick deployment to various security mechanisms. Efforts to automatically gather such information, however, are impeded by the limitations of today's entity alignment techniques. In our study, we annotate the first cybersecurity-domain entity alignment dataset and reveal the unique characteristics of security entities. Based on these observations, we propose the first cybersecurity entity alignment model, CEAM, which equips GNN-based entity alignment with two mechanisms: asymmetric masked aggregation and partitioned attention. Experimental results on cybersecurity-domain entity alignment datasets demonstrate that CEAM significantly outperforms state-of-the-art entity alignment methods.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Yue Qin",
  "date": "2022-07-04T14:19:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.978Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.978Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700704c"
  },
  "title": "Elicitation of SME Requirements for Cybersecurity Solutions by Studying Adherence to Recommendations",
  "abstract": "Small and medium-sized enterprises (SME) have become the weak spot of our economy for cyber attacks. These companies are large in number and often do not have the controls in place to prevent successful attacks, respectively are not prepared to systematically manage their cybersecurity capabilities. One of the reasons for why many SME do not adopt cybersecurity is that developers of cybersecurity solutions understand little the SME context and the requirements for successful use of these solutions. We elicit requirements by studying how cybersecurity experts provide advice to SME. The experts recommendations offer insights into what important capabilities of the solution are and how these capabilities ought to be used for mitigating cybersecurity threats. The adoption of a recommendation hints at a correct match of the solution, hence successful consideration of requirements. Abandoned recommendations point to a misalignment that can be used as a source to inquire missed requirements. Re-occurrence of adoption or abandonment decisions corroborate the presence of requirements. This poster describes the challenges of SME regarding cybersecurity and introduces our proposed approach to elicit requirements for cybersecurity solutions. The poster describes CYSEC, our tool used to capture cybersecurity advice and help to scale cybersecurity requirements elicitation to a large number of participating SME. We conclude by outlining the planned research to develop and validate CYSEC.",
  "tags": [
    "Computers and Society"
  ],
  "author": "Alireza Shojaifar",
  "date": "2020-07-16T08:36:40",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.979Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.979Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700704d"
  },
  "title": "An Assessment Methodology and Instrument for Cybersecurity: The Ireland Use Case",
  "abstract": "Governments around the world are required to strengthen their national cybersecurity capabilities to respond effectively to the growing, changing, and sophisticated cyber threats and attacks, thus protecting society and the way of life as a whole. Responsible government institutions need to revise, evaluate, and bolster their national cybersecurity capabilities to fulfill the new requirements, for example regarding new trends affecting cybersecurity, key supporting laws and regulations, and implementations risk and challenges. This report presents a comprehensive assessment instrument for cybersecurity at the national level in order to help countries to ensure optimum response capability and more effective use of critical resources of each state. More precisely, the report - builds a common understanding of the critical cybersecurity capabilities and competence to be assessed at the national level, - adds value to national strategic planning and implementation which impact the development and adaptation of national cybersecurity strategies, - provides an overview of the assessment approaches at the national level, including capabilities, frameworks, and controls, - introduces a comprehensive cybersecurity instrument for countries to determine areas of improvement and develop enduring national capabilities, - describes how to apply the proposed national cybersecurity assessment framework in a real-world case, and - presents the results and lessons learned of the application of the assessment framework at the national level to assist governments in further building cybersecurity capabilities.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Marco Alfano",
  "date": "2023-02-10T10:47:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.980Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.980Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700704e"
  },
  "title": "Autonomous Building Cyber-Physical Systems Using Decentralized Autonomous Organizations, Digital Twins, and Large Language Model",
  "abstract": "Current autonomous building research primarily focuses on energy efficiency and automation. While traditional artificial intelligence has advanced autonomous building research, it often relies on predefined rules and struggles to adapt to complex, evolving building operations. Moreover, the centralized organizational structures of facilities management hinder transparency in decision-making, limiting true building autonomy. Research on decentralized governance and adaptive building infrastructure, which could overcome these challenges, remains relatively unexplored. This paper addresses these limitations by introducing a novel Decentralized Autonomous Building Cyber-Physical System framework that integrates Decentralized Autonomous Organizations, Large Language Models, and digital twins to create a smart, self-managed, operational, and financially autonomous building infrastructure. This study develops a full-stack decentralized application to facilitate decentralized governance of building infrastructure. An LLM-based artificial intelligence assistant is developed to provide intuitive human-building interaction for blockchain and building operation management-related tasks and enable autonomous building operation. Six real-world scenarios were tested to evaluate the autonomous building system's workability, including building revenue and expense management, AI-assisted facility control, and autonomous adjustment of building systems. Results indicate that the prototype successfully executes these operations, confirming the framework's suitability for developing building infrastructure with decentralized governance and autonomous operation.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Reachsak Ly",
  "date": "2024-10-25T02:34:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.981Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.981Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700704f"
  },
  "title": "Emerging Threats in Deep Learning-Based Autonomous Driving: A Comprehensive Survey",
  "abstract": "Since the 2004 DARPA Grand Challenge, the autonomous driving technology has witnessed nearly two decades of rapid development. Particularly, in recent years, with the application of new sensors and deep learning technologies extending to the autonomous field, the development of autonomous driving technology has continued to make breakthroughs. Thus, many carmakers and high-tech giants dedicated to research and system development of autonomous driving. However, as the foundation of autonomous driving, the deep learning technology faces many new security risks. The academic community has proposed deep learning countermeasures against the adversarial examples and AI backdoor, and has introduced them into the autonomous driving field for verification. Deep learning security matters to autonomous driving system security, and then matters to personal safety, which is an issue that deserves attention and research.This paper provides an summary of the concepts, developments and recent research in deep learning security technologies in autonomous driving. Firstly, we briefly introduce the deep learning framework and pipeline in the autonomous driving system, which mainly include the deep learning technologies and algorithms commonly used in this field. Moreover, we focus on the potential security threats of the deep learning based autonomous driving system in each functional layer in turn. We reviews the development of deep learning attack technologies to autonomous driving, investigates the State-of-the-Art algorithms, and reveals the potential risks. At last, we provides an outlook on deep learning security in the autonomous driving field and proposes recommendations for building a safe and trustworthy autonomous driving system.",
  "tags": [
    "Cryptography and Security",
    "Artificial Intelligence",
    "Machine Learning"
  ],
  "author": "Hui Cao",
  "date": "2022-10-19T10:04:33",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.982Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.982Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007050"
  },
  "title": "Tackling the \"Gremlins Paradox\" : Autonomous Hot Swap Healing Protocol for Chronic Performance Problem Aversion in On-board Mission Critical Systems for Lethal Autonomous Weapons",
  "abstract": "This paper gives insights into the issue of Chronic Performance Problem Aversion in On-board Mission Critical Systems for Lethal Autonomous Weapons",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Nyagudi Musandu Nyagudi",
  "date": "2014-09-15T17:49:35",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.984Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.984Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007051"
  },
  "title": "A Treatise on Stability of Autonomous and Non-autonomous Systems: Theory and Illustrative Practical Applications",
  "abstract": "Stability is a very important property of any physical system. By a stable system, we broadly mean that small disturbances either in the system inputs or in the initial conditions do not lead to large changes in the overall behavior of the system. To be of practical use, a system must have to be stable. The theory of stability is a vast, rapidly growing subject with prolific and innovative contributions from numerous researchers. As such, an introductory book that covers the basic concepts and minute details about this theory is essential. The primary aim of this book is to make the readers familiar with the various terminologies and methods related to the stability analysis of time-invariant (autonomous) and time-varying (non-autonomous) systems. A special treatment is given to the celebrated Liapunov's direct method which is so far the most widely used and perhaps the best method for determining the stability nature of both autonomous as well as non-autonomous systems. After discussing autonomous systems to a considerable extent, the book concentrates on the non-autonomous systems. From stability point of view, these systems are often quite difficult to manage. Also, unlike their autonomous counterparts, the non-autonomous systems often behave in peculiar manners which can make the analysts arrive at misleading conclusions. Due to these issues, this book attempts to present a careful and systematic study about the stability properties of non-autonomous systems.",
  "tags": [
    "Information Theory"
  ],
  "author": "Ratnadip Adhikari",
  "date": "2013-02-25T21:36:42",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.985Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.985Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007052"
  },
  "title": "Autonomous Systems -- An Architectural Characterization",
  "abstract": "The concept of autonomy is key to the IoT vision promising increasing integration of smart services and systems minimizing human intervention. This vision challenges our capability to build complex open trustworthy autonomous systems. We lack a rigorous common semantic framework for autonomous systems. It is remarkable that the debate about autonomous vehicles focuses almost exclusively on AI and learning techniques while it ignores many other equally important autonomous system design issues. Autonomous systems involve agents and objects coordinated in some common environment so that their collective behavior meets a set of global goals. We propose a general computational model combining a system architecture model and an agent model. The architecture model allows expression of dynamic reconfigurable multi-mode coordination between components. The agent model consists of five interacting modules implementing each one a characteristic function: Perception, Reflection, Goal management, Planning and Self-adaptation. It determines a concept of autonomic complexity accounting for the specific difficulty to build autonomous systems. We emphasize that the main characteristic of autonomous systems is their ability to handle knowledge and adaptively respond to environment changes. We advocate that autonomy should be associated with functionality and not with specific techniques. Machine learning is essential for autonomy although it can meet only a small portion of the needs implied by autonomous system design. We conclude that autonomy is a kind of broad intelligence. Building trustworthy and optimal autonomous systems goes far beyond the AI challenge.",
  "tags": [
    "Systems and Control"
  ],
  "author": "Joseph Sifakis",
  "date": "2018-11-26T10:36:26",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.986Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.986Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007053"
  },
  "title": "Collective Reasoning for Safe Autonomous Systems",
  "abstract": "Collaboration in multi-agent autonomous systems is critical to increase performance while ensuring safety. However, due to heterogeneity of their features in, e.g., perception qualities, some autonomous systems have to be considered more trustworthy than others when contributing to collaboratively build a common environmental model, especially under uncertainty. In this paper, we introduce the idea of increasing the reliability of autonomous systems by relying on collective intelligence. We borrow concepts from social epistemology to exploit individual characteristics of autonomous systems, and define and formalize at design rules for collective reasoning to achieve collaboratively increased safety, trustworthiness and good decision making.",
  "tags": [
    "Multiagent Systems",
    "Computers and Society",
    "Distributed Computing",
    "Systems and Control",
    "Systems and Control (Electrical Engineering)"
  ],
  "author": "Selma Saidi",
  "date": "2023-05-18T20:37:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.987Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.987Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007054"
  },
  "title": "Evaluation of a semi-autonomous attentive listening system with takeover prompting",
  "abstract": "The handling of communication breakdowns and loss of engagement is an important aspect of spoken dialogue systems, particularly for chatting systems such as attentive listening, where the user is mostly speaking. We presume that a human is best equipped to handle this task and rescue the flow of conversation. To this end, we propose a semi-autonomous system, where a remote operator can take control of an autonomous attentive listening system in real-time. In order to make human intervention easy and consistent, we introduce automatic detection of low interest and engagement to provide explicit takeover prompts to the remote operator. We implement this semi-autonomous system which detects takeover points for the operator and compare it to fully tele-operated and fully autonomous attentive listening systems. We find that the semi-autonomous system is generally perceived more positively than the autonomous system. The results suggest that identifying points of conversation when the user starts to lose interest may help us improve a fully autonomous dialogue system.",
  "tags": [
    "Computation and Language"
  ],
  "author": "Haruki Kawai",
  "date": "2024-02-21T03:43:57",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.988Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.988Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007055"
  },
  "title": "Fundamentals of legislation for autonomous artificial intelligence systems",
  "abstract": "The article proposes a method for forming a dedicated operational context in course of development and implementation of autonomous corporate management systems based on example of autonomous systems for a board of directors. The significant part of the operational context for autonomous company management systems is the regulatory and legal environment within which corporations operate. In order to create a special operational context for autonomous artificial intelligence systems, the wording of local regulatory documents can be simultaneously presented in two versions: for use by people and for use by autonomous systems. In this case, the artificial intelligence system will get a well-defined operational context that allows such a system to perform functions within the required standards. Local regulations that provide for the specifics of the joint work of individuals and autonomous artificial intelligence systems can create the basis of the relevant legislation governing the development and implementation of autonomous systems.",
  "tags": [
    "Computers and Society",
    "Artificial Intelligence"
  ],
  "author": "Anna Romanova",
  "date": "2024-09-17T09:50:23",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.989Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.989Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007056"
  },
  "title": "Smoothing Traffic Flow via Control of Autonomous Vehicles",
  "abstract": "The emergence of autonomous vehicles is expected to revolutionize road transportation in the near future. Although large-scale numerical simulations and small-scale experiments have shown promising results, a comprehensive theoretical understanding to smooth traffic flow via autonomous vehicles is lacking. In this paper, from a control-theoretic perspective, we establish analytical results on the controllability, stabilizability, and reachability of a mixed traffic system consisting of human-driven vehicles and autonomous vehicles in a ring road. We show that the mixed traffic system is not completely controllable, but is stabilizable, indicating that autonomous vehicles can not only suppress unstable traffic waves but also guide the traffic flow to a higher speed. Accordingly, we establish the maximum traffic speed achievable via controlling autonomous vehicles. Numerical results show that the traffic speed can be increased by over 6% when there are only 5% autonomous vehicles. We also design an optimal control strategy for autonomous vehicles to actively dampen undesirable perturbations. These theoretical findings validate the high potential of autonomous vehicles to smooth traffic flow.",
  "tags": [
    "Optimization and Control",
    "Systems and Control"
  ],
  "author": "Yang Zheng",
  "date": "2018-12-22T15:25:53",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.992Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.992Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007057"
  },
  "title": "Gradient-free Multi-domain Optimization for Autonomous Systems",
  "abstract": "Autonomous systems are composed of several subsystems such as mechanical, propulsion, perception, planning and control. These are traditionally designed separately which makes performance optimization of the integrated system a significant challenge. In this paper, we study the problem of using gradient-free optimization methods to jointly optimize the multiple domains of an autonomous system to find the set of optimal architectures for both hardware and software. We specifically perform multi-domain, multi-parameter optimization on an autonomous vehicle to find the best decision-making process, motion planning and control algorithms, and the physical parameters for autonomous racing. We detail the multi-domain optimization scheme, benchmark with different core components, and provide insights for generalization to new autonomous systems. In addition, this paper provides a benchmark of the performances of six different gradient-free optimizers in three different operating environments.   Our approach is validated with a case study where we describe the autonomous vehicle system architecture, optimization methods, and finally, provide an argument on gradient-free optimization being a powerful choice to improve the performance of autonomous systems in an integrated manner.",
  "tags": [
    "Robotics",
    "Software Engineering"
  ],
  "author": "Hongrui Zheng",
  "date": "2022-02-28T03:37:51",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.993Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.993Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007058"
  },
  "title": "Socializing Autonomous Units with the Reflexive Game Theory and Resonate-and-Fire neurons",
  "abstract": "In this study the concept of reflexia is applied to modeling behavior of autonomous units. The relationship between reflexia, on the one hand, and mirror neuron system and perception of emotions, on the other hand, is introduced. The main method of using reflexia in a group of autonomous units is Reflexive Game Theory (RGT). To embody RGT in a group of autonomous agents a communication system is employed. This communication system uses frequency domain multiplexing by means of Izhikevich's resonate-and-fire neural models. The result of socialization of autonomous units by means of RGT and communication system is illustrated in several examples.",
  "tags": [
    "Multiagent Systems"
  ],
  "author": "Sergey Tarasenko",
  "date": "2015-04-19T08:37:40",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.994Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.994Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb87007059"
  },
  "title": "Assessing the Criticality of Longitudinal Driving Scenarios using Time Series Data",
  "abstract": "Unfortunately, many people die in car accidents. To reduce these accidents, cars are equipped with driving safety systems. With autonomous vehicles, the driver's behavior becomes irrelevant as the car drives autonomously. All autonomous driving algorithms must undergo extensive testing and validation, especially for safety-critical scenarios. Therefore, the detection of safety-critical driving scenarios is essential for autonomous vehicles. This publication describes safety indicator metrics based on time series covering longitudinal driving data to detect safety-critical driving scenarios.",
  "tags": [
    "Robotics"
  ],
  "author": "Nico Schick",
  "date": "2023-05-29T18:57:11",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.995Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.995Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700705a"
  },
  "title": "Motion Comfort Optimization for Autonomous Vehicles: Concepts, Methods, and Techniques",
  "abstract": "This article outlines the architecture of autonomous driving and related complementary frameworks from the perspective of human comfort. The technical elements for measuring Autonomous Vehicle (AV) user comfort and psychoanalysis are listed here. At the same time, this article introduces the technology related to the structure of automatic driving and the reaction time of automatic driving. We also discuss the technical details related to the automatic driving comfort system, the response time of the AV driver, the comfort level of the AV, motion sickness, and related optimization technologies. The function of the sensor is affected by various factors. Since the sensor of automatic driving mainly senses the environment around a vehicle, including \"the weather\" which introduces the challenges and limitations of second-hand sensors in autonomous vehicles under different weather conditions. The comfort and safety of autonomous driving are also factors that affect the development of autonomous driving technologies. This article further analyzes the impact of autonomous driving on the user's physical and psychological states and how the comfort factors of autonomous vehicles affect the automotive market. Also, part of our focus is on the benefits and shortcomings of autonomous driving. The goal is to present an exhaustive overview of the most relevant technical matters to help researchers and application developers comprehend the different comfort factors and systems of autonomous driving. Finally, we provide detailed automated driving comfort use cases to illustrate the comfort-related issues of autonomous driving. Then, we provide implications and insights for the future of autonomous driving.",
  "tags": [
    "Robotics",
    "Artificial Intelligence",
    "Computer Vision and Pattern Recognition",
    "Machine Learning"
  ],
  "author": "Mohammed Aledhari",
  "date": "2023-06-15T19:32:04",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.996Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.996Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700705b"
  },
  "title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises, Future Directions, and Applications",
  "abstract": "Edge technology aims to bring Cloud resources (specifically, the compute, storage, and network) to the closed proximity of the Edge devices, i.e., smart devices where the data are produced and consumed. Embedding computing and application in Edge devices lead to emerging of two new concepts in Edge technology, namely, Edge computing and Edge analytics. Edge analytics uses some techniques or algorithms to analyze the data generated by the Edge devices. With the emerging of Edge analytics, the Edge devices have become a complete set. Currently, Edge analytics is unable to provide full support for the execution of the analytic techniques. The Edge devices cannot execute advanced and sophisticated analytic algorithms following various constraints such as limited power supply, small memory size, limited resources, etc. This article aims to provide a detailed discussion on Edge analytics. A clear explanation to distinguish between the three concepts of Edge technology, namely, Edge devices, Edge computing, and Edge analytics, along with their issues. Furthermore, the article discusses the implementation of Edge analytics to solve many problems in various areas such as retail, agriculture, industry, and healthcare. In addition, the research papers of the state-of-the-art edge analytics are rigorously reviewed in this article to explore the existing issues, emerging challenges, research opportunities and their directions, and applications.",
  "tags": [
    "Distributed Computing",
    "Artificial Intelligence",
    "Information Retrieval"
  ],
  "author": "Sabuzima Nayak",
  "date": "2021-07-01T21:48:20",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.997Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.997Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700705c"
  },
  "title": "GENIO: Synergizing Edge Computing with Optical Network Infrastructures",
  "abstract": "Edge computing has emerged as a paradigm to bring low-latency and bandwidth-intensive applications close to end-users. However, edge computing platforms still face challenges related to resource constraints, connectivity, and security. We present GENIO, a novel platform that integrates edge computing within existing Passive Optical Network (PON) infrastructures. GENIO enhances central offices with computational and storage resources, enabling telecom operators to leverage their existing PON networks as a distributed edge computing infrastructure. Through simulations, we show the feasibility of GENIO in supporting real-world edge scenarios, and its better performance compared to a traditional edge computing architecture.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Carmine Cesarano",
  "date": "2025-02-19T12:09:54",
  "updated_at": {
    "$date": "2025-04-03T11:21:34.999Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:34.999Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3e1be53cbb8700705d"
  },
  "title": "Edge Routing with Ordered Bundles",
  "abstract": "Edge bundling reduces the visual clutter in a drawing of a graph by uniting the edges into bundles. We propose a method of edge bundling drawing each edge of a bundle separately as in metro-maps and call our method ordered bundles. To produce aesthetically looking edge routes it minimizes a cost function on the edges. The cost function depends on the ink, required to draw the edges, the edge lengths, widths and separations. The cost also penalizes for too many edges passing through narrow channels by using the constrained Delaunay triangulation. The method avoids unnecessary edge-node and edge-edge crossings. To draw edges with the minimal number of crossings and separately within the same bundle we develop an efficient algorithm solving a variant of the metro-line crossing minimization problem. In general, the method creates clear and smooth edge routes giving an overview of the global graph structure, while still drawing each edge separately and thus enabling local analysis.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Sergey Bereg",
  "date": "2012-09-19T12:50:15",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.000Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.000Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700705e"
  },
  "title": "Edge-Path Bundling: A Less Ambiguous Edge Bundling Approach",
  "abstract": "Edge bundling techniques cluster edges with similar attributes (i.e. similarity in direction and proximity) together to reduce the visual clutter. All edge bundling techniques to date implicitly or explicitly cluster groups of individual edges, or parts of them, together based on these attributes. These clusters can result in ambiguous connections that do not exist in the data. Confluent drawings of networks do not have these ambiguities, but require the layout to be computed as part of the bundling process. We devise a new bundling method, Edge-Path bundling, to simplify edge clutter while greatly reducing ambiguities compared to previous bundling techniques. Edge-Path bundling takes a layout as input and clusters each edge along a weighted, shortest path to limit its deviation from a straight line. Edge-Path bundling does not incur independent edge ambiguities typically seen in all edge bundling methods, and the level of bundling can be tuned through shortest path distances, Euclidean distances, and combinations of the two. Also, directed edge bundling naturally emerges from the model. Through metric evaluations, we demonstrate the advantages of Edge-Path bundling over other techniques.",
  "tags": [
    "Graphics"
  ],
  "author": "Markus Wallinger",
  "date": "2021-08-11T22:39:45",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.001Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.001Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700705f"
  },
  "title": "AI on the Edge: Rethinking AI-based IoT Applications Using Specialized Edge Architectures",
  "abstract": "Edge computing has emerged as a popular paradigm for supporting mobile and IoT applications with low latency or high bandwidth needs. The attractiveness of edge computing has been further enhanced due to the recent availability of special-purpose hardware to accelerate specific compute tasks, such as deep learning inference, on edge nodes. In this paper, we experimentally compare the benefits and limitations of using specialized edge systems, built using edge accelerators, to more traditional forms of edge and cloud computing. Our experimental study using edge-based AI workloads shows that today's edge accelerators can provide comparable, and in many cases better, performance, when normalized for power or cost, than traditional edge and cloud servers. They also provide latency and bandwidth benefits for split processing, across and within tiers, when using model compression or model splitting, but require dynamic methods to determine the optimal split across tiers. We find that edge accelerators can support varying degrees of concurrency for multi-tenant inference applications, but lack isolation mechanisms necessary for edge cloud multi-tenant hosting.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Qianlin Liang",
  "date": "2020-03-27T15:43:01",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.002Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.002Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007060"
  },
  "title": "Applications of Auction and Mechanism Design in Edge Computing: A Survey",
  "abstract": "Edge computing as a promising technology provides lower latency, more efficient transmission, and faster speed of data processing since the edge servers are closer to the user devices. Each edge server with limited resources can offload latency-sensitive and computation-intensive tasks from nearby user devices. However, edge computing faces challenges such as resource allocation, energy consumption, security and privacy issues, etc. Auction mechanisms can well characterize bidirectional interactions between edge servers and user devices under the above constraints in edge computing. As demonstrated by the existing works, auction and mechanism design approaches are outstanding on achieving optimal allocation strategy while guaranteeing mutual satisfaction among edge servers and user devices, especially for scenarios with scarce resources. In this paper, we introduce a comprehensive survey of recent researches that apply auction approaches in edge computing. Firstly, a brief overview of edge computing including three common edge computing paradigms, i.e., cloudlet, fog computing and mobile edge computing, is presented. Then, we introduce fundamentals and backgrounds of auction schemes commonly used in edge computing systems. After then, a comprehensive survey of applications of auction-based approaches applied for edge computing is provided, which is categorized by different auction approaches. Finally, several open challenges and promising research directions are discussed.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Houming Qiu",
  "date": "2021-05-08T02:21:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.003Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.003Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007061"
  },
  "title": "EdgeBench: A Workflow-based Benchmark for Edge Computing",
  "abstract": "Edge computing has been developed to utilize multiple tiers of resources for privacy, cost and Quality of Service (QoS) reasons. Edge workloads have the characteristics of data-driven and latency-sensitive. Because of this, edge systems have developed to be both heterogeneous and distributed. The unique characteristics of edge workloads and edge systems have motivated EdgeBench, a workflow-based benchmark aims to provide the ability to explore the full design space of edge workloads and edge systems. EdgeBench is both customizable and representative. It allows users to customize the workflow logic of edge workloads, the data storage backends, and the distribution of the individual workflow stages to different computing tiers. To illustrate the usability of EdgeBench, we also implements two representative edge workflows, a video analytics workflow and an IoT hub workflow that represents two distinct but common edge workloads. Both workflows are evaluated using the workflow-level and function-level metrics reported by EdgeBench to illustrate both the performance bottlenecks of the edge systems and the edge workloads.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Qirui Yang",
  "date": "2020-10-27T03:11:41",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.004Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.004Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007062"
  },
  "title": "Distributed Training for Deep Learning Models On An Edge Computing Network Using ShieldedReinforcement Learning",
  "abstract": "Edge devices with local computation capability has made distributed deep learning training on edges possible. In such method, the cluster head of a cluster of edges schedules DL training jobs from the edges. Using such centralized scheduling method, the cluster head knows all loads of edges, which can avoid overloading the cluster edges, but the head itself may become overloaded. To handle this problem, we propose a multi-agent RL (MARL) system that enables each edge to schedule its jobs using RL. However, without coordination among edges, action collision may occur, in which multiple edges schedule tasks to the same edge and make it overloaded. For this reason, we propose a system called Shielded ReinfOrcement learning (RL) based DL training on Edges (SROLE). In SROLE, the shield deployed in an edge checks action collisions and provides alternative actions to avoid collisions. As the central shield for entire cluster may become a bottleneck, we further propose a decentralized shielding method, where different shields are responsible for different regions in the cluster and they coordinate to avoid action collisions on the region boundaries. Our emulation and real device experiments show SROLE reduces training time by 59% compared to MARL and centralized RL.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Tanmoy Sen",
  "date": "2022-06-01T21:32:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.005Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.005Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007063"
  },
  "title": "Price-Based Distributed Offloading for Mobile-Edge Computing with Computation Capacity Constraints",
  "abstract": "Mobile-edge computing (MEC) is a promising technology to enable real-time information transmission and computing by offloading computation tasks from wireless devices to network edge.",
  "tags": [
    "Information Theory"
  ],
  "author": "Mengyu Liu",
  "date": "2017-12-02T12:39:50",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.006Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.006Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007064"
  },
  "title": "Communication-Aware Computing for Edge Processing",
  "abstract": "We consider a mobile edge computing problem, in which mobile users offload their computation tasks to computing nodes (e.g., base stations) at the network edge. The edge nodes compute the requested functions and communicate the computed results to the users via wireless links. For this problem, we propose a Universal Coded Edge Computing (UCEC) scheme for linear functions to simultaneously minimize the load of computation at the edge nodes, and maximize the physical-layer communication efficiency towards the mobile users. In the proposed UCEC scheme, edge nodes create coded inputs of the users, from which they compute coded output results. Then, the edge nodes utilize the computed coded results to create communication messages that zero-force all the interference signals over the air at each user. Specifically, the proposed scheme is universal since the coded computations performed at the edge nodes are oblivious of the channel states during the communication process from the edge nodes to the users.",
  "tags": [
    "Information Theory"
  ],
  "author": "Songze Li",
  "date": "2017-06-22T23:34:16",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.008Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.008Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007065"
  },
  "title": "Towards Edge-assisted Internet of Things: From Security and Efficiency Perspectives",
  "abstract": "As we are moving towards the Internet of Things (IoT) era, the number of connected physical devices is increasing at a rapid pace. Mobile edge computing is emerging to handle the sheer volume of produced data and reach the latency demand of computation-intensive IoT applications. Although the advance of mobile edge computing on service latency is studied solidly, security and efficiency on data usage in mobile edge computing have not been clearly identified. In this article, we examine the architecture of mobile edge computing and explore the potentials of utilizing mobile edge computing to enhance data analysis for IoT applications, while achieving data security and computational efficiency. Specifically, we first introduce the overall architecture and several promising edge-assisted IoT applications. We then study the security, privacy and efficiency challenges in data processing for mobile edge computing, and discuss the opportunities to enhance data security and improve computational efficiency with the assistance of edge computing, including secure data aggregation, secure data deduplication and secure computational offloading. Finally, several interesting directions on edge-empowered data analysis are presented for future research.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Jianbing Ni",
  "date": "2019-02-19T15:14:28",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.009Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.009Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007066"
  },
  "title": "EdgeWorkflowReal: An Edge Computing based Workflow Execution Engine for Smart Systems",
  "abstract": "Current cloud-based smart systems suffer from weaknesses such as high response latency, limited network bandwidth and the restricted computing power of smart end devices which seriously affect the system's QoS (Quality of Service). Recently, given its advantages of low latency, high bandwidth and location awareness, edge computing has become a promising solution for smart systems. However, the development of edge computing based smart systems is a very challenging job for software developers who do not have the skills for the creation of edge computing environments. The management of edge computing resources and computing tasks is also very challenging. Workflow technology has been widely used in smart systems to automate task and resource management, but there does not yet exist a real-world deployable edge computing based workflow execution engine. To fill this gap, we present EdgeWorkflowReal, an edge computing based workflow execution engine for smart systems. EdgeWorkflowReal supports: 1) automatic creation of a real edge computing environment according to user settings; 2) visualized modelling of edge workflow applications; and 3) automatic deployment, monitoring and performance evaluation of edge workflow applications in a smart system.",
  "tags": [
    "Software Engineering"
  ],
  "author": "Xuejun Li",
  "date": "2021-01-30T14:53:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.010Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.010Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007067"
  },
  "title": "Delving into Crispness: Guided Label Refinement for Crisp Edge Detection",
  "abstract": "Learning-based edge detection usually suffers from predicting thick edges. Through extensive quantitative study with a new edge crispness measure, we find that noisy human-labeled edges are the main cause of thick predictions. Based on this observation, we advocate that more attention should be paid on label quality than on model design to achieve crisp edge detection. To this end, we propose an effective Canny-guided refinement of human-labeled edges whose result can be used to train crisp edge detectors. Essentially, it seeks for a subset of over-detected Canny edges that best align human labels. We show that several existing edge detectors can be turned into a crisp edge detector through training on our refined edge maps. Experiments demonstrate that deep models trained with refined edges achieve significant performance boost of crispness from 17.4% to 30.6%. With the PiDiNet backbone, our method improves ODS and OIS by 12.2% and 12.6% on the Multicue dataset, respectively, without relying on non-maximal suppression. We further conduct experiments and show the superiority of our crisp edge detection for optical flow estimation and image segmentation.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Yunfan Ye",
  "date": "2023-06-27T03:12:58",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.011Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.011Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007068"
  },
  "title": "Watersheds on edge or node weighted graphs \"par l'exemple\"",
  "abstract": "Watersheds have been defined both for node and edge weighted graphs. We show that they are identical: for each edge (resp.\\ node) weighted graph exists a node (resp. edge) weighted graph with the same minima and catchment basin.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Fernand Meyer",
  "date": "2013-03-07T21:15:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.012Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.012Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007069"
  },
  "title": "SketchSplat: 3D Edge Reconstruction via Differentiable Multi-view Sketch Splatting",
  "abstract": "Edges are one of the most basic parametric primitives to describe structural information in 3D. In this paper, we study parametric 3D edge reconstruction from calibrated multi-view images. Previous methods usually reconstruct a 3D edge point set from multi-view 2D edge images, and then fit 3D edges to the point set. However, noise in the point set may cause gaps among fitted edges, and the recovered edges may not align with input multi-view images since the edge fitting depends only on the reconstructed 3D point set. To mitigate these problems, we propose SketchSplat, a method to reconstruct accurate, complete, and compact 3D edges via differentiable multi-view sketch splatting. We represent 3D edges as sketches, which are parametric lines and curves defined by attributes including control points, scales, and opacity. During edge reconstruction, we iteratively sample Gaussian points from a set of sketches and rasterize the Gaussians onto 2D edge images. Then the gradient of the image error with respect to the input 2D edge images can be back-propagated to optimize the sketch attributes. Our method bridges 2D edge images and 3D edges in a differentiable manner, which ensures that 3D edges align well with 2D images and leads to accurate and complete results. We also propose a series of adaptive topological operations and apply them along with the sketch optimization. The topological operations help reduce the number of sketches required while ensuring high accuracy, yielding a more compact reconstruction. Finally, we contribute an accurate 2D edge detector that improves the performance of both ours and existing methods. Experiments show that our method achieves state-of-the-art accuracy, completeness, and compactness on a benchmark CAD dataset.",
  "tags": [
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Haiyang Ying",
  "date": "2025-03-18T23:30:03",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.013Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.013Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700706a"
  },
  "title": "A Case for a Programmable Edge Storage Middleware",
  "abstract": "Edge computing is a fast-growing computing paradigm where data is processed at the local site where it is generated, close to the end-devices. This can benefit a set of disruptive applications like autonomous driving, augmented reality, and collaborative machine learning, which produce incredible amounts of data that need to be shared, processed and stored at the edge to meet low latency requirements. However, edge storage poses new challenges due to the scarcity and heterogeneity of edge infrastructures and the diversity of edge applications. In particular, edge applications may impose conflicting constraints and optimizations that are hard to be reconciled on the limited, hard-to-scale edge resources. In this vision paper we argue that a new middleware for constrained edge resources is needed, providing a unified storage service for diverse edge applications. We identify programmability as a critical feature that should be leveraged to optimize the resource sharing while delivering the specialization needed for edge applications. Following this line, we make a case for eBPF and present the design for Griffin - a flexible, lightweight programmable edge storage middleware powered by eBPF.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Giulia Frascaria",
  "date": "2021-11-29T17:21:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.015Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.015Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700706b"
  },
  "title": "Reliable Transactions in Serverless-Edge Architecture",
  "abstract": "Modern edge applications demand novel solutions where edge applications do not have to rely on a single cloud provider (which cannot be in the vicinity of every edge device) or dedicated edge servers (which cannot scale as clouds) for processing compute-intensive tasks. A recent computing philosophy, Sky computing, proposes giving each user ability to select between available cloud providers.   In this paper, we present our serverless-edge co-design, which extends the Sky computing vision. In our serverless-edge co-design, we expect edge devices to collaborate and spawn required number of serverless functions. This raises several key challenges: (1) how will this collaboration take place, (2) what if some edge devices are compromised, and (3) what if a selected cloud provider is malicious. Hence, we design ServerlessBFT, the first protocol to guarantee Byzantine fault-tolerant (BFT) transactional flow between edge devices and serverless functions. We present an exhaustive list of attacks and their solutions on our serverless-edge co-design. Further, we extensively benchmark our architecture on a variety of parameters.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Suyash Gupta",
  "date": "2022-01-04T05:06:19",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.016Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.016Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700706c"
  },
  "title": "Integration of Blockchain and Edge Computing in Internet of Things: A Survey",
  "abstract": "As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the integration of blockchain and edge computing. In particular, we first give an overview of blockchain and edge computing. We then present a general architecture of an integration of blockchain and edge computing system. We next study how to utilize blockchain to benefit edge computing, as well as how to use edge computing to benefit blockchain. We also discuss the issues brought by the integration of blockchain and edge computing system and solutions from perspectives of resource management, joint optimization, data management, computation offloading and security mechanism. Finally, we analyze and summarize the existing challenges posed by the integration of blockchain and edge computing system and the potential solutions in the future.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "He Xue",
  "date": "2022-05-26T05:14:22",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.017Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.017Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700706d"
  },
  "title": "Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence",
  "abstract": "Along with the rapid developments in communication technologies and the surge in the use of mobile devices, a brand-new computation paradigm, Edge Computing, is surging in popularity. Meanwhile, Artificial Intelligence (AI) applications are thriving with the breakthroughs in deep learning and the many improvements in hardware architectures. Billions of data bytes, generated at the network edge, put massive demands on data processing and structural optimization. Thus, there exists a strong demand to integrate Edge Computing and AI, which gives birth to Edge Intelligence. In this paper, we divide Edge Intelligence into AI for edge (Intelligence-enabled Edge Computing) and AI on edge (Artificial Intelligence on Edge). The former focuses on providing more optimal solutions to key problems in Edge Computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on the edge. This paper provides insights into this new inter-disciplinary field from a broader perspective. It discusses the core concepts and the research road-map, which should provide the necessary background for potential future research initiatives in Edge Intelligence.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Shuiguang Deng",
  "date": "2019-09-02T06:37:13",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.018Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.018Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700706e"
  },
  "title": "Data, text and web mining for business intelligence: a survey",
  "abstract": "The Information and Communication Technologies revolution brought a digital world with huge amounts of data available. Enterprises use mining technologies to search vast amounts of data for vital insight and knowledge. Mining tools such as data mining, text mining, and web mining are used to find hidden knowledge in large databases or the Internet.",
  "tags": [
    "Information Retrieval"
  ],
  "author": "Abdul-Aziz Rashid Al-Azmi",
  "date": "2013-04-12T08:04:31",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.019Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.019Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700706f"
  },
  "title": "Big Graph Mining: Frameworks and Techniques",
  "abstract": "Big graph mining is an important research area and it has attracted considerable attention. It allows to process, analyze, and extract meaningful information from large amounts of graph data. Big graph mining has been highly motivated not only by the tremendously increasing size of graphs but also by its huge number of applications. Such applications include bioinformatics, chemoinformatics and social networks. One of the most challenging tasks in big graph mining is pattern mining in big graphs. This task consists on using data mining algorithms to discover interesting, unexpected and useful patterns in large amounts of graph data. It aims also to provide deeper understanding of graph data. In this context, several graph processing frameworks and scaling data mining/pattern mining techniques have been proposed to deal with very big graphs. This paper gives an overview of existing data mining and graph processing frameworks that deal with very big graphs. Then it presents a survey of current researches in the field of data mining / pattern mining in big graphs and discusses the main research issues related to this field. It also gives a categorization of both distributed data mining and machine learning techniques, graph processing frameworks and large scale pattern mining approaches.",
  "tags": [
    "Distributed Computing"
  ],
  "author": "Sabeur Aridhi",
  "date": "2016-02-09T16:53:08",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.020Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.020Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007070"
  },
  "title": "Proceedings of Symposium on Data Mining Applications 2014",
  "abstract": "The Symposium on Data Mining and Applications (SDMA 2014) is aimed to gather researchers and application developers from a wide range of data mining related areas such as statistics, computational intelligence, pattern recognition, databases, Big Data Mining and visualization. SDMA is organized by MEGDAM to advance the state of the art in data mining research field and its various real world applications. The symposium will provide opportunities for technical collaboration among data mining and machine learning researchers around the Saudi Arabia, GCC countries and Middle-East region. Acceptance will be based primarily on originality, significance and quality of contribution.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Basit Qureshi",
  "date": "2020-01-29T07:30:00",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.021Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.021Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007071"
  },
  "title": "Multi-source Data Mining for e-Learning",
  "abstract": "Data mining is the task of discovering interesting, unexpected or valuable structures in large datasets and transforming them into an understandable structure for further use . Different approaches in the domain of data mining have been proposed, among which pattern mining is the most important one. Pattern mining mining involves extracting interesting frequent patterns from data. Pattern mining has grown to be a topic of high interest where it is used for different purposes, for example, recommendations. Some of the most common challenges in this domain include reducing the complexity of the process and avoiding the redundancy within the patterns. So far, pattern mining has mainly focused on the mining of a single data source. However, with the increase in the amount of data, in terms of volume, diversity of sources and nature of data, mining multi-source and heterogeneous data has become an emerging challenge in this domain. This challenge is the main focus of our work where we propose to mine multi-source data in order to extract interesting frequent patterns.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Julie Bu Daher",
  "date": "2020-09-17T15:39:45",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.022Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.022Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007072"
  },
  "title": "A comparative study of top-k high utility itemset mining methods",
  "abstract": "High Utility Itemset (HUI) mining problem is one of the important problems in the data mining literature. The problem offers greater flexibility to a decision maker to incorporate her/his notion of utility into the pattern mining process. The problem, however, requires the decision maker to choose a minimum utility threshold value for discovering interesting patterns. This is quite challenging due to the disparate itemset characteristics and their utility distributions. In order to address this issue, Top-K High Utility Itemset (THUI) mining problem was introduced in the literature. THUI mining problem is primarily a variant of the HUI mining problem that allows a decision maker to specify the desired number of HUIs rather than the minimum utility threshold value. Several algorithms have been introduced in the literature to efficiently mine top-k HUIs. This paper systematically analyses the top-k HUI mining methods in the literature, describes the methods, and performs a comparative analysis. The data structures, threshold raising strategies, and pruning strategies adopted for efficient top-k HUI mining are also presented and analysed. Furthermore, the paper reviews several extensions of the top-k HUI mining problem such as data stream mining, sequential pattern mining and on-shelf utility mining. The paper is likely to be useful for researchers to examine the key methods in top-k HUI mining, evaluate the gaps in literature, explore new research opportunities and enhance the state-of-the-art in high utility pattern mining.",
  "tags": [
    "Data Structures and Algorithms"
  ],
  "author": "Srikumar Krishnamoorthy",
  "date": "2018-09-04T04:18:52",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.023Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.023Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007073"
  },
  "title": "Partial Selfish Mining for More Profits",
  "abstract": "Mining attacks aim to gain an unfair share of extra rewards in the blockchain mining. Selfish mining can preserve discovered blocks and strategically release them, wasting honest miners' computing resources and getting higher profits. Previous mining attacks either conceal the mined whole blocks (hiding or discarding), or release them completely in a particular time slot (e.g., causing a fork). In this paper, we extend the mining attack's strategy space to partial block sharing, and propose a new and feasible Partial Selfish Mining (PSM) attack. We show that by releasing partial block data publicly and attracting rational miners to work on attacker's private branch, attackers and these attracted miners can gain an unfair share of mining rewards. We then propose Advanced PSM (A-PSM) attack that can further improve attackers' profits to be no less than the selfish mining. Both theoretical and experimental results show that PSM attackers can be more profitable than selfish miners under a certain range of mining power and network conditions. A-PSM attackers can gain even higher profits than both selfish mining and honest mining with attracted rational miners.",
  "tags": [
    "Cryptography and Security",
    "Distributed Computing"
  ],
  "author": "Jiaping Yu",
  "date": "2022-07-27T11:58:38",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.025Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.025Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007074"
  },
  "title": "Building a Classification Model for Enrollment In Higher Educational Courses using Data Mining Techniques",
  "abstract": "Data Mining is the process of extracting useful patterns from the huge amount of database and many data mining techniques are used for mining these patterns. Recently, one of the remarkable facts in higher educational institute is the rapid growth data and this educational data is expanding quickly without any advantage to the educational management. The main aim of the management is to refine the education standard; therefore by applying the various data mining techniques on this data one can get valuable information. This research study proposed the \"classification model for the student's enrollment process in higher educational courses using data mining techniques\". Additionally, this study contributes to finding some patterns that are meaningful to management.",
  "tags": [
    "Artificial Intelligence"
  ],
  "author": "Priyanka Saini",
  "date": "2014-05-15T02:53:44",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.026Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.026Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007075"
  },
  "title": "Spatiotemporal Data Mining: A Survey",
  "abstract": "Spatiotemporal data mining aims to discover interesting, useful but non-trivial patterns in big spatial and spatiotemporal data. They are used in various application domains such as public safety, ecology, epidemiology, earth science, etc. This problem is challenging because of the high societal cost of spurious patterns and exorbitant computational cost. Recent surveys of spatiotemporal data mining need update due to rapid growth. In addition, they did not adequately survey parallel techniques for spatiotemporal data mining. This paper provides a more up-to-date survey of spatiotemporal data mining methods. Furthermore, it has a detailed survey of parallel formulations of spatiotemporal data mining.",
  "tags": [
    "Computer Vision and Pattern Recognition",
    "Distributed Computing",
    "Machine Learning"
  ],
  "author": "Arun Sharma",
  "date": "2022-06-26T00:08:06",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.027Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.027Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007076"
  },
  "title": "Analysis of Email Fraud detection using WEKA Tool",
  "abstract": "Data mining is also being useful to give solutions for invasion finding and auditing. While data mining has several applications in protection, there are also serious privacy fears. Because of email mining, even inexperienced users can connect data and make responsive associations. Therefore we must to implement the privacy of persons while working on practical data mining",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Tarushi Sharma",
  "date": "2014-05-05T06:23:09",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.029Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.029Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007077"
  },
  "title": "A Noise Addition Scheme in Decision Tree for Privacy Preserving Data Mining",
  "abstract": "Data mining deals with automatic extraction of previously unknown patterns from large amounts of data. Organizations all over the world handle large amounts of data and are dependent on mining gigantic data sets for expansion of their enterprises. These data sets typically contain sensitive individual information, which consequently get exposed to the other parties. Though we cannot deny the benefits of knowledge discovery that comes through data mining, we should also ensure that data privacy is maintained in the event of data mining. Privacy preserving data mining is a specialized activity in which the data privacy is ensured during data mining. Data privacy is as important as the extracted knowledge and efforts that guarantee data privacy during data mining are encouraged. In this paper we propose a strategy that protects the data privacy during decision tree analysis of data mining process. We propose to add specific noise to the numeric attributes after exploring the decision tree of the original data. The obfuscated data then is presented to the second party for decision tree analysis. The decision tree obtained on the original data and the obfuscated data are similar but by using our method the data proper is not revealed to the second party during the mining process and hence the privacy will be preserved.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Mohammad Ali Kadampur",
  "date": "2010-01-20T08:22:46",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.030Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.030Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007078"
  },
  "title": "Spatio-Temporal Data Mining: A Survey of Problems and Methods",
  "abstract": "Large volumes of spatio-temporal data are increasingly collected and studied in diverse domains including, climate science, social sciences, neuroscience, epidemiology, transportation, mobile health, and Earth sciences. Spatio-temporal data differs from relational data for which computational approaches are developed in the data mining community for multiple decades, in that both spatial and temporal attributes are available in addition to the actual measurements/attributes. The presence of these attributes introduces additional challenges that needs to be dealt with. Approaches for mining spatio-temporal data have been studied for over a decade in the data mining community. In this article we present a broad survey of this relatively young field of spatio-temporal data mining. We discuss different types of spatio-temporal data and the relevant data mining questions that arise in the context of analyzing each of these datasets. Based on the nature of the data mining problem studied, we classify literature on spatio-temporal data mining into six major categories: clustering, predictive learning, change detection, frequent pattern mining, anomaly detection, and relationship mining. We discuss the various forms of spatio-temporal data mining problems in each of these categories.",
  "tags": [
    "Machine Learning",
    "Artificial Intelligence",
    "Computer Vision and Pattern Recognition"
  ],
  "author": "Gowtham Atluri",
  "date": "2017-11-13T17:17:29",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.031Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.031Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb87007079"
  },
  "title": "An Open-Source Integration of Process Mining Features into the Camunda Workflow Engine: Data Extraction and Challenges",
  "abstract": "Process mining provides techniques to improve the performance and compliance of operational processes. Although sometimes the term \"workflow mining\" is used, the application in the context of Workflow Management (WFM) and Business Process Management (BPM) systems is limited. The main reason is that WFM/BPM systems control the process, leaving less room for flexibility and the corresponding deviations. However, as this paper shows, it is easy to extract event data from systems like Camunda, one of the leading open-source WFM/BPM systems. Moreover, although the respective process engines control the process flow, process mining is still able to provide valuable insights, such as the analysis of the performance of the paths and the mining of the decision rules. This demo paper presents a process mining connector to Camunda that extracts event logs and process models, allowing for the application of existing process mining tools. We also analyzed the added value of different process mining techniques in the context of Camunda. We discuss a subset of process mining techniques that nicely complements the process intelligence capabilities of Camunda. Through this demo paper, we hope to boost the use of process mining among Camunda users.",
  "tags": [
    "Software Engineering"
  ],
  "author": "Alessandro Berti",
  "date": "2020-09-14T05:49:32",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.032Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.032Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700707a"
  },
  "title": "Mining Rank Data",
  "abstract": "The problem of frequent pattern mining has been studied quite extensively for various types of data, including sets, sequences, and graphs. Somewhat surprisingly, another important type of data, namely rank data, has received very little attention in data mining so far. In this paper, we therefore addresses the problem of mining rank data, that is, data in the form of rankings (total orders) of an underlying set of items. More specifically, two types of patterns are considered, namely frequent rankings and dependencies between such rankings in the form of association rules. Algorithms for mining frequent rankings and frequent closed rankings are proposed and tested experimentally, using both synthetic and real data.",
  "tags": [
    "Machine Learning",
    "Statistical Machine Learning"
  ],
  "author": "Sascha Henzgen",
  "date": "2018-06-15T10:36:40",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.033Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.033Z"
  }
},
{
  "_id": {
    "$oid": "67ee6f3f1be53cbb8700707b"
  },
  "title": "A Brief Study of Privacy-Preserving Practices (PPP) in Data Mining",
  "abstract": "Data mining is the way toward mining fascinating patterns or information from an enormous level of the database. Data mining additionally opens another risk to privacy and data security.One of the maximum significant themes in the research fieldis privacy-preserving DM (PPDM). Along these lines, the investigation of ensuring delicate information and securing sensitive mined snippets of data without yielding the utility of the information in a dispersed domain.Extracted information from the analysis can be rules, clusters, meaningful patterns, trends or classification models. Privacy breach occur at some stage in the communication of data and aggregation of data. So far, many effective methods and techniques have been developed for privacy-preserving data mining, but yields into information loss and side effects on data utility and data mining effectiveness downgraded. In the f ocal point of consideration on the viability of Data Mining, Privacy and rightness should be improved and to lessen the expense.",
  "tags": [
    "Cryptography and Security"
  ],
  "author": "Dhinakaran D",
  "date": "2023-04-28T03:24:17",
  "updated_at": {
    "$date": "2025-04-03T11:21:35.034Z"
  },
  "created_at": {
    "$date": "2025-04-03T11:21:35.034Z"
  }
}]